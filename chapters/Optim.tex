\setchapterpreamble[u]{\margintoc}
\chapter{Optimization \& Gradient Descent}
\labch{Optim}

Optimzers have played a very important role for the development of neural networks and their comeback after the so-called 'AI winter'.

The very early implementations of networks like the perceptron, had very straight forward rules for tuning the weights within the network.

These networks were inspired by nature and so was training them.
In accordance to observations made on neurons, weights were 
%what fires together wires together (Hebb's rule)

\section{Optimization Problems}
%optimization
%lagrangian for SVM


\section{Gradient Descent}
%hwo does gradient descent work?

\section{Backpropagation}
%backpropagation finally allowed to train neural networks efficiently

%forward pass
%reverse pass
%gradient descent step
%vanishing gradient problem

\section{Vanishing Gradients}
\subsection{Normalization}
\subsection{Residual Networks}
%Motivate by facilitating training or look into book
Ioffe and Szegedy introduced BN to ease training of feed-forward networks.
affine parameters $\gamma$ and $\beta$


\section{Optimization Algorithms}
\subsection{Momentum}
\subsection{Stochastic Gradient Descent}
\subsection{AdaM}
\subsection{Other Algorithms}

