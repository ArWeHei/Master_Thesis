\setchapterpreamble[u]{\margintoc}
\chapter{Artistic Computer Vision}
\labch{ACV}
This chapter shall dive into related work that combines computer vision with artistic content.

Applying computer vision techniques to images with artistic content has been interesting and challenging at the same time.
Due to the often observed change in appearance that can be observed for many artistic images, existing CV pipelines for \ie classification usually do not work as intended.
This makes it all the more interesting to see, how these pipelines are affected by this domain gap.

At the same time artistic images bring something into the mix which 

%in SBR stroke is the basic unit texture synthesis fills each pixel individually.

%non-photorealistic rendering vs. photrealism
%clesly realted to texture synthesis and transfer
%games and movies benefit from this
%since the 1990's starting with artistic rendering of images (paint by numbers)

%with neural network more practical applicatin of style transfer have become popular:
% - funny iphone filters
% - translating images between other domains (google maps, satelites etc.) -> many possible applications
% quantitative style (materials used, color shoice etc.)
% qualitative style (how are people drawn etc.)

% historically there have been many different approaches
%try to group these approaches on the problem formulation and the approach

%color choice
%how to display something (is it unnaturally large or small ...)
%direction of brush strokes
%choice of material
%which details are kept which details are lost?

%all of this together makes style

%usually style transfer formulates this much simpler (maybe see gatys formulation?):
%create something that maintains the orginal content to some  degree but people would categorize together with other works of that 'style'
% style is rather something by what art can be grouped together 'easily'


\section{Problem Formulation \& Approach}
The problem that style transfer might seem easy to capture at first, but it quickly becomes harder when trying to formulate it.
As 'style' is simply defined as "a typical way of doing something", it includes actually all aspects 

%style transfer
%create IMAGE that catches style as well a possible, doesn't rally matter how, the pixel should make sense in the end

%painterly rendering
%create something that looks like is has been drawn with a paint brush (or any other such tool, e.g. has some gloss to it etc.)
%don't really concentrate on deeper style but just the apprearance

%drawing networks
%do not care as much about the apprearance or the style but how to compose the image in a different parameter space, FAST

%brush stroke extraction and image analysis
%get more information about the image, that can help to classify it, or identify it. Do not get all brush strokes but those that are characteristic

%drawing networks and painterly rendering are really much of the same with different focus on what matters.


%this work would fall into painterly rendering territory paired with brush stroke extraction


%features of DNN have been used to classify paintings

\section{Brush Stroke Analysis}
%no updates in recent years
%relatively small field
%shift towards creating art rather than analyzing it
%what to do with the data besides identifying forgeries?





\section{Style Transfer}
\labsec{ST}

This field of style transfer has its origins 
%most active field
%style and texture are similar problem

\subsection{Early Approaches}
%IMAGE PROCESSING BASED TECHNIQUES
%filters or LIC as approaches

\citeauthor*{imageanalogies} proposed a way of stylizing images by using trainable filters.
They call their technique 'image analogies' since the transformation between two training images is analogously applied to a test image.

Basically, for creating the impression of brush strokes, a photograph $A$ and a painting of that photograph $A'$ are required.
As this is rarely the case, \citeauthor*{iamgeanalogies} showed that using anisotropic diffusion works also reasonably well to generate $A$ from $A'$.
Then their algorithm searches locally around a position $p$ for the best filter parameters $F(p)$ that transform $A(p)$ into $A'(p)$.
By using another search algorithm to match similar regions $B(p)$ and $A(p)$, $F(p)$ can be used to transform $B(p)$ into $B'(p)$.
Thus $B'$ -- a stylized version of $B$ -- can be obtained by transforming it analogously to $A \rightarrow A'$.
%Hertzmann et al 2010 smooth a painting and optimize filter parameters for the inverse transformation -> apply the filters to photos
    %similar to superresolution and texture synthesis
%first approach is just grouping pixels simialrly to super-pixels
Other works like \citeauthor*{texturetransfer}~\cite{texturetransfer} have built on this principle idea which has led to a fast texture transfer algorithm \cite{fasttexturetransfer}.
Besides only matching local region according to their pixel values, \citeauthor*{texturetransfer} obtain a flow map (which they call 'directions'), that is based on the content image's gradient.
This flow is then also matched against the style image.
\begin{figure}
    \includegraphics{texturetransfer}
    \caption[]{Target image $T$ is combined with a dictional map specially obtained from $T$ and a style image  $S$. The result maintains the direction's flow while presenting the texture from $S$.}
    \labfig{texturetransfer}
\end{figure}

%Lee et al 2010 transfer the texture of brush stroke onto an image much like style transfer (this is the intersection)
%painterly rendering is some kind of style transfer but shall be dealt with explicitly
%focus is on approaches that work only with pixels
%filters
%texton based approaches

%Some early approaches include histogram matching on linear ï¬lter responses [19] and non-parametric sampling [12, 15].
%       . Pyramid-based texture analysis/synthesis
%       Split and match: example-based adaptive patch sampling for unsupervised style transfer
%       Image quilting for texture synthesis and transfe
%low-level statistics and fail to perceive semantic structures



\subsection{Neural Style Transfer}
%A bit similar to transfer learning -> utilize the pretrained features especially of the early layers.
The field of style transfer has really gained traction in 2015 with the publication of \textbf{A Neural Algorithm for Artistic Style} by \citeauthor*{gatys}.
It was the first approach to transfer the style of one image to another and at the same time maintaining a high contextual fidelity.
In retrospective, this work really kicked off neural style transfer as a field.

\citeauthor*{gatys} themselves pinpoint the novelty of their approach as 'manipulations in feature spaces' as opposed to previous approaches that 'directly manipulate the pixel representation of an image'\cite{gatys}.
They use existing neural architectures and extract information in two separate ways, such that content and style can be separated.

Previous works already used \textbf{perceptual loss} to accumulate information on the content in an image \cite{percep_loss}, or check whether two images have the same content \cite{other_percep_loss}.
%networks trained on object recognition increasingly care about the content with every layer
Perceptual loss is based on the VGG-19 architecture \cite{VGG} which is a deep CCN trained for object classification on ImageNet \cite{imagenet}.
By arguing that the network's layer activations increasingly respond to the content when following the networks' hierarchy.
Some much even, that it is possible to reconstruct the content of an image by using the activations of one such layer.

For reconstruction of an image's content, gradient descent is performed on a white noise image.
The gradient descent aims to minimize the perceptual distance between the reconstruction and the target image.
Perceptual distance is defined as the L2-distance between the activations of two images in deep layer of the VGG-network.

For image vector $\vec{x}$ with $\vec{x} \in = \R^{M_0}, M_0 = H_x \dot W_x $, a layer $l$ of the network has $N_l$ feature maps of size $M_l$.
In this case $M_l$ is equal to the height times the width of the feature map of the $l$-th layer.
The activations of the $i$-th filter ($i \in N_l$) at position $j$ ($j \in M_l$) at layer $l$ can then be represented by matrix $F(\vec{x})_{ij}^l \in \R^{N_l \times M_l}$.
The perceptual distance is then defined as
\begin{align}
    d_{\text{percep}}(\vec{x}, \vec{y}) = \sum_{i, j} (F(\vec{x})_{ij}^l - F(\vec{y})_{ij}^l)^2 = \norm{\tensor{F(\vec{x})}^l - \tensor{F(\vec{y})}^l}_2^2
\end{align}
, which allows to define the perceptual loss or content loss as 
\begin{align}
    \L_{\text{content}} = \frac{1}{2} \sum_{i, j} (F(\vec{x})_{ij}^l - F(\vec{y})_{ij}^l)^2 = \frac{1}{2} \norm{\tensor{F(\vec{x})}^l - \tensor{F(\vec{y})}^l}_2^2
\end{align}

\marginnote{The factor $\frac{1}{2}$ will cancel out when deriving $\L_{\text{content}}$ with respect to $F(\vec{x})_{ij}^l$.}

Minimizing the content loss between two images, by using gradient descent the content of an image can be restored (see \reffig{content_style_loss}).

\begin{figure*}
    \includegraphics{content_style_loss}
    \caption[]{Reconstructions of content(bottom) and style(top) using different layers. \cite{gatys}}
    \labfig{content_style_loss}
\end{figure*}

As approximating the content of an image has now become possible, the question is whether this is possible with style as well.
\citeauthor*{gatys} again turned to the pre-trained VGG network for this.
They explicitly reduce style to texture for this reason and thus search for a feature space that captures \textbf{texture} rather than content.
Subsequently, \citeauthor*{gatys} propose the use of \textbf{Gram matrices} as they capture the correlations of feature-activations over their spatial extent.

The Gram matrix of a given matrix $\tensor{A}$ is the inner product of all column vectors in $\tensor{A}$.
\begin{align}
    G = \langle a_{i}, a_{j} \rangle = \tensor{A}^T \tensor{A} \text{ if $a_1$...$a_j$ are column vectors of $\tensor{A}$}
\end{align}
The resulting Gram matrix G now has the form $j \times j$ and captures texture information but no longer the global content.

\refsec{problem} already mentioned that style is very complex and exists at various scales at the same time which \citeauthor*{gatys} address by using many layers.
As these layers sit at different depths their field of view varies and each layer captures information at a different scale.
Early layers will tend to hold small scale information, later layers will hold larger scale information with every layer.

\citeauthor*{gatys} first compute the Gram matrices each layer $l$ for both the target style image and the current image.
Then they use the L2 distance metric to measure the discrepancy between them.
\begin{align}
    G(\vec{x})^l & = \frac{1}{(2 N_x^l M_x^l)^2} F(\vec{x})^l{}^T F(\vec{x})^l \\
    G(\vec{y})^l & = \frac{1}{(2 N_y^l M_y^l)^2} F(\vec{y})^l{}^T F(\vec{y})^l \\
    d_{\text{style}}^l(\vec{x}, \vec{y}) & = \norm{G(\vec{x})^l - G(\vec{y})^l}^2_2 \\
\end{align}

\marginnote{The denominator of $\frac{1}{4 N_x^l{}^2 M_x^l{}^2}$ is squared since the Gram matrix is the product of a matrix with itself transposed.}

The style distances at each layer are then weighted and summed up to make up the style loss:

\begin{align}
    \L_{\text{style}} = \sum_{l} w_l d_{\text{style}}^l(\vec{x}, \vec{y})
\end{align}

%again one can visualize the style/texture that is perceived
This style loss can again by used together with gradient descent in order to check whether it is possible to reconstruct the texture of an image much like the content of an image.
\reffig{content_style_loss} shows that it is in fact possible to reconstruct the texture of the image at various scales.
Specifically the local consistency of each texture becomes larger, the deeper the layer sits.

%mix content and style
\citeauthor*{gatys} now combine the losses for one content image $\vec{c}$ with a style image $\vec{s}$ and optimize $\vec{x}$ in the same way the reconstructions have been obtained.

\begin{align}
    \L_{\text{total}} = \lambda_{\text{content}} \L_{\text{content}}(\vec{x}, \vec{c}) + \lambda_{\text{style}} \L_{\text{style}}(\vec{x}, \vec{s})
\end{align}

\begin{marginfigure}
    \includegraphics{gatys_ST}
    \caption[]{Style transfer examples by \citeauthor*{gatys}. \cite{gatys}}
    \labfig{content_style_loss}
\end{marginfigure}

The result of this can be seen in \reffig{gatys_ST}

\subsubsection{Follow-Up Research}
There has been some follow-up research on \citeauthor*{gatys}'s work which addresses mainly how the style loss works.

\citeauthor*{MMD} have shown that the style loss is equivalent to calculating the \textbf{maximum mean discrepancy (MMD)} between the features of each layer \cite{MMD}.
MMD is a test-statistic for a null hypothesis $p=q$ with the data $X = \{x_i\}^n_{i=1}$, sampled from $p$, and $Y = \{y_i\}^n_{i=1}$, sampled from q, at hand.
It can be used as a difference measure as well and vanishes only if $p=q$.
\begin{align}
    \text{MMD}^2[X, Y] & = \frac{1}{n^2} \sum^n_{i=1} \sum^n_{i'=1} k(\vec{x}_i, \vec{x}_{i'}) \\
    & + \frac{1}{m^2} \sum^m_{j=1} \sum^m_{j'=1} k(\vec{y}_j, \vec{y}_{j'}) \\
    & - \frac{2}{nm} \sum^n_{i=1} \sum^m_{j=1} k(\vec{x}_i, \vec{y}_{j})
\end{align}

MMD can be based on different kernel functions $k$ and \citeauthor*{MMD} have shown that the style loss is equivalent to the squared MMD with a polynomial kernel.
Consequently they were able to show, that style transfer work with different kernel functions as well and even by explicitly matching the batch statistics (see \reffig{MMD}):
\begin{align}
    d_{\text{style}}^l(\vec{x}, \vec{y}) & = \frac{1}{N_l} \sum_{i = 1}^{N_l} \left( (\mu^i_{F(\vec{x})^l} - \mu^i_{F(\vec{y})^l}) + (\sigma^i_{F(\vec{x})^l} - \sigma^i_{F(\vec{y})^l}) \right)
\end{align}

\begin{figure}
    \includegraphics{MMD}
    \caption[]{Reconstructed textures for Starry Night using different kernel functions $k$ \cite{MMD}}
    \labfig{MMD}
\end{figure}


\citeauthor*{LenDu} tested whether pre-trained weights play an important role when performing style transfer.
He was able to show basic style transfer even with random initialized networks, but results vary wisely depending on the random initialization.
Ultimately, it is possible so obtain some style transfer with this technique but the pre-trained weights seem to play an important role in stablizing the reconstruction process.

\subsection{State of the Art}

\subsubsection{Real-Time Style Transfer}
%johnson
Following \citeauthor*{gatys} seminal work, other have followed suit in trying to stylize images with neural networks.
\citeauthor*{Johnson} were the first to use the same losses but train a feed-forward architecture with it \cite{johnson}.
They were able to significantly speed up the stylization process like this as stylization was performed in a single feed-forward pass instead of a lengthy gradient descent optimization.
Ultimately, this enabled them to generate stylized images in real-time from a given content image, using a deep residual convolutional neural network.
\begin{figure*}
    \includegraphics{johnson_net}
    \caption[]{Training set-up by \citeauthor*{johnson}. \cite{johnson}}
    \labfig{johnson_net}
\end{figure*}

%markov random field ansatz?

%adain
\subsubsection{Arbitrary \& Universal Style Transfer}
\citeauthor*{AdaIN} used a different feed-forward approach for arbitrary style.
They first encode an arbitrary style image $\vec{s}$ as well as a content image $\vec{c}$ using a pre-trained VGG network.
This allows them to obtain the activations at a very deep layer of the network $F^l(\vec{s})$ and $F^l(\vec{c})$.
Then they compute the second order statistics for both $\mu_F^l(\vec{s}), \sigma_F^l(\vec{s})$ and $\mu_F^l(\vec{c}), \sigma_F^l(\vec{c})$.
Using adaptive instance normalization (AdaIN), they rescale the content activations such that they match the statistics of the style activations.
\begin{align}
    F'^l = \sigma_F^l(\vec{s}) \frac{F^l(\vec{c} - \mu_F^l(\vec{c})}{\sigma_F^l(\vec{c})} + \mu_F^l(\vec{s}) 
\end{align}
Finally, they train a decoder that minimizes the style and content loss, as they have been proposed by \citeauthor*{gatys}.
They achieve comparable results to other style transfer approaches at a similar speed to \citeauthor*{johnson} while allowing for any target style eventhough only training only once.
\begin{figure*}
    \includegraphics{adain_net}
    \caption[]{Training set-up by \citeauthor*{AdaIN}. \cite{AdaIN}}
    \labfig{johnson_net}
\end{figure*}

%wct
\todo{this is actually just PCA decompsition to a certain degree}
A similar approach by \citeauthor*{WCT} relies on matching the covariance and the mean of content and style activations.
They do that through what they call 'whitening and coloring transform' \cite{WCT}.
First they whiten $F^l(\vec{c})$ into $\hat{F}^l$ such that $\hat{F}^l \hat{F}^l{}^T = I$
\begin{align}
    \hat{F}^l = E_c D_c^{-\frac{1}{2}} E_c^T (F^l(\vec{c}) - \mu_c)
\end{align}
Where $D_c$ is the diagonal matrix of eigenvalues of the covariance matrix and $E_c$ is the respective orthogonal matrix of eigenvectors.
such that
\begin{align}
    F^l(\vec{c}) F^l(\vec{c})^T = E_c D_c E_c^T
\end{align}

Then coloring is performed by rescaling the whitened representation $\hat{F}^l$ into $\tilde{F}^l$
\begin{align}
    \tilde{F}^l = E_s D_s^{\frac{1}{2}} E_s^T \hat{F}^l + \mu_s \\
    F^l(\vec{s}) F^l(\vec{s})^T = E_s D_s E_s^T
\end{align}

The resulting $\tilde{F}^l$ is then decoded with a pre-trained decoder to render the final stylized result.
\citeauthor*{WCT} use pre-train the decoder solely on natural images and perceptual loss and reconstruction loss as objective.
Additionally, they introduce a pipeline that performs style transfer on multiple scales sequentially.
They achieve good results in real-time with just training the decoder once.

\subsubsection{Bidirectional Style Transfer}
%cyclegan
\citeauthor*{CycleGAN} went a different way on style transfer and rely on a generative adversarial objective to identify style in an image.
Specifically, they transform images between any two domains $x \in \mathcal{X}$ and $y \in mathcal{Y}$, not just photos and artworks.
For this, they use two discriminators ($D_X$ and $D_Y$, one for each domain) as well as two transformation networks ($G: X \rightarrow Y$ and $H: Y \rightarrow X$).
Each translation network then transforms a given sample from one domain into the other and the discriminator assesses the result.

\begin{align}
    \L_{\text{adv}} = \log D_Y(y) + \log(1 - D_Y(G(x)))
\end{align}

Additionally, the transformed image is then transformed \textit{again} and compared to the original input, in what they call \textbf{cycle loss}.

\begin{align}
    \L_{\text{cycle}} = \norm{F(G(x)) - x}^2_2
\end{align}

In the end, \citeauthor*{CycleGAN} are able to stylize and de-stylize images with their networks $G$ and $H$.
The main novelty here though is the good stylization quality they achieve without any of the previously introduces style losses.
They have also shown one way, in which GANs are also capable of performing style transfer reasonably well.
Other notable efforts were \todo{list GAN-based style transfer efforts}.

\subsubsection{Adversarial Style Transfer}
Building on these GAN-based approaches, \citeauthor*{artsiom} improved the quality of adversarial style transfer and extended it to abstract styles as well.
They argue that ImageNet-based approaches inherently favor photorealistic styles through the data set that ImageNet has been trained on \cite{artsiom}.
Furthermore, approaches like cycleGAN suffer a similar fate as the back-transformation with cycle consistency opposes loss of detail in more abstract styles.

In order to retain content and global structure of an image, they introduce a fixed-point loss, which requires the stylized image to stay as-is when being re-stylized.
\begin{align}
    \L_{\text{content}} = \norm{E(G(E(x))) - E(x)}^2_2
\end{align}
To minimize this loss, the encoder must understand original content and stylized content.
They also implement a transformed reconstruction loss for better visual quality of the stylized image
\begin{align}
    \L_{\text{transformed}} = \norm{T(x) - T(G(E(x)))}^2_2
\end{align}
The results show good visual quality, especially concerning the details and loss of details for abstract styles.
Also this approach focusses on stylizing not only for a single image but the style of an artist in general.

\citeauthor*{dima} take this further and focus on stylizing different content specifically.
This means, a person is differently depicted than a tree, considering level of detail, colorscheme \etc. , which holds with real-world experience.
They achieve this by using the same fixed-point loss that \citeauthor*{artsiom} but combine it with a second update step.
In this second update step they require similar scenes to be placed closely in feature space and dissimilar scene to lie further apart.
They add a transformation block between encoder and decoder shape the feature space accordingly.

\subsubsection{Others}
There exist many other approaches that are capable of transferring style.
Some focus very heavily on stylization of portraits using self-attention modules \cite{ugatit}.
Others choose an approach similar to cycleGAN but add a shared encoding space for content and separate attribute spaces where style is encoded \cite{unit, munit, drit, drit++}.
With the latter ones mainly focussing on separating shape and appearance of images and recombining them arbitrarily.
One such example is taking the posture of a person in one image and combining it with the clothes and appearance of a person in another image.

The lines between these the applications and style transfer as it has been presented are blurry with many approaches that are capable of performing both.


\section{Painterly Rendering}
\labsec{PR}

%refer to painterly rendering review

Painterly rendering is a field of computer vision that focusses stylization of images on giving the impression that certain tools were used.
Most often this would be the looks of pencil drawings or brush stroke paintings as these looks are very distinctive.
Thus, painterly renderings rely on a brush stroke or a pencil line as its smallest unit which generates the data.
Style transfer, in contrast, still uses individual pixels as smallest unit.

The use of these larger units often comes with a level of abstraction which painterly rendering approaches often enforce explicitly though the coarseness said units.
One could see this as a hard regularization.
This is the reason why painterly rendering is rarely compared to style transfer (which has been described in the last section) as style transfer achieves this abstraction more implicitly.
For instance, an impressionistic style is often achieved by limiting the length and width of brush strokes in painterly renderings.

There are several different key approaches how to achieve this, which shall be presented chronologically.

The earliest approaches were stroke-based renderings, which artificially generate single strokes.
With such strokes at hand the challenge is to both improve the quality of these strokes and improve the algorithm which aligns them.
This field is especially close to the approach of this thesis.

Early filter-based rendering approaches then appeared with superior computational efficiency and ultimately led to style transfer and texture transfer.
This has already been explained int he previous section but shall be mentioned here to place it historically.

Lately, drawing networks revived stroke-based rendering with modern machine learning techniques.
The focus of this field is really to remove any hand-tuning of parameters that stroke based rendering required and imitate the way humans would draw.


\subsection{Stroke-Based Rendering}
Stroke-based rendering focuses on generating real-looking brush strokes and composing images with these.
Two aspects which are of importance in this thesis as well.

\citeauthor*{paintbynumbers} introduced stroke-based rendering with his seminal work \citetitle{paintbynumbers} \cite{paintbynumbers}.
In this work he presents two methods which would allow for reconstructing images in an abstracted representation.
His first approach is interactive and requires a user to click a certain points in an image to place a brush stroke there.
Then his software would automatically select a brush stroke color and size.
He proposed several ideas on how to align the brush strokes on the canvas.
Either users could do this on their own, or the orientation would be perpendicular to either the global gradient (uniform alignment) or local gradient (non-uniform alignment) of the image.
\begin{marginfigure}
    \includegraphics{haeberlihandcrafted}
    \includegraphics{haeberligradientdriven}
    \caption[]{Interactively painted images using \citeauthor*{paintbynumbers}'s method with a hand-selected orientation (\reffig{handselected}) and a gradient-driven orientation(\reffig{gradientdriven}).}
    \labfig{haeberliportrait}
\end{marginfigure}
\citeauthor*{paintbynumbers} even introduced the use of a scanned brush stroke texture in his algorithm.
\begin{marginfigure}
    \includegraphics{haeberlibrushstroke}
    \caption[]{Rendered image with a brush stroke texture}
    \labfig{haeberlitexture}
\end{marginfigure}

Additionally to his interaction-based approach, \citeauthor*{paintbynumbers} also introduced a relaxation based approach \cite{paintbynumbers}.
In this approach a given set of 100 brush strokes is iteratively perturbed.
If the perturbation minimizes the energy function (L2-distance), the perturbation is kept.
If not, another perturbation is applied.
\begin{marginfigure}
    \includegraphics{haeberlirelaxation}
    \caption[]{Image approximated by relaxation.}
    \labfig{haeberlirelaxation}
\end{marginfigure}
This is described by \citeauthor*{hertzmannreview} as trial-and-error algorithm and very similar to genetic algorithms which shall be explained later \cite{hertzmannreview}.

Based on \citeauthor*{paintbynumbers}'s work, other authors automated the process of stroke positioning.
At the same they improved various aspect of brush strokes which were well categorized in a review by \citeauthor*{PRreview}.
The relevant categories are position, path and orientation, length and width, ordering, and color \cite{PRveview}.

%litwinowicz
\citeauthor*{apple} proposed a straight-forward way of placing brush strokes evenly spaced over the entire image.
Parameters are then inferred similar to \citeauthor*{paintbynumbers}'s approach.
To give a more random feel to these brush strokes, the obtained parameters are randomly perturbed according to preset parameters.
Also, a weakness of orienting brush strokes perpendicular to the gradient is dealt with.
For large uniform areas with little to no gradient, the orientation could become arbitrary.
\citeauthor*{apple} proposes to refine the gradient field by interpolating between the boundaries of large uniformly colored areas.
Also, \citeauthor*{apple} introduced temporal coherence to brush strokes, which meant that brush strokes move with the optical flow between frame in a video.

%hertzmann
\citeauthor*{hertzmann} reformulated the problem as an energy minimization problem in two publications \cite{hertzmanreview, Hertzmann}.
\begin{align}
    E(R) & = E_{\text{recon}}(R) + E_{\text{area}}(R) + E_{\text{nstr}}(R) + E_{\text{cov}}(R) \\
    E_{\text{recon}}(R) & = \sum_{x \in W, y \in H} w_{\text{recon}}{}_{x, y} \norm{I_{x, y}(R) - I_{x, y}}^2_2 \\
    %E_{\text{area}}(R) & = w_{\text{area}} \sum_{r \in R} \text{area){r} \\
    %E_{\text{nstr}}(R) & = w_{\text{nstr}} (\text{\#R}) \\
    %E_{\text{cov}}(R) & = w_{\text{cov}} (\text{\#empty pixels in $I(R)$}) \\
\end{align}
where $R$ is a brush stroke representation, $I$ is the target image, and $I(R)$ is the rendered representation.
$x$ and $y$ are pixel positions.
By adjusting the different weights, properties of the rendering can be altered.
$w_{\text{recon}}$ can vary spatially and dictate how well the reconstruction must fit the original image in certain areas.

Additionally, he added long strokes as B-splines with arbitrary control points.
In contrast, \citeauthor*{paintbynumbers} and \citeauthor*{apple} argued that short straight brush strokes would aid the perception of impressionistic style.
Furthermore, \citeauthor*{Hertzmann} added advanced rendering for brush strokes with synthetic textures in his work \cite{Hertzmann}.

\citeauthor*{Hertzmann} combined all these aspects in his approach with advanced relaxation methods similar to \citeauthor*{paintbynumbers}.
Based on trial-and-error search, \citeauthor*{Hertzmann} samples a local region along the many dimensions that represent a single brush stroke.
The best set of parameters that minimizes the energy function $E$ is then picked as new parameters.

In order to achieve better visual quality \citeauthor*{Hertzmann} also employs a coarse-to-fine multi-layer rendering approach.
Hereby, he blurred the image in the early iterations of his method and fixed the brush size at a large value.
Blurring of the image would then be gradually reduced along with the brush size.
The final implementation is further optimized to accelerate the relaxation algorithm and allow for more brush strokes than \citeauthor*{paintbynumbers}'s approach.

Ultimately, \citeauthor*{Hertzmann} achieves respectable results with his approach and many ideas of this thesis can be found in his works as well.

\subsubsection{Stroke Rendering}
Parallel to the advancements in arranging brush strokes in stroke-based rendering, others improved the rendering quality for many different styles.
Most notably for this work, \citeauthor*{baxter} were the first to implement a full 3D simulation of a brush and the process of placing paint on a canvas.
They were even able to simulate mixing of color and different levels of dryness this way.
Ultimately, they were able to showcase a drawing by a real artist based on the simulations in real-time \cite{baxter}.

\citeauthor*{wetbrush} took this even further by including more accurate simulations of fluid dynamics and even single bristles in a brush stroke.
They were able to generate a 3D model of paint on canvas from this which they could make subject to different lighting \cite{wetbrush}.
\citeauthor*{adobe} tried to imitate this with the goal to achieve real-time simulation of the methods presented by \citeauthor*{wetbrush}.
Indeed, they were successful up to a point, where deviations to \cite{wetbrush} became visible for too many stacked layers of paint.




\subsection{Drawing Networks}
%gaining traction
Drawing networks belong to the realm of printerly rendering approaches, yet are rarely compared with their predecessors.

Some of the first approaches to rediscover this area used recurrent neural networks to imitate predefined stroke sequences \cite{sketchRNN, graves}.
Such approaches require paired data where the sequence for a sketch or handwriting is already known.

%SPIRAL
%SPIRAL was first
%relies on a graphics engine
%reinforcement learning
%the networks predicts strokes repeatedly based on the current state of the canvas.
%Discrimiantor critiques the result.
%Data like MNIST works well, CelebA is very blurry
%Main novelty is unsupervised training
\citetitle{SPIRAL} (SPIRAL) by \citeauthor*{SPIRAL} were the first to break this barrier and learn sequences without such paired data \cite{SPIRAL}.
They used a graphics engine and deep reinforcement learning, to train their network.
Based on the current state of the canvas, SPIRAL predicts an action in form of brush stroke parameters.
This process is iteratively applied to the canvas $N$ times.
Only at the end the result is evaluated in an adversarial fashion against the target image.
Notably, the renderer is seen as a black-box and SPIRAL is agnostic to the given interface.
Results show that this approach works well for simple stroke-based data such aas handwriting data sets.
Tests on images, such as the CelebA faces data set, show limitations as SPIRAL could only be trained for up to 20 strokes.


Other approaches followed suit, after \citeauthor*{SPIRAL}'s seminal work.
Many approaches were inpsired by \citeauthor{worldmodel}'s idea to encode an environment through an autoencoder into latent space.
A model is then trained on this fewer-dimensional latentspace representation instead of the real-world data.


%strokenet
\citeauthor*{strokenet} employ differentiable rendering combined with two encoders to predict a single brush stroke in a feed-forward manner \cite{strokenet}.
%position encoder
First they train a postion encoder (which actually is a decoder) that transforms an input position to a coordinate in a 64x64 matrix.
%generator
Then, they train the renderer with $n$ control points decoded by the positon encoder combined with additonal information about brush size and pressure.
The dataset is randomly generated with a fixed number of control points from a custom render engine.
%agent
In the end, the agent is trained, which encodes the current canvas as well as the target image to predict a set of parameters for a single brush stroke.

THeir method works notably better for predicting single brush strokes on empty canvas, that dollwing brush stroks for more complex shapes.
\citeauthor*{strokenet} also showed tests on images but got very poor results.

%neural painters
\citeauthor*{neuralpainters} chose a different approach to differentiable rendering.
He first trained a generator to decode action space input to brush stroke images in adversarial fashion.
Then he used an LSTM-based agent to predict a sequence of brush stroke parameters which are rendered by the generator.
As the whole pipeline is differentiable, \citeauthor{neuralpainters} was able to avoid the use of reinforcement learning.
Results show better image reconstruction than SPIRAL when using the generator, but slightly worse results when combining the obtained action with the original renderer.
\citeauthor{neuralpainters} also presents the capability of reconstructing content similar to \citeauthor{gatys} by minimizing the perceptual loss directly in action space with his network.

%learning2Paint
\citeauthor*{learning2paint} also employ a pre-trained decoder and use more advanced reinforcement learning techniques \cite{learning2paint}.
This allows allows their network to predict strokes for an arbitrary number of steps, where SPIRAL could only predict up to 16 strokes.
Their results show significantly more detail, especially as the number of strokes increases.

%LpaintB
All previous approaches work only on very limited resolutions.
This is why \citeauthor*{paintbot} extend reinforcement-based approaches to higher resolutions by using more advanced reinforcemnt learning rechniques \cite{paintbot, LpaintB}.
They do not use a differentiable renderer but a sliding-window approach which allows them to generalize to images of larger scales.
Notably, they train each network specifically for an image, as the training does not generalize well for different images.
They argue that this can be seen as a form of style transfer but such reconstructions mainly show a simple color shift.

\citeauthor*{LpaintB}'s approach is the closest approach to this thesis, considering the goal of their approach.
They are able to generate stroke-based renderings of high-resolution photographs as well as images of painting just shy of 1 megapixel.
Each network is specifically trained for a single target image, which takes at least $1 \si{\hour}$ to accomplish \cite{paintbot}.


\subsection{Genetic Algorithms}
Genetic algorithms are typically not closely associated with painterly rendering or drawing networks, even though they represent just a different approach to algorithms for this problem.

Genetic algorithms already perform a similar task in order to approximate images by other geometric shapes or even smaller photos (also known as the popular photo mosaic effect).
Starting with a random set of circles that are parameterized by their position, radius, and color, it then chooses the most successful samples and resamples in a region around these.
This process is repeated until a certain level of convergence is reached.

%refer to neural style transfer: a review

\begin{marginfigure}
    \includegraphics{genetic_starry_night}
    \caption[]{Starry Night approximated by a genetic algorithm using only circles. \url{https://effyfan.com/2018/03/02/w6-van-gogh-flowfield/}}
    \labfig{genetic}
\end{marginfigure}

As well as this does work, it is very much computationally expensive as most samples will not fit the image, thus searching for the small set of fitting shapes requires to evaluate all the wrong shapes as well.
Considering artworks, brushstrokes have many more degrees of freedom, and artworks usually consist of upwards of a few thousand brushstrokes.
Consequently, it would be considerably more challenging to apply to this problem until computational resources have become a few magnitudes more powerful.

\begin{figure}
    \includegraphics{photomosaic_starry_night}
    \caption[]{Photo mosaic of Starry Night using only images by the Hubble Space Telescope. \url{http://www.astro.uvic.ca/~alexhp/new/figures/starrynight_HST.001.jpg}}
    \labfig{photomosaic}
\end{figure}


\section{Conclusion}
Works like these show that there is interest in improving the missing details in style transfer.
Ultimately, it it would be desirable that the fields of style transfer and painterly rendering converge further.
Style transfer brings impressive perception of style.
Painterly rendering brings realistic composition and/or low-level details.

This thesis aims to improve reconstruction quality for painterly renderings without reinforcement learning techniques while at the same time generating an interpretable brush stroke representation.
