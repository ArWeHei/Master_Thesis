\setchapterpreamble[u]{\margintoc}
\chapter{Artistic Computer Vision}
\labch{ACV}
Applying computer vision techniques to images with artistic content is an interesting and challenging task at the same time.
Due to the often observed change in appearance in many artistic images (\eg abstraction), existing CV pipelines for \eg classification usually do not work as intended.
This makes it all the more interesting to see, how these pipelines are affected by this domain gap and whether this can be overcome.

Yet, at the same time artistic images combined with computer vision often results in interesting and fascinating applications for users.
A popular example for this is \textbf{style transfer}, where normal images are filtered such that they look like paintings.
Another example is art created with the help of neural networks \cite{CV_art}.
Research in the artistic field also incorporates techniques from computer vision and natural sciences to aid the detection of forgeries \cite{picassomatissefake} \cite{guardian_article}.

These examples show that computer vision can play an important role in the artistic domain and the other way around.
Since this thesis is deeply anchored at the intersection of both fields, different problems and their proposed solutions shall be presented.



%in SBR stroke is the basic unit texture synthesis fills each pixel individually.

%non-photorealistic rendering vs. photrealism
%clesly realted to texture synthesis and transfer
%games and movies benefit from this
%since the 1990's starting with artistic rendering of images (paint by numbers)

%with neural network more practical applicatin of style transfer have become popular:
% - funny iphone filters
% - translating images between other domains (google maps, satelites etc.) -> many possible applications
% quantitative style (materials used, color shoice etc.)
% qualitative style (how are people drawn etc.)

% historically there have been many different approaches

%color choice
%how to display something (is it unnaturally large or small ...)
%direction of brush strokes
%choice of material
%which details are kept which details are lost?

%all of this together makes style

%usually style transfer formulates this much simpler (maybe see gatys formulation?):
%create something that maintains the orginal content to some  degree but people would categorize together with other works of that 'style'
% style is rather something by what art can be grouped together 'easily'


%style transfer
%create IMAGE that catches style as well a possible, doesn't rally matter how, the pixel should make sense in the end

%painterly rendering
%create something that looks like is has been drawn with a paint brush (or any other such tool, e.g. has some gloss to it etc.)
%don't really concentrate on deeper style but just the apprearance

%drawing networks
%do not care as much about the apprearance or the style but how to compose the image in a different parameter space, FAST

%brush stroke extraction and image analysis
%get more information about the image, that can help to classify it, or identify it. Do not get all brush strokes but those that are characteristic

%drawing networks and painterly rendering are really much of the same with different focus on what matters.


%this work would fall into painterly rendering territory paired with brush stroke extraction


%features of DNN have been used to classify paintings

\section{Brushstroke Extraction}
%no updates in recent years
%relatively small field
%shift towards creating art rather than analyzing it
%what to do with the data besides identifying forgeries?
%computer aided forgery detection is standard
Brushstroke extraction is one way of retrieving data for image analysis.
Other approaches rely on analysis via convolutional neural networks \cite{conv_based} or texton statistics \cite{textons}.
Yet, the most impressive results have been obtained when analyzing drawings on the pencil-stroke level \cite{picassomatissefake}.
Similar (if few) approaches exist for paintings and show that such approaches are feasible and open the door for one major application of brush stroke extraction.

\citeauthor*{rhythmic} were the first to extract brush strokes from many paintings on a larger scale and combine it with forgery detection \cite{rhythmic}.
They introduced an algorithm that leverages edge detection as well as image segmentation to find connected components (regions enclosed by a continuous edge) in an image.
These connected components then serve as stroke candidates.
Each candidate is further processed to a single-line representation and then checked for various properties such as width-to-length ratio.
As a result \citeauthor*{rhythmic} claim that they detect ~60\% of brush strokes which were manually labeled on average.
They further obtain features from their line representation and are able to achieve respectable results in forgery detection and classification of artistic periods.
\begin{figure*}
    \subfloat[original]{%
        \includegraphics[width=.70\textwidth]{images/clogs}%
        \labfig{clogs}%
    }\qquad
    \subfloat[extracted]{%
        \includegraphics[width=.70\textwidth]{images/clogsbs}%
        \labfig{clogsbs}%
    }
    \caption{Original painting and extracted brushstrokes by \citeauthor*{rhythmic}}
    \labfig{rhythmic}
\end{figure*}

\citeauthor*{lamberti} later took an advanced approach and specifically modeled a brush stroke detection algorithm on the pixel level.
They propose to randomly sample evenly spaced pixels over the image and build up connected regions by adding adjacent pixels if they are similar enough.
This is repeated until the process comes to a stop or the brushstroke grows too large.
If the region is too large, the similarity metric is adjusted and the growing process starts from scratch.
If region are too small the whole seed is discarded.
Candidate brushstrokes of the correct size are then approximated with an ellipsoid and shape constraints as well as alignment with the connected region are tested.
\citeauthor*{lamberti} specifically fine-tune parameters in their algorithm to improve compliance with manually labeled brushstrokes.
In the end their results fare worse on metrics proposed by \citeauthor{rhythmic} such as detection rate but better in their own metrics.
Notably, \citeauthor*{lamberti} seems to be the only publication solely aimed at brushstroke extraction.

\begin{marginfigure}
    \subfloat{%
        \includegraphics[width=.90\textwidth]{images/cutout_orig}%
        \labfig{cutout_orig}%
    }\qquad
    \subfloat{%
        \includegraphics[width=.90\textwidth]{images/cutout_manual}%
        \labfig{cutout_manual}%
    }\qquad
    \subfloat{%
        \includegraphics[width=.90\textwidth]{images/cutout_li}%
        \labfig{cutout_li}%
    }\qquad
    \subfloat{%
        \includegraphics[width=.90\textwidth]{images/cutout_lamberti}%
        \labfig{cutout_lamberti}%
    }
    \caption{Comparison of techniques on a cut out patch from van Gogh's \textit{The Little Arsienne} (a). Manually labeled brushstrokes (b) \cite{rhythmic}, extracted brushstrokes by \citeauthor*{rhythmic} (c) \cite{rhythmic}, extracted brushstrokes by \citeauthor*{lamberti} (d) \cite{lamberti}. Source:~\cite{lamberti}}
    \labfig{cutout_strokes}
\end{marginfigure}


\section{Style Transfer}
\labsec{ST}

Style transfer is probably the most popular and advanced field in artistic computer vision.
The goal of style transfer is to generate an image representation which resembles the content of an input image but features the style of another image or image collection.
With the focus lying on altering the image on the pixel-level such that the stylized image and the style-template seem to belong to the same group of images (\eg images drawn by van Gogh)

This definition of style is hard to quantify, and thus relies on human judgement.
Subsequently, many publications offer expert- or non-expert-based quality estimates, which are hard to compare and thus shall not be further mentioned.
Then again, such an open definition of style allows the same techniques to be applied to related problems such as texture transfer or image-to-image translation.

The high quality that recent style transfer has achieved over the years requires to compare any stylization approach with this field as well.

\subsection{Early Approaches}
%IMAGE PROCESSING BASED TECHNIQUES
%filters or LIC as approaches

As one of the earlies approaches, \citeauthor*{imageanalogies} proposed a fast way of stylizing images by using trainable filters, while contemporary approaches relied on painterly rendering (see \refsec{PR}).
They call their technique 'image analogies' since the transformation between two training images is analogously applied to a test image.

Basically, for creating the impression of brush strokes, a photograph $A$ and a painting of that photograph $A'$ are required.
As this is rarely the case, \citeauthor*{imageanalogies} showed that using anisotropic diffusion works also reasonably well to generate $A$ from $A'$.
Then an algorithm searches locally around a spatial position $p$ for the best filter parameters $F(p)$ that transform $A(p)$ into $A'(p)$.
By using another search algorithm to match similar regions $B(p)$ and $A(p)$, $F(p)$ can then be used to transform $B(p)$ into $B'(p)$.
Thus $B'$ -- a stylized version of $B$ -- can be obtained by transforming $B \rightarrow B'$ analogously to $A \rightarrow A'$.

Other works like \citeauthor*{texturetransfer}~\cite{texturetransfer} have built on this basic idea and advanced versions of it \cite{fasttexturetransfer} \cite{texturetransfer}.
Besides only matching local regions according to their pixel values, \citeauthor*{texturetransfer} obtain a flow map (which they call 'directions'), that is based on the content image's gradient.
This flow is then also matched against the style image.
\begin{figure}
    %\includegraphics{texturetransfer.eps}
    %\fontsize{7pt}{7pt}\selectfont
    \begin{tiny}
    \def\svgwidth{\textwidth}
    \import{./images/}{texturetransfer.pdf_tex}
    \end{tiny}
    \caption{Target image $T$ is combined with a dictional map specially obtained from $T$ and a style image  $S$. The result maintains the direction's flow while presenting the texture from $S$. Source:~\cite{texturetransfer}}
    \labfig{texturetransfer}
\end{figure}
They are able to transfer texture (but only texture) from a given painting to a content image reasonably well.

%Lee et al 2010 transfer the texture of brush stroke onto an image much like style transfer (this is the intersection)
%painterly rendering is some kind of style transfer but shall be dealt with explicitly
%focus is on approaches that work only with pixels
%filters
%texton based approaches

%Some early approaches include histogram matching on linear ï¬lter responses [19] and non-parametric sampling [12, 15].
%       . Pyramid-based texture analysis/synthesis
%       Split and match: example-based adaptive patch sampling for unsupervised style transfer
%       Image quilting for texture synthesis and transfe
%low-level statistics and fail to perceive semantic structures



\subsection{Neural Style Transfer}
%A bit similar to transfer learning -> utilize the pretrained features especially of the early layers.
Despite some previous admirable results\cite{texturetransfer}, style transfer really gained traction in 2015 with the publication of \citetitle{gatys} by \citeauthor*{gatys}.
It was the first approach to transfer the style as more than just texture of one image to another and at the same time maintaining a high contextual fidelity.
In retrospective, this work really kicked started neural style transfer for the following years.
Because of the weight this publication has and some similarities in the approach of this thesis this work is presented more extensively.


\citeauthor*{gatys} themselves pinpoint the novelty of their approach as 'manipulations in feature spaces' as opposed to previous approaches that 'directly manipulate the pixel representation of an image'\cite{gatys}.
They use existing neural architectures and extract information in two separate ways, such that content and style can be separated.

%Previous works already used \textbf{perceptual loss} to visualize representations in classifiers 

%gain information on content in an image \cite{percep_loss}, or check whether two images have the same content \cite{other_percep_loss}.
\textbf{Perceptual loss} is based on the VGG-19 architecture \cite{VGG} which is a deep CNN trained for object classification on ImageNet \cite{imagenet}.
\citeauthor*{gatys} argue that the network's layer activations increasingly respond to the content when following the networks' hierarchy.
So much even, that it is possible to reconstruct the content of an image by imitating the activations of one such layer (same content does not imply pixel-wise identity).

For reconstruction of an image's content, gradient descent is performed on a white noise image.
The gradient descent aims to minimize the perceptual distance between the reconstruction and the target image.
Perceptual distance is defined as the L2-distance between the activations of two images in a deep layer of the VGG-network.

\marginnote{An image vector is often used to represent a 2-dimensional image as a single column vector such that spatial information is neglected at first. Yet, the position inside the image vector still corresponds directly to a specific position in the original 2D representation}
For an image vector $\vec{x}$ with $\vec{x} \in = \R^{M_0}, M_0 = H_x \dot W_x $, a layer $l$ of the network has $N_l$ feature maps of size $M_l$.
In this case $M_l$ is equal to the height times the width of the feature map of the $l$-th layer  $M_l = H_l \dot W_l $.
The activations of the $i$-th filter ($i \in N_l$) at position $j$ ($j \in M_l$) at layer $l$ can then be represented by a matrix $F(\vec{x})_{ij}^l \in \R^{N_l \times M_l}$.
The perceptual distance is then defined as
\begin{align}
    d_{\text{percep}}(\vec{x}, \vec{y}) = \sum_{i, j} (F(\vec{x})_{ij}^l - F(\vec{y})_{ij}^l)^2 = \norm{\tensor{F(\vec{x})}^l - \tensor{F(\vec{y})}^l}_2^2
\end{align}
, which allows to define the perceptual loss or content loss as 
\begin{align}
    \L_{\text{content}} = \frac{1}{2} \sum_{i, j} (F(\vec{x})_{ij}^l - F(\vec{y})_{ij}^l)^2 = \frac{1}{2} \norm{\tensor{F(\vec{x})}^l - \tensor{F(\vec{y})}^l}_2^2
\end{align}
\marginnote{The factor $\frac{1}{2}$ will cancel out when deriving $\L_{\text{content}}$ with respect to $F(\vec{x})_{ij}^l$.}
Minimizing the content loss between two images by using gradient descent restores the content of an image (see \reffig{content_style_loss}).
\begin{figure*}
    \includegraphics{content_style_loss}
    \caption{Reconstructions of content(bottom) and style(top) using different layers. Source:\cite{gatys}}
    \labfig{content_style_loss}
\end{figure*}

As approximating the content of an image is possible, the question is whether the same is possible for style.
\citeauthor*{gatys} again turn to a pre-trained VGG network for this.
They identify style as texture and thus search for a feature space that captures \textbf{texture} much like perceptual loss captures content.
Subsequently, \citeauthor*{gatys} propose the use of \textbf{Gram matrices} as they capture the correlations of feature-activations over their spatial extent.

The Gram matrix of a given matrix $\tensor{A}$ is the inner product of all column vectors in $\tensor{A}$.
\begin{align}
    G = \langle a_{i}, a_{j} \rangle = \tensor{A}^T \tensor{A} \text{ if $a_1$...$a_j$ are column vectors of $\tensor{A}$}
\end{align}
The resulting Gram matrix G now has the form $j \times j$ and captures texture information but no longer the global content.

Arguably, style/texture is very complex and exists at various scales at the same time which \citeauthor*{gatys} address by using many layers at the same time.
As these layers sit at increasing depths their field of view increases as well and each layer captures information at a different scale.
Early layers will tend to hold small scale information, later layers will hold larger scale information.

\citeauthor*{gatys} first compute the Gram matrices of each layer $l$ for both the target-style image and the current image.
Then they use the L2 distance metric to measure the discrepancy between them.
\begin{align}
    G(\vec{x})^l & = \frac{1}{(2 N_x^l M_x^l)^2} F(\vec{x})^l{}^T F(\vec{x})^l \\
    G(\vec{y})^l & = \frac{1}{(2 N_y^l M_y^l)^2} F(\vec{y})^l{}^T F(\vec{y})^l \\
    d_{\text{style}}^l(\vec{x}, \vec{y}) & = \norm{G(\vec{x})^l - G(\vec{y})^l}^2_2 \\
\end{align}
\marginnote{The denominator of $\frac{1}{4 N_x^l{}^2 M_x^l{}^2}$ is squared since the Gram matrix is the product of a matrix with its transposed self.}
The style distances at each layer are then weighted and summed up to make up the style loss:
\begin{align}
    \L_{\text{style}} = \sum_{l} w_l d_{\text{style}}^l(\vec{x}, \vec{y})
\end{align}
%again one can visualize the style/texture that is perceived
This style loss can again by used together with gradient descent in order to check whether it is possible to reconstruct the texture of an image much like the content of an image.
\reffig{content_style_loss} shows that it is in fact possible to reconstruct the texture of the image at various scales.
Specifically the local consistency of each texture becomes larger, the deeper the layer sits.

%mix content and style
\citeauthor*{gatys} now combine the losses for a content image $\vec{c}$ with a style image $\vec{s}$ and optimize $\vec{x}$ in the same way previous reconstructions have been obtained.
\begin{align}
    \L_{\text{total}} = \lambda_{\text{content}} \L_{\text{content}}(\vec{x}, \vec{c}) + \lambda_{\text{style}} \L_{\text{style}}(\vec{x}, \vec{s})
\end{align}
The result of this can be seen in \reffig{gatys_ST}
\begin{marginfigure}
    \includegraphics{gatys_ST}
    \caption{Style transfer examples by \citeauthor*{gatys}. Source:~\cite{gatys}}
    \labfig{gatys_ST}
\end{marginfigure}

\subsubsection{Follow-Up Research}
There has been some follow-up research on \citeauthor*{gatys}'s work which addresses mainly how the style loss works.

\citeauthor*{MMD} have shown that the style loss is equivalent to calculating the \textbf{maximum mean discrepancy (MMD)} between the features of each layer \cite{MMD}.
MMD is a test-statistic for a null hypothesis $p=q$ with data $X = \{x_i\}^n_{i=1}$, sampled from $p$, and $Y = \{y_i\}^n_{i=1}$, sampled from $q$, at hand.
It can be used as a difference measure and vanishes only if $p=q$.
\begin{align}
    \text{MMD}^2[X, Y] & = \frac{1}{n^2} \sum^n_{i=1} \sum^n_{i'=1} k(\vec{x}_i, \vec{x}_{i'}) \\
    & + \frac{1}{m^2} \sum^m_{j=1} \sum^m_{j'=1} k(\vec{y}_j, \vec{y}_{j'}) \\
    & - \frac{2}{nm} \sum^n_{i=1} \sum^m_{j=1} k(\vec{x}_i, \vec{y}_{j})
\end{align}
MMD can be based on different kernel functions $k$ and \citeauthor*{MMD} have shown that the style loss is equivalent to the squared MMD with a polynomial kernel.
Consequently they were able to show, that style transfer works with different kernel functions as well and even by explicitly matching the batch statistics (see \reffig{MMD}):
\begin{align}
    d_{\text{style}}^l(\vec{x}, \vec{y}) & = \frac{1}{N_l} \sum_{i = 1}^{N_l} \left( (\mu^i_{F(\vec{x})^l} - \mu^i_{F(\vec{y})^l}) + (\sigma^i_{F(\vec{x})^l} - \sigma^i_{F(\vec{y})^l}) \right)
\end{align}
\begin{figure}
    \includegraphics{MMD}
    \caption{Reconstructed textures for Starry Night using different kernel functions $k$ \cite{MMD}}
    \labfig{MMD}
\end{figure}

\citeauthor*{lendu} tested whether pre-trained weights play an important role when performing style transfer~\cite{lendu}.
He was able to show basic style transfer even with random initialized networks, but results vary widely depending on parameters of the random initialization.
Ultimately, it is possible so obtain some style transfer with this technique but the pre-trained weights seem to play an important role in stabilizing the reconstruction process.

\subsection{State of the Art}

\subsubsection{Real-Time Style Transfer}
%johnson
Following \citeauthor*{gatys} seminal work, others have followed suit in trying to stylize images with neural networks.
\citeauthor*{johnson} were the first to use the same losses but train a feed-forward architecture \cite{johnson}.
They were able to significantly speed up the stylization process like this as stylization is performed in a single feed-forward pass instead of a lengthy gradient descent optimization.
Ultimately, this enabled them to generate stylized images in real-time from a given content image, using a deep residual convolutional neural network.
\begin{figure*}
    \includegraphics{johnson_net}
    \caption{Training set-up by \citeauthor*{johnson}. \cite{johnson}}
    \labfig{johnson_net}
\end{figure*}

%adain
\subsubsection{Arbitrary \& Universal Style Transfer}
\citeauthor*{AdaIN} used a different feed-forward approach for arbitrary style.
They first encode an arbitrary style image $\vec{s}$ as well as a content image $\vec{c}$ using a pre-trained VGG network.
This allows them to obtain the activations at a very deep layer of the network $F^l(\vec{s})$ and $F^l(\vec{c})$.
Then they compute the second order statistics for both $\mu_F^l(\vec{s}), \sigma_F^l(\vec{s})$ and $\mu_F^l(\vec{c}), \sigma_F^l(\vec{c})$.
Using adaptive instance normalization (AdaIN), they rescale the content activations such that they match the statistics of the style activations.
\begin{align}
    F'^l = \sigma_F^l(\vec{s}) \frac{F^l(\vec{c} - \mu_F^l(\vec{c})}{\sigma_F^l(\vec{c})} + \mu_F^l(\vec{s}) 
\end{align}
Finally, they train a decoder that minimizes the style and content loss, as they have been proposed by \citeauthor*{gatys}.
They achieve comparable results to other style transfer approaches at a similar speed to \citeauthor*{johnson} while allowing for any target style eventhough only training only once.
\begin{figure*}
    \includegraphics{adain_net}
    \caption{Training set-up by \citeauthor*{AdaIN}. \cite{AdaIN}}
    \labfig{AdaIN}
\end{figure*}

%wct
A similar approach by \citeauthor*{WCT} relies on matching the covariance and the mean of content and style activations.
They do that through what they call 'whitening and coloring transform' \cite{WCT}.
First they whiten $F^l(\vec{c})$ into $\hat{F}^l$ such that $\hat{F}^l \hat{F}^l{}^T = I$
\begin{align}
    \hat{F}^l = E_c D_c^{-\frac{1}{2}} E_c^T (F^l(\vec{c}) - \mu_c)
\end{align}
Where $D_c$ is the diagonal matrix of eigenvalues of the covariance matrix and $E_c$ is the respective orthogonal matrix of eigenvectors.
such that
\begin{align}
    F^l(\vec{c}) F^l(\vec{c})^T = E_c D_c E_c^T
\end{align}
Then coloring is performed by rescaling the whitened representation $\hat{F}^l$ into $\tilde{F}^l$
\begin{align}
    \tilde{F}^l = E_s D_s^{\frac{1}{2}} E_s^T \hat{F}^l + \mu_s \\
    F^l(\vec{s}) F^l(\vec{s})^T = E_s D_s E_s^T
\end{align}
The resulting $\tilde{F}^l$ is then decoded with a pre-trained decoder to render the final stylized result.
\citeauthor*{WCT} use pre-train the decoder solely on natural images and perceptual loss and reconstruction loss as objective.
Additionally, they introduce a pipeline that performs style transfer on multiple scales sequentially.
They achieve good results in real-time with just training the decoder once.

\subsubsection{Bidirectional Style Transfer}
%cyclegan
\citeauthor*{CycleGAN} went a different way on style transfer and rely on a generative adversarial objective to identify style in an image.
Specifically, they transform images between any two domains $x \in \mathcal{X}$ and $y \in \mathcal{Y}$, not just photos and artworks.
For this, they use two discriminators ($D_X$ and $D_Y$, one for each domain) as well as two transformation networks ($G: X \rightarrow Y$ and $H: Y \rightarrow X$).
Each translation network then transforms a given sample from one domain into the other and the discriminator assesses the result.
\begin{align}
    \L_{\text{adv}} = \log D_Y(y) + \log(1 - D_Y(G(x)))
\end{align}
Additionally, the transformed image is then transformed \textit{again} and compared to the original input, in what they call \textbf{cycle loss}.
\begin{align}
    \L_{\text{cycle}} = \norm{F(G(x)) - x}^2_2
\end{align}
In the end, \citeauthor*{CycleGAN} are able to stylize and de-stylize images with their networks $G$ and $H$.
The main novelty here though is the good stylization quality they achieve without any of the previously introduces style losses.
They have also shown one way, in which GANs are also capable of performing style transfer reasonably well.

\subsubsection{Adversarial Style Transfer}
Building on these GAN-based approaches, \citeauthor*{artsiom} improved the quality of adversarial style transfer and extended it to abstract styles as well.
They argue that ImageNet-based approaches inherently favor photorealistic styles through the data set that ImageNet has been trained on \cite{artsiom}.
Furthermore, approaches like cycleGAN suffer a similar fate as the back-transformation with cycle consistency opposes loss of detail in more abstract styles.

In order to retain content and global structure of an image, they introduce a fixed-point loss, which requires the stylized image to stay as-is when being re-stylized.
\begin{align}
    \L_{\text{content}} = \norm{E(G(E(x))) - E(x)}^2_2
\end{align}
To minimize this loss, the encoder must understand original content and stylized content.
They also implement a transformed reconstruction loss for better visual quality of the stylized image
\begin{align}
    \L_{\text{transformed}} = \norm{T(x) - T(G(E(x)))}^2_2
\end{align}
The results show good visual quality, especially concerning the details and loss of details for abstract styles.
Also this approach focusses on stylizing not only for a single image but the style of an artist in general.

\citeauthor*{dima} take this further and focus on stylizing different content specifically.
This means, a person is differently depicted than a tree, considering the level of detail, colorscheme \etc. , which holds with real-world experience.
They achieve stylization that resembles this behavior by using the same fixed-point loss that \citeauthor*{artsiom} used but combine it with a second update step.
In this second update step they require similar scenes to be placed closely in feature space and dissimilar scene to lie further apart.
They add a transformation block between encoder and decoder shape the feature space accordingly.

\subsubsection{Others}
There exist many other approaches that are capable of transferring style -- often as a byproduct of texture transfer or image-to-image translation.
Some focus very heavily on stylization of portraits using self-attention modules \cite{ugatit}.
Others choose an approach similar to cycleGAN but add a shared encoding space for content and separate attribute spaces where style is encoded \cite{munit, drit, drit++}.
With the latter ones mainly focussing on separating shape and appearance of images and recombining them arbitrarily.
One such example is taking the posture of a person in one image and combining it with the clothes and appearance of a person in another image.


\section{Painterly Rendering}
\labsec{PR}

%refer to painterly rendering review

Painterly rendering is a field of computer vision that understand stylization of images as giving the impression that a certain medium was used.
Most often this would be the looks of pencil drawings or paintings as their looks are very distinctive, especially when compared to regular photographs.
Thus, painterly renderings rely on a brushstroke or a pencil line as its smallest unit from which the data is generated.
Style transfer, in contrast, relies on pixels as its smallest unit.

The use of these larger units automatically comes with a level of abstraction which painterly rendering approaches often reinforce though the adjusting the coarseness of said units.
One could see this as a hard regularization.
Style transfer (see \refsec{ST}), on the other hand, aims to learn this abstraction.
Still, an impressionistic style in painterly rendering could be achieved by limiting the length and width of brush strokes in painterly renderings.

This thesis' approach follows similar regularization to painterly rendering and thus different key approaches shall be presented.

The earliest approaches to painterly rendering were stroke-based renderings, which artificially generate single brushstrokes.
The challenge then becomes twofold 1) improve the quality of these strokes and 2) improve the algorithm which aligns them.
Both problems will again come up in this thesis.

Early filter-based rendering approaches then appeared with superior computational efficiency and ultimately led to style transfer and texture transfer.
This has already been explained in the previous section but shall be mentioned here to place it historically and contextually.

Lately, drawing networks revived stroke-based rendering with modern machine learning techniques.
The focus of this field is really to remove any hand-tuning of parameters that stroke-based rendering required before and to a certain degree imitate the way humans would draw.

\subsection{Stroke-Based Rendering}

\citeauthor*{paintbynumbers} introduced stroke-based rendering with his seminal work \citetitle{paintbynumbers} \cite{paintbynumbers}.
In this work he presents two methods which would allow for reconstructing images in an abstracted representation.
His first approach is interactive and requires a user to click a certain points in an image to place a brush stroke there.
Then his software would automatically select a brush stroke color and size.
He proposed several ideas on how to align the brush strokes on the canvas.
Either users could do this on their own, or the orientation would be perpendicular to either the global gradient (uniform alignment) or local gradient (non-uniform alignment) of the image.
\begin{marginfigure}
    \subfloat{%
        \includegraphics{haeberlihandcrafted}
        \labfig{handselected}%
    }\qquad
    \subfloat{%
        \includegraphics{haeberligradientdriven}
        \labfig{gradientdriven}%
    }
    \caption{Interactively painted images using \citeauthor*{paintbynumbers}'s method with a hand-selected orientation (a) and a gradient-driven orientation(b).}
    \labfig{haeberliportrait}
\end{marginfigure}
\citeauthor*{paintbynumbers} even introduced the use of a scanned brush stroke texture in his algorithm.
\begin{marginfigure}
    \includegraphics{haeberlibrushstroke}
    \caption{Rendered image with a brush stroke texture}
    \labfig{haeberlitexture}
\end{marginfigure}

Additionally to his interaction-based approach, \citeauthor*{paintbynumbers} also introduced a relaxation based approach \cite{paintbynumbers}.
In this approach a given set of 100 brush strokes is iteratively perturbed.
If the perturbation minimizes the energy function (L2-distance), the perturbation is kept.
If not, another perturbation is applied.
\begin{marginfigure}
    \includegraphics{haeberlirelaxation}
    \caption{Image approximated by relaxation.}
    \labfig{haeberlirelaxation}
\end{marginfigure}
This is described by \citeauthor*{hertzmannreview} as trial-and-error algorithm and very similar to genetic algorithms \cite{hertzmannreview}.

Based on \citeauthor*{paintbynumbers}'s work, other authors automated the process of stroke positioning.
At the same they improved various aspect of brush strokes which were well categorized in a review by \citeauthor*{PRreview}.
The relevant categories are position, path and orientation, length and width, ordering, and color \cite{PRreview}.

%litwinowicz
\citeauthor*{apple} proposed a straight-forward way of placing brush strokes evenly spaced over the entire image.
Parameters are then inferred similar to \citeauthor*{paintbynumbers}'s approach.
To give a more random feel to these brush strokes, the obtained parameters are randomly perturbed according to preset parameters.
Also, a weakness of orienting brush strokes perpendicular to the gradient is dealt with.
For large uniform areas with little to no gradient, the orientation could become arbitrary.
\citeauthor*{apple} proposes to refine the gradient field by interpolating between the boundaries of large uniformly colored areas.
Also, \citeauthor*{apple} introduced temporal coherence to brush strokes, which meant that brush strokes move with the optical flow between frame in a video.

%hertzmann
\citeauthor*{hertzmann} reformulated the problem as an energy minimization problem in two publications \cite{hertzmannreview, Hertzmann}.
\begin{align}
    E(R) & = E_{\text{recon}}(R) + E_{\text{area}}(R) + E_{\text{nstr}}(R) + E_{\text{cov}}(R) \\
    E_{\text{recon}}(R) & = \sum_{x \in W, y \in H} w_{\text{recon}}{}_{x, y} \norm{I_{x, y}(R) - I_{x, y}}^2_2
    %E_{\text{area}}(R) & = w_{\text{area}} \sum_{r \in R} \text{area){r} \\
    %E_{\text{nstr}}(R) & = w_{\text{nstr}} (\text{\#R}) \\
    %E_{\text{cov}}(R) & = w_{\text{cov}} (\text{\#empty pixels in $I(R)$}) \\
\end{align}
where $R$ is a brush stroke representation, $I$ is the target image, and $I(R)$ is the rendered representation.
$x$ and $y$ are pixel positions.
By adjusting the different weights, properties of the rendering can be altered.
$w_{\text{recon}}$ can vary spatially and dictate how well the reconstruction must fit the original image in certain areas.

Additionally, he added long strokes as B-splines with arbitrary control points.
In contrast, \citeauthor*{paintbynumbers} and \citeauthor*{apple} argued that short straight brush strokes would aid the perception of impressionistic style.
Furthermore, \citeauthor*{Hertzmann} added advanced rendering for brush strokes with synthetic textures in his work~\cite{Hertzmann}.

\citeauthor*{Hertzmann} combined all these aspects in his approach with advanced relaxation methods similar to~\citeauthor*{paintbynumbers}.
Based on trial-and-error search, \citeauthor*{Hertzmann} samples a local region along the many dimensions that represent a single brush stroke.
The best set of parameters that minimizes the energy function $E$ is then picked as new parameters.

In order to achieve better visual quality \citeauthor*{Hertzmann} also employs a coarse-to-fine multi-layer rendering approach.
Hereby, he blurred the image in the early iterations of his method and fixed the brush size at a large value.
Blurring of the image would then be gradually reduced along with the brush size.
The final implementation is further optimized to accelerate the relaxation algorithm and allow for more brush strokes than \citeauthor*{paintbynumbers}'s approach.

Ultimately, \citeauthor*{Hertzmann} achieves respectable results with his approach and many ideas of this thesis can be found in his works as well.

\subsubsection{Stroke Rendering}
Parallel to the advancements in arranging brush strokes in stroke-based rendering, others improved the rendering quality for many different styles.
Most notably for this work, \citeauthor*{baxter} were the first to implement a full 3D simulation of a brush and the process of placing paint on a canvas.
They were even able to simulate mixing of color and different levels of dryness this way.
Ultimately, they were able to showcase a drawing by a real artist based on the simulations in real-time \cite{baxter}.

\citeauthor*{wetbrush} took this even further by including more accurate simulations of fluid dynamics and even single bristles in a brush stroke.
They were able to generate a 3D model of paint on canvas from this which they could make subject to different lighting \cite{wetbrush}.
\citeauthor*{adobe} tried to imitate this with the goal to achieve real-time simulation of the methods presented by \citeauthor*{wetbrush}.
Indeed, they were successful up to a point, where deviations to \cite{wetbrush} became visible for too many stacked layers of paint~\cite{adobe}.


\subsection{Drawing Networks}
%gaining traction
Drawing networks belong to the realm of painterly rendering approaches, yet are rarely compared with their predecessors as their architectures differ significantly.
Drawing network often rely on recurrent architectures (see \refch{MLandCV}) or iterative approaches where the current state of the drawing is taken into account along the target-image.

Some of the first approaches used the mentioned recurrent neural networks to imitate predefined stroke sequences \cite{sketchRNN, graves}.
Such approaches require paired data where the sequence for a sketch or handwriting is already known.
Obviously, this data is hard to acquire such that SPIRAL was the first publication to gain more attention in this field.

\citetitle{SPIRAL} (SPIRAL) by \citeauthor*{SPIRAL} was the first approach to learn drawing sequences without such paired data \cite{SPIRAL}.
They used a graphics engine and deep reinforcement learning, to train their network.
Based on the current state of the canvas, SPIRAL predicts an action in form of brush stroke parameters.
This process is iteratively applied to the canvas $N$ times.
Only at the end the result is evaluated in an adversarial fashion against the target image.
Notably, the renderer is seen as a black-box and SPIRAL is agnostic to the given interface.
Results show that this approach works well for simple stroke-based data such aas handwriting data sets.
Tests on images, such as the CelebA faces data set, show limitations as SPIRAL could only be trained for up to 20 strokes.


Other approaches followed suit, after \citeauthor*{SPIRAL}'s seminal work.
They would often combine the idea behind SPIRAL with \citeauthor{worldmodel}'s idea to encode an environment through an autoencoder into latent space.
A model is then trained on this fewer-dimensional latent space representation instead of the real-world data.
\citeauthor{worldmodel} call their idea a 'world model'.

%strokenet
\citeauthor*{strokenet} employ differentiable rendering combined with two encoders to predict a single brush stroke in a feed-forward manner \cite{strokenet}.
%position encoder
First they train a postion encoder (which actually is a decoder) that transforms an input position to a coordinate in a 64x64 matrix.
%generator
Then, they train the renderer with $n$ control points decoded by the positon encoder combined with additonal information about brush size and pressure.
The dataset is randomly generated with a fixed number of control points from a custom render engine.
%agent
In the end, the agent is trained, which encodes the current canvas as well as the target image to predict a set of parameters for a single brush stroke.

Their method works notably better for predicting single brush strokes on empty canvas, that dollwing brush stroks for more complex shapes.
\citeauthor*{strokenet} also showed tests on images but got very poor results.

%neural painters
\citeauthor*{neuralpainters} chose a different approach to differentiable rendering.
\marginnote{Long short-term memory (LSTM) is a basic building block in recurrent neural networks which allows to store information for many steps.
It specifically learns which inputs to look at, when to forget something and which outputs to generate from the current state of the network.}
He first trained a generator to decode action space input to brush stroke images in adversarial fashion.
Then he used an LSTM-based agent to predict a sequence of brush stroke parameters which are rendered by the generator.
As the whole pipeline is differentiable, \citeauthor{neuralpainters} was able to avoid the use of reinforcement learning.
Results show better image reconstruction than SPIRAL when using the generator, but slightly worse results when combining the obtained action with the original renderer.
\citeauthor{neuralpainters} also presents the capability of reconstructing content similar to \citeauthor{gatys} by minimizing the perceptual loss directly in action space with his network.

%learning2Paint
\citeauthor*{learning2paint} also employ a pre-trained decoder and use more advanced reinforcement learning techniques \cite{learning2paint}.
This allows allows their network to predict strokes for an arbitrary number of steps, where SPIRAL could only predict up to 16 strokes.
Their results show significantly more detail, especially as the number of strokes increases.

%LpaintB
All previous approaches work only on very limited resolutions.
This is why \citeauthor*{paintbot} extend reinforcement-based approaches to higher resolutions by using more advanced reinforcemnt learning rechniques \cite{paintbot, LpaintB}.
They do not use a differentiable renderer but a sliding-window approach which allows them to generalize to images of larger scales.
Notably, they train each network specifically for an image, as the training does not generalize well for different images.
They argue that this can be seen as a form of style transfer but such reconstructions mainly show a simple color shift.

\citeauthor*{LpaintB}'s approach is another approach close to this thesis, considering the goal of their approach.
They are able to generate stroke-based renderings of high-resolution photographs as well as images of painting just shy of 1 megapixel.
Each network is specifically trained for a single target image, which takes at least $1 \si{\hour}$ to accomplish \cite{paintbot}.


\subsection{Genetic Algorithms}
Genetic algorithms are typically not closely associated with painterly rendering or drawing networks, even though they represent just a different approach to algorithms for this problem.

Genetic algorithms already perform a similar task in order to approximate images by other geometric shapes or even smaller photos (also known as the popular photo mosaic effect).
Starting with a random set of circles that are parameterized by their position, radius, and color, it then chooses the most successful samples and resamples in a region around these.
This process is repeated until a certain level of convergence is reached.

%refer to neural style transfer: a review

As well as this does work, it is very much computationally expensive as most samples will not fit the image, thus searching for the small set of fitting shapes requires to evaluate all the wrong shapes as well.
Considering artworks, brushstrokes have many more degrees of freedom, and artworks usually consist of upwards of a few thousand brushstrokes.
Consequently, it would be considerably more challenging to apply to this problem until computational resources have become a few magnitudes more powerful.

\begin{figure}
    \includegraphics{photomosaic_starry_night}
    \caption{Photo mosaic of Starry Night using only images by the Hubble Space Telescope. \url{http://www.astro.uvic.ca/~alexhp/new/figures/starrynight_HST.001.jpg}}
    \labfig{photomosaic}
\end{figure}



\section{Conclusion}
Works like these show that there is interest in improving the missing details in style transfer.
Ultimately, it would be desirable that the fields of style transfer and painterly rendering converge further.
Style transfer brings impressive perception of style.
Painterly rendering brings realistic composition and/or low-level details.

This thesis aims to improve reconstruction quality for painterly renderings without reinforcement learning techniques while at the same time generating an interpretable brush stroke representation.
