\setchapterpreamble[u]{\margintoc}
\chapter{Machine Learning and Computer Vision}
\labch{MLandCV}
In this chapter the theoretical backgrounds of Neural Networks ought to be explored.

As Masters thesis is (partly) meant to belong to the field of physics a physical/biological motivation will set the foundation for the theoretical background.

Thereafter the bridge to neural networks in general will be made followed by neural networks that are strongly associated to the field of computer vision.

\section{Artificial Neural Networks}

\subsection{Neurobiological Inspiration}
%Nature is often inspiration (birds and planes)

%arguabnly implementations deviate from nature (plane and bird) and people say to refrain from using these analogies.

%scalability

%neurophysiologist Warren McCulloch and the mathematician Walter Pitts invented first ANN
%simplified computational model of how biological neurons might work together

%histrocal rollercoaster of interest (AI winter) ?

%short electrical impulses are signals

%it seems that neurons are often organized in consecutive layers

As the name "Neural Network" already states, the inspiration for this field comes from nature's own structure of the brain.

Within the brain, specific cells called neurons are connected to each other through dendrites, synapses and axons.
What makes neurons special, is their ability to propagate messages between one another through their connections.

If a neuron receives a signal through one of its axons, the signal is accumulated with other incoming signals in the cells membrane potential.
As more signals come in the membrane potential can increase or decrease depending on the nature of the signal itself.
If the neuron's membrane potential exceeds a certain threshold, the potential at the membrane rapidly rises and falls, which is called an action potential or "spike" due to the behavior of the potential over time.

This spike propagates through the whole neuron and its dendrites towards the synapses. The synapses are the connections between the firing neuron's dendrites and the  receiving neuron's axons.
At the synapse, the action potential is converted from an electrical signal to a chemical signal and back to an electrical signal as it enters the axon of an adjacent neuron, where again signals from all axons are accumulated.
The synapses act as weights to the signal, and are capable to propagate the signal either as a positive or negative potential to the axon.

All of this happens billions and billions of times in each human's brain at every given second, which makes this a highly volatile process.

Obviously computers are (currently) not able to follow the exact same structure, -- even though there are attempts at closing this gap \cite{brainscales} -- as they are built on different principles.
Besides computers working with digital signals and data, the brain, as all of nature works in an analog fashion.
This comes with further differences such as the very high level of recurrency and  parallelism that is present in the brain when compared to a computer which works in a rather serial fashion.

Nonetheless, when stepping one step back and looking at local structures instead of the bits they are made of comparisons between computers and the brain seem more likely.

But should the basic element of a neural network be designed?
Ideally, it should be capable of doing the same things as a normal neuron does.
Additionally, it should work very efficiently and fit the digital nature of a computer.

When looking at a neuron more precisely,...
%start with a very much simplified model McCulloch and Pitts
%binary input and binary output
%neurons is activated if a number of inputs is active
%neurons can also be inhibitant
%focussed on logical computations

%explain the logic from neuron to perceptron

%perceptron invented by Frnak Rosenblatt (linear threshold unit)

\subsubsection{Activation Functions}
%weighted sum and step function (heaviside)

\subsection{Mathematical Inspiration}
%linear binary classification (like linear SVM)
%find values for weights

%perceptron is one layer of LTUs
%weights shoul dbe reinforced when they lead to the correct output



While this introduction to Perceptrons is possible they can be derived more easily as linear classifiers.....
%algebraic fomralism
%bias trick


\subsubsection{Activation Functions}
Activation functions have already been described 

\section{Loss Function}
%What is the goal of training?
%hint at backpropagation but this will come in later.

\section{Fully Connected Neural Networks}
\subsection{Multi-Layer Perceptron}

%inability to solve XOR problem
%thus stack multiple 

%hiddenlayers

After motivating the perceptron, this section will focus on combining multiple layers of perceptrons, which make up the \textbf{multi layer perceptron (MLP)}.
The MLP combines several perceptron layers in series which is the first step towards what is described as deep neural networks.
In a first thought experiment the original input layer and the output layer shall be separated by an additional layer that works quite simialrly to the input layer.
Instead of receiving a signal 'from outside' the added layer takes the input layer's activations as a new input signal ....


\subsection{Deep Neural Networks}
%two or more hidden layers -> deep neural network
Deep neural networks repeat the step from the previous section over and over again which results in a \textbf{deep} architecture, where deep refers to the number of layers....


\section{Convolutional Neural Networks}
\subsection{Convolutions}
%mathematical explanation of what a convolution does
%advantage of convolutions as they use fewer weights
%unified behavior over space

%process information hierarchically
%each layer is a collection of image filters

\subsection{Normalization}
%Motivate by facilitating training or look into book

\subsection{Regularization}
%addition to loss but just keep training on track

