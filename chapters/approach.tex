\setchapterpreamble[u]{\margintoc}
\chapter{Approach}
\labch{Approach}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Motivation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Motivation}
\labsec{motivation}

%previous approaches have several problems
%-very low resolution only (learning2paint)
%-no brush strokes (style transfer)
%-bad reconstruction(SPIRAL)
%-hand-crafted/do not generalize (painterly rendering)
%-bad brush storkes (brushs troke extraction)
%-train whole network for a single image(drawingnet)
%-too inefficient (genetic algorithms)
%
%pick an orthogonal approach to drawing networks
%seems as a step back but could show whether it is possible at all
%emphasize more on the decoder quality than others did
%
%Goal is twofold: extract brush strokes is first
%see whether this stylizes normal images as well.

This thesis main goal is to extract a set of brushstrokes, that collectively reconstructs the image of a given painting. \\
The way these brushstrokes have to fit together creates dependencies between them.
It is important where brushstrokes intersect or whether they run in parallel.
Therefore, a brushstroke can not be placed without considering other brushstrokes in the same region.

A simple yet similar example is the game of Tangram.
\begin{marginfigure}
    \includegraphics{tangram}
    \caption[]{An Example of Tangram.}
    \labfig{tangram}
\end{marginfigure}
Tangram is a Chinese puzzle game that has the objective of replicating a given silhouette with a set of 7 unique shapes.
All of these shapes have to be used and they may not overlay or be cut.
To solve the puzzle it is necessary to consider rather all parts together than a single part at a time.
Changing the position or rotation of one shape has consequences for all other shapes.\\
Quite similarly, the parameters of one brushstroke affects all nearby brushstrokes as well.
The fact that brushstrokes can overlap, have different colors and complex paths makes the matter even worse.

Thus, existing approaches are currently not capable of solving this complex task sufficiently.
\refch{ACV} presented many such methods which either aim at stylizing images or extracting brushstrokes.\\
Those which extract brushstrokes, are only capable of doing so for about half of all brushstrokes in a painting.
Such shortcomings are acceptable for further analysis, which prefers few accurate brushstrokes over many inaccurate ones.
However, it will not suffice for reconstructing the entire image.\\
Neural style transfer, which builds on filter-based approaches, is generally not suited for this problem.
Since a set of pixels is directly transformed into another set of pixels, there is no brushstroke data available.\\
Painterly renderings do not suffer from such problems.
They are inherently based on using brushstrokes to generate an entire image.
Most of these approaches focus on stylizing images, but few have tried to imitate paintings as well~\cite{paintbot,lpaintb}.\\
Both approaches trained recurrent neural networks specifically to reconstruct a single image.
The resulting approximations of paintings would resemble the original input, but not on a brushstroke level.
Even though they claim, that the trained neural network could be reused for style transfer, the use of a prediction network is likely a burden to the approach.
Ultimately, a set of parameters is sought and training a network to predict these parameters adds overhead in most scenarios.
Having such a network only really makes sense if it can be reused on many images.

This thesis will present an approach which is orthogonal to many recent implementations of drawings networks and painterly rendering.
Instead of training a network to predict the brushstroke parameters, the parameters are optimized directly.

Early painterly rendering approaches built on a similar set-up.
They would formulate an energy function to describe how well the parameters describe the image~\cite{hertzmann_review}.
Finding a minimum for these energy functions requires advanced searching algorithms like evolutionary algorithms~\cite{genetic_algo}.
\begin{marginfigure}
    \includegraphics{genetic_starry_night}
    \caption[]{'The Starry Night' approximated by a genetic algorithm using only circles. \url{https://effyfan.com/2018/03/02/w6-van-gogh-flowfield/}}
    \labfig{genetic}
\end{marginfigure}
Albeit being capable of reconstructing images using pretty much any shape (also called unit), they have a major draw-back.
Such search algorithms are often inefficient since they are based on random sampling.
The sampling becomes even more inefficient if the number of degrees of freedom per unit is large, as it is the case for brushstrokes.

A solution to this problem is featured in many recently published approaches that use drawing networks.
Some reference \citeauthor*{worldmodel}'s idea of a world model~\cite{neuralpainters, learning2paint, strokenet} and present what is called a \textbf{neural renderer}.\\
Such a neural renderer makes the process of placing a brushstroke on canvas differentiable.
Thereby, a prediction network can learn easier compared to an approach without such a neural renderer.\\
Combining a differentiable renderer with a direct optimization-based approach would mitigate the inefficiency which other approaches suffered from.
Instead of randomly searching for minima, it would become possible to follow the gradient towards a minimum.
Thus, the approach of this thesis will consist of two components. \\
\begin{enumerate}
    \item A differentiable renderer which can generate images of brushstrokes from a parameter representation.
    \item An optimization procedure that iteratively approximates an image through brushstroke representations.
\end{enumerate}
Notably, \citeauthor*{gatys}'s approach to style transfer uses a similar set-up but optimizes pixels instead of brushstroke parameters.

The following two sections will present how each component of the approach has been realized.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Neural Renderer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Neural Renderer}
\labsec{NeuralRend}

Generating brushstrokes artificially is a complex task.
There exist plenty of applications that are dedicated to this.
It is still possible to discern real brushstrokes from application-generated ones, even though some research is bridging this gap~\cite{wetbrush}.\\
It would be even harder to think of a differentiable way to generate brushstrokes from a set of parameters.

Neural networks solve this issue by finding a differentiable pipeline through training.
Previous works have shown that neural networks are capable of conditionally generating high-resolution and high-quality images and neural networks are inherently differentiable.
Therefore, it requires only data of brushstrokes and their parameters to train the network.\\
As previously stated, many already did so.
However, these approaches do not focus on the quality of the renderer.
Thus, they use simple painting applications like \citetitle{libmypaint} to generate the training data.
In this approach there shall be more emphasis on the render quality.

\subsection{Data Set}
\labsubsec{dataset}
There is no data sets openly available for this task
Therefore, a data set must be created explicitly for this approach, for which a painting tool (or something similar) has to be picked.\\
Three qualities are important when is comes to creating the data set:
\begin{enumerate}
    \item Suitable data format: As brushstrokes usually overlap in a painting, the data should already provide information about opacity or transparency, favorably in the common RGBA format.
    \item Data set size and variability: Only with enough different data points available, it will be possible to train a renderer reliably.
    \item Image quality: Brushstrokes should be as close to real-world brushstrokes as possible to ensure high-quality renderings.
\end{enumerate}
\marginnote{RGBA has a fourth channel besides the three color channels, which hold information about the opacity of every pixel.}

\subsubsection{Brushstroke Images}
There are some data sets with single scanned hand-drawn brushstrokes available on the internet~\cite{zolee_on_onlygfx}.
Some even come with the background removed on which they were drawn on.\\
Such data has the advantage that it consists of actual real-world brushstrokes.
On the other side, the data is very sparse.
Brushstrokes are very similar throughout the data set considering their path and color and come without any further information.
This would have the generator to interpolate data in the best case or not work for any unknown inputs in the worst case.\\
These limitations to the data make it unlikely that a generator could learn a coherent representation from it.

\subsubsection{Painting Libraries}
Painting libraries such as libmypaint are the de facto standard for neural renderer in drawing networks~\cite{neuralpainters, learning2paint, strokenet}.\\
They allow to fully control the output through parameters and thus make training for the renderer easier.
Still, this data falls short regarding the authenticity of rendered strokes, which are obviously computer generated.\\
This kind of data would allow to train a renderer, though it would likely cause the final images to look a bit ``flat''.

\subsubsection{Fluid Simulations}
Fluid simulations like those by \citeauthor*{wetbrush} give a better render quality compared to painting libraries. 
Unfortunately, only few implementations are openly available and if so are not as advanced.\\
\citeauthor*{SPIRAL} provide a fluid simulation package based on \citetitle{fluidpaint}~\cite{fluidpaint}.
It uses simple fluid dynamics to give artificial brushstrokes a more plastic look.
One problem, that comes with all simulations is that the brushstroke path not controlled directly, but by moving a virtual paintbrush.
Nevertheless, it is possible to partly account for this when creating the data set.\\
Another problem is the simulation time.
State-of-the-art approaches are just capable of rendering in real-time which is too slow for an entire data set.
Luckily, \citetitle{fluidpaint} is not as computationally expensive and can be parallelized to generate $100,000$ samples in about two hours.

All in all, \citetitle{fluidpaint} has been chosen as the tool to generate the data set.
The superior render quality justifies the additional work required to generate the data set.\\
There are other applications which are capable of simulating brushstrokes reasonably well like \citetitle{artrage}~\cite{artrage} and \citetitle{krita}~\cite{krita}.
Unfortunately, these application do not provide an interface to efficiently generate a data set.
\begin{marginfigure}
    \subfloat[Photograph]{%
        \includegraphics[width=.8\textwidth]{images/real_red_stroke}%
        \labfig{strokes:real}%
    }\qquad
    \subfloat[ArtRage]{%
        \includegraphics[width=.8\textwidth]{images/artrage_red_stroke}%
        \labfig{strokes:artrage}%
    }\qquad
    \subfloat[libmypaint]{%
        \includegraphics[width=.8\textwidth]{images/affinity_red_stroke}%
        \labfig{strokes:libmypaint}%
    }\qquad
    \subfloat[FluidPaint]{%
        \includegraphics[width=.8\textwidth]{images/fluidpaint_red_stroke}%
        \labfig{strokes:fluidpaint}%
    }
    \caption[]{Comparison of similar brushstrokes in each data set}
    \labfig{strokes}
\end{marginfigure}

\subsubsection{Brushstroke Formalism}
The means of brushstroke production are now seized and what is left is to formulate the parameters that define a brushstrokes.
These parameters must quantify the following three properties of brushstrokes:
\begin{itemize}
    \item color
    \item thickness/brush size
    \item path/curve
\end{itemize}

Naturally, color is approximated by three 8-bit numbers in the RGB format.\\
Path and thickness depend on the given coordinate scaling which is $[0, 1]$ in \citetitle{fluidpaint}.
Valid brush sizes are any value in $[0, 1]$ from an infinitely small brushstroke ($0$) to one as wide as the canvas ($1$).
Both these edge cases do not make sense in this approach, thus the range is constrained to $[.03, .2]$.
This range includes only brushstrokes that are visible but do not cover the whole canvas.\\

Quantifying the path is a little more tricky.
\citetitle{fluidpaint} is internally only able to move the brush in a straight line between $a$ and $b$ for a single time step $t \rightarrow t+1$.
Subsequently, any curved paths must be split into linear/straight segments that together approximate a curved line.
As more time steps mean longer simulation times and fewer steps mean edgy curves, a value of 20 time-steps per stroke emerged as a good compromise.\\
There are additional issues which stem from the fact that \citetitle{fluidpaint} gives only control over the brush's path.
Due to the length of the bristles an offset is introduced.
Also, the bristles need to be aligned such that a ``clean'' brushstroke is drawn.
Further information how this on how this has been implemented can be found in the appendix (see \refch{fluidpaint}).
\begin{marginfigure}
    \includegraphics{bezier_sample}
    \caption[]{Virtual bristles in \citetitle{fluidpaint} as they are initialized.}
\end{marginfigure}
\todo{add figure of bristles/fluidpaint bristles}

Another question is how to express a curved path in numbers for the neural renderer.
A straightforward representation would be a sequence of points that make up the curved path.
Such an approach allows for the highest versatility.
\begin{marginfigure}
    \includegraphics{bezier_sample}
    \caption[]{(a) typical path of a brushstroke in a painting. (b) a rarely observed path~\cite{onlygfx}.}
    \labfig{genetic}
\end{marginfigure}
At the same time it introduces a noticeable amount of parameters as each point consists of 2 coordinates, totaling 40 additional dimensions.
Also, each point is not independent of the other points but should lie withing a certain range and follow a reasonable path. 
Otherwise, the resulting brushstrokes would resemble more of a scribble than what a brushstroke is expected to look like.

Drawing networks often face the same issue and many rely on Bézier curves as a parametrization~\cite{SPIRAL, neuralpainters, learning2paint}.\\
\textbf{Bézier curves} parametrize curved paths and are often used in computer graphics.
Instead of linear segments, they define analytical paths for curves of different orders.
In this case, quadratic Bézier curves are already sufficient.\\
A curve is defined by its start and end point, as well as a control point (more points for higher orders).
These points are connected by lines such that control points have two lines connecting them, and start- and end points only one line.
Over the time interval $[0, T]$ a virtual point moves along each of these lines linearly.
\marginnote{The linear motion starts at the point with the lower index.
The start point has the lowest index, the end point the highest index.
Any control points in between get higher indices in the order they are placed.
Thus a virtual point will always start at the start point $t=0$ and end at the end point $t = T$.}\\
As two such points, in the quadratic case, give two paths a another line is introduced to connect these two virtual points.
This line will now change as the virtual points move linearly in $[0, T]$.
Another virtual point is then placed on this line and also moves linearly in $[0, T]$.
This point will follow a more complex path as the line on which it moves, is not static anymore.
Thus, the path of this point defines a curve as originally intended.\\
A quadratic Bézier curve will only bend into one direction or follow a straight path.\\
\marginnote{For higher orders, the displayed process can be applied iteratively and allows for more complex curves.}
As brushstrokes usually follow a rather simple path and fewer parameters are preferred, quadratic Bézier curves are used as parametrization.
This is also the case in many drawing networks~\cite{SPIRAL, neuralpainters, learning2paint}.
\begin{marginfigure}
    \includegraphics{bezier_sample}
    \caption[]{Sample of a 3rd degree Bezier curve, using the De-Casteljau-algorithm, \url{https:\/\/de.wikipedia.org\/wiki\/Bézierkurve\#\/media\/Datei:Bezier-cast-3.svg}}
    \labfig{genetic}
\end{marginfigure}

Ultimately, there will be ten values that are sufficient to parametrize brushstrokes with certain constraints:
\begin{itemize}
    \item Three 2D coordinates that define the Bezier curves (six values).
    \item One brush size parameter.
    \item Three values in RGB space.
\end{itemize}

\subsubsection{Data Constraints}
This section already mentioned that data cannot be generated in real-time for the training, instead it is generated beforehand.
A data set of $100,000$ brushstrokes is sampled in the ten-dimensional feature space that has been introduced.
However, the data still needs further constraints to facilitate the renderer's training.\\
Such constraints should limit the data to valid brushstrokes only.
``Valid'' brushstrokes will be defined as brushstrokes that resemble the shape of many real-world brushstrokes.
This primarily concerns two relations within a brushstroke:
\begin{itemize}
    \item Its width-to-length ratio.
    \item Its curvature.
\end{itemize}

Brushstrokes should be at least two times as long as they are wide.
\begin{align}
    \norm{\vec{s} - \vec{e}} \overset{!}{\leq}  2 \times (\text{brush size}) \labeq{bs}
\end{align}
Due to the simulation in \citetitle{fluidpaint}, shorter brushstrokes will show artifacts caused by the bristles' length.
The length of these bristles directly correlates to the brush size/width.\\
Another motivation is the intended use-case, which will focus on van Gogh paintings.
As van Gogh did not practice pointillism, most of his strokes have a length to them, which brings such a constraint in line with some characteristics of van Gogh's style.\\
Subsequently, $\vec{s}$, $\vec{e}$ and the brush size are sampled together such that $\vec{s}$ and $\vec{e}$ are further apart than two-times the brush size.\\
The brush size is sampled from a uniform distribution in $[.03, .2]$ and $\vec{s}$ and $\vec{e}$ are sampled in a uniform distribution in $[0, 1]^2$.
If the relation
\begin{align}
    \norm{\vec{s} - \vec{e}} \geq 2 \times \text{brush size}
\end{align}
does hold this set of parameters is included in the data set, otherwise the whole set is re-sampled.

Similar constriants apply to the curvature:
Most brushstrokes (especially those by van Gogh) have a certain ``flow'' or ``smoothness'' to them.
``Smooth'' brushstrokes do not follow a kinky path but feature curvatures with large radii.\\
\begin{marginfigure}
    %\includegraphics{patch_starrynight}
    \caption[]{Brushstrokes by van Gogh in a painting (The Starry Night).}
\end{marginfigure}
Thus, the data set will be restricted to brushstrokes that follow these descriptions.
To achieve this with random sampling in place, a multivariate Gaussian distribution is placed between start ($\vec{s}$) and end point ($\vec{e}$).
The Gaussian is parametrized by an offset $\vec{mu}$ and a covariance matrix $\mat{\Sigma}$.
$\vec{mu}$ is picked such that the distribution is centered between $\vec{s}$ and $\vec{e}$.
\begin{align}
    \vec{\mu} = \frac{\vec{s} + \vec{e}}{2}
\end{align}
One of the principal axes of $\mat{\Sigma}$ shall align with the directional vector between $\vec{s}$ and $\vec{e}$.
\begin{align}
    \vec{q}_1 & = \vec{s} - \vec{e} \\
    \hat{\vec{q}}_1 & = \frac{\vec{q}_1}{\norm{\vec{q}_1}} \\
    \hat{\vec{q}}_2 & = \mat{R} \hat{\vec{q}}_1 \text{ s.t. } \hat{\vec{q}}_2 \perp \hat{\vec{q}}_1 \\
    \mat{Q} = (\hat{\vec{q}}_1, \hat{\vec{q}}_2)
\end{align}
$\hat{\vec{q}}_1$ and $\hat{\vec{q}}_2$ build an orthonormal 2D basis which are the eigenvectors of the eigendecomposition of $\mat{\Sigma}$ .
The respective eigenvalues $\lambda_1$ and $\lambda_2$ are handpicked such that the distribution is narrow along $\hat{\vec{q}}_1$ but broad along $\hat{\vec{q}}_2$.
\begin{align}
    \lambda_1 & = \frac{1}{200} \\
    \lambda_2 & = \frac{1}{25} \\
    \mat{\Lambda} & = \begin{pmatrix} \lambda_1 & \\ & \lambda_2 \end{pmatrix}
\end{align}
Thus, the eigendecomposition of $\mat{\Sigma}$ is known and $\mat{\Sigma}$ can be constructed from it.
\begin{align}
    \mat{\Sigma} = \mat{Q} \mat{\Lambda} \mat{Q}^T
\end{align}
Figure \ref{fig:datageneration} shows samples from this distribution for an exemplary set of start and end point.
\begin{marginfigure}
    \resizebox{\textwidth}{!}{
        \input{images/scatter.pgf}
    }
    \caption[]{Exemplary scatter plot for given start and end point to visualize the covariance matrix}
    \labfig{scatter}
\end{marginfigure}

This distribution is intended to follow the expected distribution of brushstrokes as they appear in the real world.
The majority of brushstrokes will be straight or just slightly bent due to the maximum of the PDF of the Multivariate Gaussian being at the center $\vec{\mu}$.
Bent brushstrokes will mostly be symmetric as $\hat{\vec{q}}_2$ is the long axis of the multivariate Gaussian.
Thus, the region which is of most interest will be densely populated.\\
Still, some brushstrokes will have their bent slightly towards either end of the brushstroke and others will feature tight curvature.
\begin{align}
    p(\vec{c}| \vec{s}, \vec{e}) & = \mathcal{N}(\vec{\mu}, \mat{\Sigma}) \labeq{checkpoint} \\
\end{align}
If $\vec{c}$ lies outside $[0, 1]^2$ it is re-sampled.\\
$\vec{s}$, $\vec{e}$ and $\vec{c}$ are also scaled with the handpicked factor of $0.7$ to ensure the brushstrokes are rendered completely within the window and not cut by an edge of the render window.

The color of the brushstrokes is not constrained as it should be possible to fit any color distribution equally well.
Each RGB value is sampled from a uniform distribution as a color.

The resulting tuple of start, end and control point, brush size, and RGB color is then rendered.

The render canvas size was chosen to be 64x64 pixels for several reasons:
First, even with such a small canvas size, training of the renderer takes about one day.
Secondly, the larger the render canvas size becomes, the deeper the renderer needs to be, which results in more computational overhead not only in training but in the optimization procedure as well.
Lastly, there will be upwards of a thousand brushstrokes in a single image and increasing the canvas size to 128x128 would require four times as much memory per rendered image.
As 1000 brushstrokes would already account for $1000 x 64 x 64 x 4 \si{\byte} \approx 16.4 \si{\gibi \byte}$ a four-fold increase is significant.

After rendering, the data set is renormalized to the range $[-1, 1]$ for convenience and to facilitate training as well.

$100,000$ samples that follow these constraints make up the training data set.
The constraints helped in fact to train the renderer.

\subsection{Architecture}
The architecture of the brushstroke generator follows that of an inverse VGG network.
It is widely used and has shown that it should be capable of handling this task.\\
The architecture consists of three dense layers with 256 dimensions, leaky ReLU activation and instance normalization at the beginning.
The output is then spatially arranged in a 4x4 grid which is the base for the following convolutional blocks.\\
Each blocks applies a convolution with kernel size 3x3 and stride 1x1 followed by a leaky ReLU activation and instance normalization.
At the end of the block the activations are bilinearly upsampled.
The number of kernels decreases logarithmically with every layer.
Notably every convolution has two additional input channels with coordinates along each axis as values.
This is called CoordConv~\cite{coordconv}.
Two other additions to the convolutional block have been inspired by StyleGAN~\cite{stylegan}.
First, the activations are augmented with weighted noise inputs.
The scale of the additional noise is a trainable parameter in the network.
Secondly, a style modulation is applied to the normalized activations.
This means that an additional fully connected pipeline with four layers is implemented to give a set of activations.
These activations are then the inputs to single independent fully connected layers at every scale.
These FC layers predict the affine parameters of every instance normalization.\\
The last convolutional block has neither normalization nor upsampling but a hyperbolic tangent function to restrict the output to $[-1, 1]$.\\

The discriminator is similarly built.\\
It employs the same convolutional blocks with a few changes.
There is no style modulation or noise augmentation.
Instead, there are multi-scale inputs like in MS-GAN~\cite{MSG-GAN}.
Thus, the input image is downscaled to the resolution of each layer and then concatenated to the existing activations of that layer.
Also, instead of upscaling there is max-pool downscaling after each block.
The convolutions have the same kernel size, kernel number and stride in each layer as the generator.\\
When the activation reach a resolution of $4\times4$, they are flattened and serve as an input to a three-layer fully connected network.
These FC layers are again symmetrically configured to the generator.
The last FC layer consists of only eight unit, which act are predictors.

\reffig{architecture} shows a visual depiction of the architectures.

\todo{reinsert these figures}
%\begin{figure*}
%    \resizebox{1.5\textwidth}{!}{
%        \input{images/generator.tex}
%    }
%    \resizebox{1.5\textwidth}{!}{
%        \input{images/discriminator.tex}
%    }
%        \caption{Visualization of the generator and discriminator architectures}\labfig{genarch}
%        \labfig{architecture}
%\end{figure*}

\subsection{Training}
Training is set up in an adversarial fashion.
Thus, the generator predicts a brush stroke form parameters and the discriminator tries to tell is apart from a given sample from the data set.
RaGAN defines the adversarial objective during training.\\
Also the L2 distance is used in the first phase of training until it falls under a threshold of $0.2$.
This way e meaningful decoding is learned in the beginning but the is no blurring in later training stages.
L2 distance and the FID score serve as evaluation metrics during training.
The L2 distance does not qualify as a sufficient metric for later training stages due to the stochastic nature of the brushstrokes.
The FID score serves as a metric for the visual similarity to the data set as the visual comparison of the generated samples has proven difficult between different experimental runs.
\marginnote{The Frechet inception distance (FID) is calculated by comparing the statistics of InceptionNet~\cite{inception}-activations for two batches of samples from the dataset and generated samples~\cite{FID}.
This is somewhat similar to perceptual distance.}

A two-time-step update rule was implemented to stabilize training further.
This way, the discriminator and generator are updated at different frequencies.
If, for instance, the discriminator becomes too strong and the generator's loss increases, the generator is update more often than the discriminator to balance both networks again.

\subsection{Results}
\reffig{brushstroke} shows a generated sample compared to a rendered sample with the same parameters.
The images are split into their RGB channels and the alpha channel to allow evaluating the two independently.
\reffig{single_brushstroke} shows all four channels merged for a generated brushstroke.
It becomes immediately clear that the neural renderer is capable of imitating \citetitle{fluidpaint}.
\begin{marginfigure}
    \resizebox{\textwidth}{!}{
        \input{images/scatter.pgf}
    }
    \caption[]{Generated sample and the corresponding data set sample compared.}
    \labfig{scatter}
\end{marginfigure}
\begin{marginfigure}
    \resizebox{\textwidth}{!}{
        \input{images/scatter.pgf}
    }
    \caption[]{Generated brushstroke.}
    \labfig{scatter}
\end{marginfigure}

Further analysis of the results will be presented in the next chapter.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Stroke Approximation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stroke Approximation}
\labsec{strokeapprox}

\subsection{Data Set}
\labsubsec{dataset}
The data set will be presented in this section due to the data defining demands for later networks.
Also, the data set for the optimization task should meet a few requirements.

In order to focus on the brushstrokes in an image, the data set should consist of relatively high-resolution images.
At the same time, most brushstrokes should fit into a 64x64 window.
Any larger strokes can not be approximated with a single rendering window and thus would falsely be approximated by multiple brushstrokes.

Thus, more information per image is required, than merely the resolution.
\todo{reference li and lamberti aaas they did the same}
The scale of the image, therefore, becomes inquired information in this task.
Ideally, each image should be accompanied by its measurement as this allows for rescaling the image accordingly, given that one knows how large a typical brushstroke is.

At last, the painting's technique must be oil on canvas or similar techniques as this is what the renderer has been trained for.

All these requirements on the data are met by data directly available from the Van Gogh Museum in Amsterdam.
Each high-resolution image is categorized by its technique as well as the period in which it was painted.
This is accompanied by information on the dimensions of the image.

\subsection{Optimization Algorithm}
The next stage of the approach takes the pre-trained neural renderer and uses it to approximate brushstrokes in images.
In theory, the neural renderer should allow gathering meaningful gradients in parameter space even though the losses are calculated in image space, as explained in section
\ref{sec:motivation}.
There, it was outlined already that the method of choice will be an optimization procedure that relies on the differentiability of the neural renderer.

Nevertheless, this is not the only possible approach to dissect images into sets of parameters.
As explained in \ref{sec:PR}, there exist a variety of approaches for this kind of task.
Some of these promise a fast inference of parameters from images, which makes the optimization approach in this work seem like a step back at first.
The following section is dedicated to justifying the chosen approach by comparing the existing state-of-the-art approaches.

The targeted task is to approximate the representation of an image with roughly 1,000,000 pixels = 1MP by means of $\approx 10.000$ brushstrokes (see section \ref{ssec:motivation}).

\subsubsection{Genetic Algorithms}
\labsubsubsec{genalg}
\todo{put much of this into the related work section}
Genetic algorithms are possibly the most straight-forward approach.
As it was laid out in sections \ref{sec:motivation} and \ref{sec:genetic}, genetic algorithms use random sampling and previous best results to approximate fitting solutions to the rendering problem, which was described in section \ref{sec:renderproblem}.

Current state-of-the-art solutions are capable of finding a solution in $\approx 1h$ when searching for an approximation of a 1MP image with simple geometric shapes like circles or triangles.
This corresponds to roughly $1 \times 10^{9}$ sampling steps.
The time frame and the number of sampling steps depend on target shape density, target accuracy, sampling rate per shape, and degrees of freedom (with the latter requiring more sampling).

Looking back at section \ref{sec:renderer}, each brushstroke has 10 degrees of freedom with some inter-dependencies between them.
Also it is known that the obtained neural renderer is capable of generating $\approx 300 \frac{\text{images}}{s}$.
This means that rendering $10^{9}$ sampled brushstrokes will take:
$$
\frac{10^{9} \text{samples}}{300 \frac{\text{images}}{s}} \approx 3.33 \times 10^{6} s
\approx 926 h \approx 38.6 d
$$
Clearly, this is an absurd amount of time to spend \textbf{per image}.
Furthermore, the sampling magnitude has not even been corrected for by the additional degrees of freedom that brushstrokes provide.

Ultimately, this means that genetic algorithms are not an option for this task, as they are too inefficient.

\subsubsection{Brushstroke Extraction}
\labsubsubsec{bse}
\todo{put much of this into the related work section}
Next in the line are algorithm-based approaches that extract brushstrokes from an image using standard computer vision techniques.

Besides texton based image characterization \cite{textons} and pure filter based approaches \cite{conv_based} there have been approaches to extract brushstrokes or some of their characteristics \cite{brushstrokecharacteristics} \cite{brushstrokeextraction}.

The latter would pose a valid option for the task at hand.
Unfortunately, the best existing techniques fail to detect brushstrokes reliably over the whole image but only identify the most significant ones (see figure \ref{fig brushstrokeextracted}) /todo{add example image}.
Other approaches are only able to extract a few characteristics, like the orientation of a brushstroke, which proves insufficient as well.
\todo{Other approaches will usually not get parameters but again pixels, which they analyze.}
\todo{show some of the extracted brushstrokes}

\subsubsection{Stroke Based Rendering}
\labsubsubsec{sbr}
Stroke Based Rendering or Painterly Rendering also uses algorithms to approximate an image through brushstrokes with the original goal to achieve some stylization along the way.
While early works relied on interactive approaches, later publications were then able to automate this process entirely.
Judging from tests by the authors, such an approach would take between \todo{this number} and \todo{this number} hours per image.
As figures \ref{fig:sbr1} and \ref{fig:sbr2} show, these approaches obtain similar results to genetic algorithms and tend to draw an image from a coarse scale to a more delicate scale over time instead of a locally coherent manner.

These approaches were not intended to be used to obtain brushstrokes from a painting but focus on stylizing images.
This renders such an approach unfit for the goal of this thesis as well.

\subsubsection{Drawing Networks}
\labsubsubsec{drawingnetworks}
\todo{put much of this into the related work section}
Lastly, drawing networks are the newest iteration of approaches in this field.
Beginning with \todo{which one was first?}, there have been approaches that make use of feed-forward or recurrent neural networks combined with either supervised training or deep reinforcement learning.
The best results of these approaches are shown in figure \ref{fig:drawingnets}.

Noticeably, all of these images have a maximum resolution of 256 pixels along their longest edge.
There have not been any approaches yet, which can go significantly beyond this limitation.
Notably, the high computational costs of training recurrent neural networks seem to be an obstacle when shooting for higher resolutions.
Such resolutions can not be deemed sufficient when looking at individual brushstrokes, as outlined in \ref{ssec:dataset}.

Consequently, drawing networks must also be ruled out.

\subsubsection{Combined Approach}
\labsubsubsec{combinedapproach}

Even though there have been plenty of previous approaches to the task of extracting brushstrokes from images or similar tasks like rendering images through brushstrokes, none of these quite meet all the requirements of this task.
Namely, high-resolution input images, brushstroke focussed rendering or retrieval, limited resources, and realistic depiction of brushstrokes.

Although brushstroke extraction is the topic of this chapter, stroke based rendering and drawing networks both seem capable of making brushstroke extraction more feasible with their inverse approach.
Drawing networks introduced differentiable renderers as a tool to facilitate training.
Stroke based renderers achieve a parametrization implicitly by focussing on replicating the entire image instead of extracting individual brushstrokes.

They can be combined into a unified approach that uses the differentiable renderer and the objective of recreating an entire image.
The resulting approach does not use re-sampling like genetic algorithms but relies on gradient descent to converge to a solution significantly faster.

The limited capabilities of such a renderer would also guarantee that any approximation is composed of only valid brushstrokes instead of single pixels as it would be the case with normal generative models.

All in all, this approach is probably the best suited for approximating a target image only through brushstrokes as closely as possible.


The decision of whether to use an optimization-based approach is also linked to another question: Whether to sequentially or parallelly place brushstrokes.

As an example, most drawing networks and stroke-based renderers rely on a sequential approach.
Intuitively, it makes sense to use a sequential approach as artists also place their brushstrokes sequentially on the canvas.
However, there is a significant difference in how existing computer vision approaches place strokes compared to artists.
As neural networks search for ways to reduce the loss function as much as possible and as fast as possible, images tend to be made up of large canvas-filling brushstrokes at early stages, which in turn reduce the L2 loss.

This is contrary to how an artist usually works.
Artists tend to fill the background based on content (\eg sea or sky is often painted first) and will often leave some blank canvas to paint foreground objects later on.
They also do not use giant brushes for this but normal-sized brushes with which they place many strokes.
Thus, sequential approaches differ significantly from real-world paint processes.

In contrast, parallel approaches predict the whole painting as one set of parameters.
They are not as widespread due to the computational pitfalls that come with predicting brushstrokes for a whole painting at once compared to predicting only a few at a time.
Also, parallel approaches still need to predict in which order brushstrokes should be placed on the canvas (this will be explained in section \ref{sssec:order}) such that brush strokes can overlap.
Nonetheless, the advantage to sequential approaches is the fact, that foreground an background strokes can influence one another.

An example would be a stroke in the foreground, changing its path and revealing the canvas beneath.
Then another stroke in the background should cover this up if the color matches.
For sequential approaches, the background stroke can not be changed afterward, and it is very complicated to formulate a loss that propagates this information.
As artists already plan their future brushstrokes when painting the background, it would require a drawing network to plan far ahead.
At the same time, an optimization-based approach will not need to do that.

Another argument for parallel optimization is the focus on actually visible brushstrokes.
As artists cover up previous strokes, again and again, it becomes a shot in the dark to guess what these first brush strokes might have looked like.
Subsequently, any such guess is ill-posed and only introduces noise to the problem.
A parallel approach does not care for how the background could have been drawn but only focus on what is visible in the given image.

\subsubsection{Pitfalls of Feed-Forward Approaches}
\labsubsubsec{ffapproaches}

In the development of this thesis, there were experiments targeting a feed-forward approach before ultimately deciding on an optimization-based approach that is now presented.
As it would exceed the scope of this thesis, the arising problems and cause for discarding this approach shall be discussed.

First, the computational burden of a feed-forward approach is very high.
Existing feed-forward drawing networks compromise image resolution to realize their implementation.
As it was possible to implement this for small scale data like the cMNIST data set, such an approach seems feasible at first. \todo{margin explain cmnist} \todo{add image that were drawn by ff network}
However, since compromising image resolution is not a viable option (see \refsec{dataset}, one must find ways around this problem.
Fully convolutional architectures are a popular solution, which allows training on small scale data and inference on large scale images.
Unfortunately, the problem of predicting many spatially brushstrokes with complex parameters has proven too difficult for fully convolutional approaches.

This goes hand in hand with another problem that occurred: the placement of brushstrokes on the canvas.
As artists are not bound to the same pixel grid as computers typically are, they can place brushstrokes freely on the canvas.
More so, they can pack brushstrokes densely in one area while distributing them broadly in another.
Classic CNNs are not able to allow for similar behavior as they always require a grid layout.
Experiments with either displaceable grid cells or stacked grids have proven did work on a small scale.
However, together with a fully convolutional architecture, the approach did not seem to scale to larger images.

Thus, an optimization-based approach became favorable as it offers good approximations at high resolutions with manageable computational overhead.

\subsection{Optimization Procedure}
\labsubsec{opt}
The previous section \ref{ssec:ffapproaches} already specified why an optimization-based approach is preferable over a feed-forward or recurrent approach.
This section aims to give more details about the optimization procedure.


\subsubsection{Rendering Layout}
\labsubsubsec{layout}
Fundamentally, the optimization procedure is inspired by stroke-based rendering procedures.
It could also be compared to the style transfer approach by \citeauthor*~{gatys} \cite{gatys}, with parameters optimized instead of pixels.
The difference to normal stroke-based renderers, though, is the limited size of a rendered brushstroke in this work's renderer.

This poses a significant challenge that might not be obvious at first.

Ideally, the optimization procedure should be able to place strokes freely on the canvas, as this allows for an unbiased approximation.
Furthermore, it would allow allocating many small strokes in areas where the artist placed many strokes and use fewer and wider strokes in other areas.
However, due to various limitations, which were explained in \ref{sec:NR}, the renderer is not able to render single brushstrokes in a 1MP frame. Similar approaches only perform on relatively small canvas sizes, likely due to this issue \todo{add ref}.

Dissecting the target image into many small patches and then running the optimization procedure on these individual patches is a workaround for this.
After the patches are rendered, they are then fused along their edges to give the full image.
A good comparison is a grid of renders where each grid cell is the center of many renderings at the same time \todo{add figure to this}.
The major issue of such an approach is the adjacent edges where the grid cells are joined.
Also, a grid structure will almost always differ from the inherent distribution of brushstrokes in an image;
grid edges will, more often than not, separate brushstrokes between two grid cells.
A simple solution to this problem is transitioning from a stacked grid structure to an overlapping grid. \todo{graphics!!!!!}

By overlapping the grid cell, obvious edges between render windows are hidden as every edge lies within the frame of a different render window.
Choosing a lattice vector size smaller than the render window's dimensions such a grid can easily be realized.

Still, this kind of initialization requires a very even distribution of brushstrokes, ideally with a brushstroke at the center of each cell.
As this is not the case, and stroke densities will vary locally, the grid layout is prone to erroneously enforce a grid-like layout of strokes where there is none.
This is due to the inability of the grid to account for local changes in density and the following propagation of error.
Starting with a single region of high-density strokes in the vicinity of one grid cell, this cell would ideally render a narrow stroke to achieve high accuracy.
Neighboring cells then have to shift their strokes towards the center of that grid cell to account for the free space that is not covered by the narrow brushstrokes.
This shift must then be accounted for by the next neighboring cells and so on, which will cause all strokes in a row or column to shift towards this one spot with a high stroke density.

Now, a painting usually has many such high-density regions, which possibly cancel the shift that is caused by other areas.
As a result, the renderings are likely to not shift at all, as shifting mostly cancels out over the whole image.
Subsequently, an area of high stroke density will not have enough strokes available in its local region and thus will be covered up by a single broad stroke as this minimizes the L2 loss.

The core of the problem is the previously imposed lattice structure that propagates local density shifts along its principal axes.

One possible solution is getting rid of the lattice structure and replacing it with a more random structure that also covers the image sufficiently.
This can be accomplished by using \textbf{super-pixels}.
Superpixels were popular in the 'pre-neural network era' of computer vision and were used in image segmentation tasks \cite{superpixel_segmentation}.
Nevertheless, superpixels-like aglorithms are also a popular starting point for brushstroke extraction algorithms \cite{lamberti}.

Basically, superpixels are pairwise disjoint groups of pixels in an image that would usually join pixels with similar colors in a local region.
Straight away, it is obvious why this is interesting for brushstroke extraction.
The distribution of superpixels will not follow a grid-layout as the previous approach, and the location of superpixels should relate to the given color distribution in the image.
It is easy to imagine that the location of the superpixel centers would be a good prior for locations of render windows as well.
Also, as the colors of pixels inside a superpixel should be similar, one can use the mean color of a superpixel as an initialization for the color of the brushstroke.

Ultimately, a superpixel segmentation will be used to infer positions for render windows as well as the color initialization of each stroke.

\subsubsection{Rendering Order}
\labsubsubsec{order}
Another problem that will come up during the optimization procedure is the order in which strokes are rendered (already mentioned in \refsec{opt}).
Real-world brushstrokes are also subject to the same issue as the current brushstroke will always be placed on top of brushstrokes painted earlier, (see section \ref{sssec:combinedapproach}).

As the optimization-based approach relies on parallel optimization of brushstrokes, it must decide which strokes are in the foreground and which are in the background.
Otherwise, as this would randomly change, edges in the image might be obstructed, and optimization could oscillate between solutions where different strokes lie in the foreground.
It could also prevent brushstrokes from overlapping such that they cover disjoint areas.
All of these outcomes would be unfavorable as it tends to produce worse results in the end.

The solution which is presented in this thesis is an additional parameter that describes a brushstroke's accuracy.
The accuracy is defined as the L2 distance of each stroke's pixel to the corresponding pixel in the target image multiplied by this each pixel's alpha value.
This removes any pixels which are of no interest from the loss and focusses only on the rendered pixels.
\begin{align}
    \text{accuracy} = 1 - \frac{1}{N} \sum_{p \in \text{pixels}} \norm{p - p'}_2^2 \text{ with } \vec{p} = \vec{p'} 
\end{align}
    \todo{improve this, especially the p=p' part}

The resulting value describes how well the pixels of the rendered stroke match their respective pixels in the target image.
Consequently, any brushstroke with higher accuracy will be more faithful to the target image than strokes with lower accuracy.
Placing these brushstrokes in the foreground should thus result in a smaller L2 loss than the other way around.
Vice versa, brushstrokes that connect two same colored areas will aggregate a lower accuracy as the brushstroke is compared at the intersection as well.
A rendered brushstroke that fits the foreground brushstroke will not be affected by this, thus keeping its high accuracy and laying on top of the other brush stroke.

Notably, the accuracy should not be included in the brushstroke's loss, as this would prohibit background strokes from covering larger areas and result in behavior that is similar to the non-overlapping issue previously described.
Thus, the accuracy of each stroke will be calculated as it is rendered.


\subsubsection{Initilization}
\labsubsubsec{init}

The following section will focus on initialization details for all parameters of a brushstroke, their position, and the confidence value.

Besides the original ten parameters of each brushstroke, which were explained in section \ref{sssec:formalism}, the previous two sections introduced an accuracy parameter for ordering and two translation parameters that define the position of the render window along each axis.
All of these parameters must be initialized before the optimization procedure starts.
Ideally, the initialization should not introduce any bias to the optimization process.
At the same time, an initialization should facilitate training and accelerate convergence in the early stages of optimization.

Unfortunately, the placement of the render window will surely enforce a bias on the optimization (see section \ref{sssec:layout}).
Thus, a superpixel initialization was motivated for the translation parameters as well as the color of the brushstrokes.
Subsequently, the translation parameter for each render window will be equal to the position of the weighted mass center of its respective superpixel.

The initial color will be taken from the mean color value of the superpixel.

The brush size will be initialized with the minimum possible value.
This will let brushstrokes not overlap at the beginning of optimization.
Only when the brushstrokes already roughly fit their local region, they shall intersect and be ordered by their accuracy.
Therefore, the initial accuracy will be 0 everywhere, as the accuracy is recalculated after every optimization step.

Other patch parameters, notably $\vec{s}$, $\vec{e}$ and $\vec{c}$, will be initialised using a narrow Gaussian distribution with $\sigma = .1$ and values clipped to $[-1, 1]$ because there is no prior information available on how the brushstrokes are oriented.
Instead, this approach relies on the optimization procedure to be minimally biased by this initialization of the path variables.

\subsubsection{Partial Updates}
\labsubsubsec{partupdate}

When building an optimizer based on the information provided until now, GPU memory limitations become a guaranteed issue.
Even as it might not be evident at first, the optimization procedure imposes a considerable requirement for memory on the graphics card, due to two parts of the training:

First, the number of brushstrokes can easily become very high with large images as input.
With a render window size of 64x64, the brushstrokes are relatively small, and paintings can consist of a few thousand brushstrokes.
This would equate to a batch size of a few thousand for the brushstroke renderer.
Tests have shown that on the latest hardware with 12GB memory, the maximum number of brushstrokes rendered in parallel is $\approx 256$.
Obviously, this is one to two orders of magnitude smaller than what would be needed to optimize all strokes in the painting in parallel.
Still, there is a way around this bottleneck, by optimizing the image not as a whole but as smaller patches consisting of 256 brushstrokes at a time, giving a partial update routine.

Each patch comprises the 256 nearest render windows to a randomly sampled location on the canvas.
These 256 brushstrokes are then rendered from their parameters in order to obtain a gradient later on.
Then strokes are ordered according to their accuracy, placed on canvas ('padded') and blended ('stitched').
At last, the loss is calculated and back-propagated to update the parameters of the patch.

When restricting the number of brush strokes, bordercases do become an issue.
Brushstrokes that lie at the perimeter of the patch are not fully surrounded by other patches.
The result is a discrepancy of what the gradient to the brushstroke would look like if the brushstroke had laid in the middle of the patch.
A solution to this is laying a ring of already rendered brushstrokes around the patch.
The ring guarantees that all brush strokes that brushstrokes at the border of the patch are surrounded by neighboring brushstrokes the same way, as if they were in the middle of the patch.
Notably, the brushstrokes that belong to the ring around the patch, serve as 'dummy' brushstrokes and are optimized.
Without any need for optimization, these brushstrokes do not need to be rendered in this optimization step.
Instead, they could have been rendered earlier and the rendered image is just reused.
For this reason rendered brushstrokes for all parameters are saved in a \textbf{render image catalog}.
Equally, the collection of brushstrokes that shall later compose the image shall be called a \textbf{parameter catalog}.
The parameters as well as the images in these catalogs are updated whenever the respective brushstroke parameter has been updated.
With the render image catalog at hand, it is possible to use the previously rendered brushstrokes to stitch the image as a whole with the newly rendered brushstrokes of the image patch embedded.

As this will cause the border strokes to be surrounded by other brushstrokes to all sides (even if not all of them are freshly rendered), the effect of the partial update routine vanishes.

The other problem for huge input images is that the stitching of brushstrokes takes up a considerable amount of memory.
In detail, each brushstroke must be placed on the virtual canvas individually,  where the canvas' size is that of the input image.
This would equate to a couple of thousand 1MP images being stored in memory before they are stitched to a single 1MP images.
As a single 1MP image carries roughly $1000 \times 1000 \times 4 \text{channels} \times 8 Bit = 32 \times 10^{6} Bit = 4 \times 10^{6} Byte = 4 MB$ of information, a few thousand of these will easily exceed the memory of most graphics cards.
\marginnote{Images atypically saved as unsigned 8-Bit integers, thus 8-Bit per channel.}

Luckily, the previous workaround for optimizing only 256 brushstrokes will work as well without rendering the full image at every step.
Since most brushstrokes are not re-rendered and thus will not be supplied with a gradient, their main task is to regulate losses for edge strokes of the image patch.
As this does not need far away strokes, but only those close to the optimized strokes, a ring of pre-rendered strokes around the re-rendered patch will suffice.

\todo{approximate the number of brushstrokes needed to surround the patch}
This reduces the number of involved brushstrokes per optimization step from a few thousand down to a couple hundred.
Besides allowing for the partial update routine to be performed at all, it should also increase performance significantly compared to an approach that involves all brushstrokes.

\subsection[Placing \& Blending]{Image Placing \& Blending}
\labsubsec{placing_blending}
As the update and optimization procedure has now been explained thoroughly, it is now time to explain the process of placing and blending a rendered brushstroke a bit further.

After each stroke has been rendered, it needs to be placed according to the translation parameters (see \ref{sssec:layout}).
This requires dynamically placing each brushstroke inside a zero-filled tensor.

By calculating each pixel's global positioning in the rendered image individually, it is possible to scatter the pixels of the original rendered image into the larger zero-filled tensor and obtain a globally placed brushstroke.

The task of blending the resulting canvas-sized rendered brushstrokes together, or stitching them, is more complicated, though.
Due to the canvas' alpha channel, it is possible to blend only relevant information while the rest of the image will be ignored.

As far as conventional alpha blending goes, two images are blended by multiplying each pixel value with the alpha value of the top-layer image while the background image is multiplied with the complement to the alpha value:
\begin{align}
    p_{x,y} = p^{\text{top}}_{x, y} \times \alpha^{top}_{x, y}
    + p^{\text{bottom}}_{x, y} \times (1 - \alpha^{top}_{x, y})
    \forall (x, y) \in \mathcal{D}(\text{image})
\end{align}
\todo{correct this equation and make it nicer}

For multiple layers, this process can be repeated in various fashions, after the strokes are ordered according to their accuracy.
Either one could start from the bottom and blend the two back-most strokes, followed by the next third last strokes and so one, or one could start this process from the front with the two strokes in the very front being blended at first, then the stroke with the third-highest accuracy \etc.

Both of the approaches would yield the same result but differ only in the order in which they were blended.
Subsequently, both methods will have $(n-1)$ blending operations to compute per pixel.

Blending brushstrokes in a position-aware manner can reduce this number.
The majority of pixels for each padded brushstroke is non-informative, as the alpha value is zero (see \reffig{stroke_on_canvas}).
This opens the possibility of go from blending \textit{all} pixels of \textit{all} brush strokes to blending just those pixels with non-zero alpha values.
As before there were many layers that represented one single brush stroke each, few layers that do not represent brush strokes can achieve the same result.
Instead of representing brush strokes, these few layers represent the order in which pixels should be blended.
This means going from a layer-focussed approach to a rather pixel-focussed approach to alpha-blending.

The easiest way to accomplish this, is to first find the maximum blending-depth over all pixels, where the blending-depth $k$ is the number of layers where the alpha value is not zero.
\begin{align}
    k = \argmax_{p \in \text{pixels}} \sum_{i \in \# \text{layers}} \mathds{1}(\alpha_i > 0)
\end{align}

Then the top $k$ layer indices for each pixel are picked, which reduces the number of blending operations from $(n-1)$ to $(k-1)$.
Importantly, the top $k$ indices should not be ordered by their alpha values but remain in the order that was imposed by sorting according to the accuracy value.
Otherwise, the order will most certainly be mixed up, and the pixel with the highest alpha value will always lie on top instead of the pixel that belongs to the most accurate brushstroke.
Especially, as brushstroke renderings fade out towards their edges, this makes a significant difference.


Another way of accelerating the process of alpha-blending is \textbf{vectorizing}.
Instead of iteratively applying the computations, a vectorized operation can perform these computations in parallel.
Vectorizing makes it necessary to construct a tensor with the following properties:

For $\tensor{I} \in [0, 1]^{H \times W \times 4} $ the image target, the shape will be defined
as $\mathcal{S}(\tensor{I}) = (H, W, 4)$.
Each alpha channel will have the values $\alpha^{hw} \in [0, 1]$ for $h = 0, ..., H$ and $w = 0, ..., W$.

The set of rendered and padded brushstrokes $\tensor{J}$ will have the shape $(N, H, W, 4)$ with $N$ depicting the number of brushstrokes that ought to be stitched simultaneously.

Now, looking at each individual pixel in $\tensor{J}$, which is described by $(z^{hw}_n, \alpha^{hw}_n)$ for $n = 1, ..., N$ and $z^{hw}_n \in [0, 1]^{3}$, $z^{hw}$ describes the RGB values and $\alpha^{hw}$ the alpha-channel for a pixel at $(h, w)$.

A blending operation can then be defined by
\begin{align}
    z'^{hw} & = \tilde{\alpha}^{hw} \cdot z^{hw} \\
    \text{or} \\
    z'^{hw} & = \sum_{n=1}^N \tilde{\alpha}^{hw}_n  z^{hw}_n \\
\end{align}

with $z'^{hw}$ the resulting RGB values of the blended pixel and $\tilde{\alpha}^{hw}$ a vector that holds the merged alpha values for each pixel:
\begin{align}
    \tilde{\alpha}^{hw} & =
    \begin{pmatrix}
        \alpha^{hw}_1 & &\\
        \alpha^{hw}_2 & (1 - \alpha^{hw}_1) &\\
        \alpha^{hw}_3 & (1 - \alpha^{hw}_2) & (1 - \alpha^{hw}_1)\\
        \vdots & &\\
    \end{pmatrix}
    \\
    & = \alpha^{hw} \odot 
    \begin{pmatrix}
        1  &\\
        (1 - \alpha^{hw}_1) &\\
        (1 - \alpha^{hw}_2) & (1 - \alpha^{hw}_1)\\
        \vdots &\\
    \end{pmatrix}
    \\
    \rightarrow  \tilde{\alpha}^{hw}_n & = \alpha^{hw}_n \prod^{n-1}_{i=1} (1 - \alpha^{hw}_i)
\end{align}

with $\odot$ the element-wise product

What is left, is to find a way to construct $\tilde{\alpha}^{hw}$ from $\alpha^{hw}$.

For this an auxiliary matrix $\beta^{hw}$ is constructed:
\begin{align}
    \beta^{hw} = \alpha^{hw} \times \mathbb{1}_{1 \times N} = 
    \begin{pmatrix}
        \alpha^{hw}_1 & \alpha^{hw}_2 & \hdots & \alpha^{hw}_n\\
        \alpha^{hw}_1 & \alpha^{hw}_2 & \hdots & \alpha^{hw}_n\\
        \vdots & \vdots & \ddots & \vdots \\
        \alpha^{hw}_1 & \alpha^{hw}_2 & \hdots & \alpha^{hw}_n\\
    \end{pmatrix}
\end{align}
with
$$
\mathbb{1}_{1 \times N} = \begin{pmatrix}
        1 \\
        1 \\
        \vdots\\
        1 \\
    \end{pmatrix}^T
$$

Then $\beta^{hw}$ is strictly triangulated such that:
\begin{align}
    \gamma^{hw} & = \beta^{hw} \odot
    \begin{pmatrix}
        0 & 0 & 0 &\hdots & 0\\
        1 & 0 & 0 &\hdots & 0\\
        1 & 1 & 0 &\hdots & 0\\
        \vdots & \vdots & \ddots & \ddots & \vdots \\
        1 & 1 & \hdots & 1 & 0\\
    \end{pmatrix}
    \\ & = 
    \begin{pmatrix}
        0 & 0 & 0 & \hdots & 0\\
        \alpha^{hw}_1 & 0 & 0 & \hdots & 0\\
        \alpha^{hw}_1 & \alpha^{hw}_2 & 0 & \hdots & 0\\
        \vdots & \vdots & \ddots & \ddots & \vdots \\
        \alpha^{hw}_1 & \alpha^{hw}_2 & \hdots & \alpha^{hw}_{n-1} & 0\\
    \end{pmatrix}
    \\
    \rightarrow \delta^{hw} = 1 - \gamma^{hw} & = 
    \begin{pmatrix}
        1 & 1 & 1 & \hdots & 1\\
        (1 - \alpha^{hw}_1) & 1 & 1 & \hdots & 1\\
        (1 - \alpha^{hw}_1) & (1 - \alpha^{hw}_2) & 1 & \hdots & 1\\
        \vdots & \vdots & \ddots & \ddots & \vdots \\
        (1 - \alpha^{hw}_1) & (1 - \alpha^{hw}_2) & \hdots & (1 - \alpha^{hw}_{n-1}) & 1\\
    \end{pmatrix}
\end{align}

By multiplying the elements along each row in $\delta^{hw}$ one gets:

\begin{align}
    \epsilon^{hw}_i = \prod^N_{j=1} \delta^{hw}_{ij} & =
    \begin{pmatrix}
        1  &\\
        (1 - \alpha^{hw}_1) &\\
        \vdots &\\
        \prod^{N-1}_{j=1} (1 - \alpha^{hw}_j)
    \end{pmatrix}
    \\ \rightarrow \tilde{\alpha}^{hw} = \epsilon^{hw} \odot \alpha^{hw} & =
    \begin{pmatrix}
        \alpha^{hw}_1 & &\\
        \alpha^{hw}_2 & (1 - \alpha^{hw}_1) &\\
        \alpha^{hw}_3 & (1 - \alpha^{hw}_2) & (1 - \alpha^{hw}_1)\\
        & \vdots &\\
        \alpha^{hw}_N & \prod^{N-1}_{j=1} (1 - \alpha^{hw}_j)
    \end{pmatrix}
\end{align}

This vectorized version of alpha blending will introduce a new possible bottleneck as it is, since $\beta^{hw}$ will be a tensor of shape $(N, N, H, W)$, which will equate to
$$
256 \times 256 \times 256 \times 256 \times 4 \si{\byte} = 2^{36}\si{\byte} = 64\si{\gibi\byte}
$$
alone.

This is where the previous position-aware alpha blending tricks becomes useful.
By computing $\beta^{hw}$ only through the top $k$ values of $\alpha^{hw}$ instead of the full tensor $\alpha^{hw}$, the size will be reduced to
$$
k \times k \times 256 \times 256 \times 4 \si{\byte} = k^2 2^{22}\si{\byte} = k^2 \times 4\si{\mebi\byte}
$$
as the shape is reduced to $(k, k, H, W)$.

Ultimately, this accelerates optimization by a factor of $2-3$, as it will be shown in \ref{sec:exp:vectorization}.

It must be mentioned that the upper boundary for computational complexity in using this kind of alpha-blending is $\mathcal{O}(N \log k)$, since the top $k$ search is bound by this complexity.
\todo{calcualte this}

\subsubsection{Losses}
\labsubsubsec{losses}

In this section, the different kinds of losses for the optimization procedure will be discussed.

First off, the L2 loss or \textbf{mean squared error} is an obvious choice for this task.
\begin{align}
    L_{\text{MSE}} = \frac{1}{HW} \sum_{p \in \text{pixels}} \norm{z(p) - z'(p)}_2^2
\end{align}

Since the MSE loss focuses on minimizing the pixel-wise error between the target image and the fully rendered approximation by brushstrokes, it will cause the rendered image to match the target image mainly in color.
At the same time, MSE loss is prone to blurring, which results in washed out edges in the rendered image.
Thus, this loss is must be accompanied by additional losses to make up for the shortcoming of MSE loss.

A popular choice for preserving the content in an image, which is associated with preserving edges, is \textbf{perceptual loss}.
Perceptual loss is based on a VGG Network \cite{VGG} that is pre-trained on ImageNet \cite{imagenet}.
To compute the loss, the activations of deep layers (usually the fourth convolutional block) of the pre-trained VGG network are inferred and then compared using MSE loss.
\begin{align}
    L_{\text{perceptual}} = \frac{1}{H_f, W_f} \sum_{p \in \text{pixels}} \norm{f(p) - f'(p)}_2^2
    \labeq{percep}
\end{align}
The resulting distance is meant to capture how well edges between the two input images are preserved which should be equal to whether the content in both images is the same.
Together with MSE loss, a perceptual loss is often used to get better reconstructions than with MSE loss alone, as edges of objects in the image are better preserved, prohibiting blurriness that would occur otherwise.

As an extension to perceptual loss, \citeauthor*{lpips} introduced perceptual similarity or \textbf{LPIPS loss}, which weighs the different layers of the pre-trained VGG-network differently in order to increase the effectiveness of perceptual distance between two images \cite{lpips}.
LPIPS loss is meant to preserve edges even better than perceptual loss does with a similar computational overhead.

Besides losses that operate in pixel space, it is also necessary to restrict the action space for each brushstroke.
As explained in section \ref{ssec:render:dataset}, the renderer has been trained on a limited data set, which puts constraints on how curved brushstrokes may be and how the ratio between length and width ought to look like.
These constraints must be enforced in the optimizing process as well.
Because the renderer has not been trained on data outside of the generated data set, the renderer will likely break if the input parameters lie too far outside the training space.
The results would then be renderings with no output, distorted brushstrokes, or just noisy output, as seen in \ref{ssec:ablation:renderer}.

Thus, one must think of an additional loss to confine the parameter space to the same space as the generated brushstroke data during optimization.
There are two ways of achieving this:
\begin{itemize}
    \item Discriminators
    \item Explicitly coded losses
\end{itemize}

\paragraph{Discriminators} are a popular choice in this context because even if the data distribution is not known beforehand, a discriminator is still able to learn the distribution from data and thus point out wrong parameter combinations in this case.
Still, a discriminator comes with a few compromises, as the target distribution will never be entirely learned rather than well approximated by it.
This leaves room for weaknesses as well as local minima in the discriminator's prediction, which would result in worse quality for this task.
Usually, these weaknesses are made up for during adversarial training as if the generator overfits to such weaknesses, and the discriminator will quickly penalize such a solution.
In the optimization routine, which is employed for this problem, it is not possible to train the discriminator online as the limited amount of brushstrokes will allow the discriminator to overfit the problem easily.
Thus only a pre-trained discriminator with its said weaknesses can be used in this case.

\paragraph{Handpicked losses} As the data distribution for the generated data set is actually known in this case (see \ref{sssec:creation}), it is also possible to manually define losses that confine the brushstrokes.
The width constraint -- as a first example -- can easily be enforced by penalizing whenever the brushstroke's width $w(x)$ is more than half the length $l(x)$ between the start point $\vec{s}(x)$ and the end point $\vec{e}(x)$ of the brushstroke $x$:
\begin{align}
    L_{\text{bs}} & = \frac{1}{\vert X \vert}\sum_{x \in X} \max(0, 2w(x) - l(x)) \\
    & = \frac{1}{\vert X \vert}\sum_{x \in X} \max(0, 2w(x) - \norm{\vec{s(x)} - \vec{e(x)}}_2)
\end{align}

This is a bit more complicated regarding the limitation that is introduced to the control point $\vec{c}(x)$.
As $\vec{c}(x)$ was sampled from a multivariate Gaussian with fixed parameters in the data set, it should now follow a similar distribution in relation to the direction $\vec{s}(x) - \vec{e}(x)$ of each stroke.

This can be achieved by first defining two orthonormal basis vectors which are either parallel $\vec{n}_{se}^\parallel(x)$ or orthogonal $\vec{n}_{se}^\bot(x)$ to the directional vector $\vec{s}(x) - \vec{e}(x)$:
\begin{align}
    \vec{n}_{se}^\parallel(x) & = \frac{\vec{s}(x) - \vec{e}(x)}{\norm{\vec{s}(x) - \vec{e}(x)}_2} \\
    \vec{n}_{se}^\bot(x) & = R_{\pi/2} \frac{\vec{s}(x) - \vec{e}(x)}{\norm{\vec{s}(x) - \vec{e}(x)}_2} \\
    \text{with } R_{\pi/2} & =
    \begin{pmatrix}
        \cos \pi/2 & -\sin \pi/2 \\
        \sin \pi/2 & \cos \pi/2
    \end{pmatrix}
\end{align}

Then $\vec{c}(x)$ can be projected into the coordinate system spanned by  $\vec{n}_{se}^\parallel(x)$ and $\vec{n}_{se}^\bot(x)$:
\begin{align}
    c^\parallel(x) & = (\vec{c}(x) - \vec{a}(x)) \cdot \vec{n}_{se}^\parallel(x)  \\
    c^\bot(x) & = (\vec{c}(x) - \vec{a}(x)) \cdot \vec{n}_{se}^\bot(x)  \\
    \vec{a}(x) & = \frac{\vec{s}(x) + \vec{e}(x)}{2}
\end{align}

Now, the axes of the original multivariate distribution co-align with $\vec{c}^\parallel(x)$ and $\vec{c}^\bot(x)$.
By calculating the mean and standard deviation along these projections, they can be compared to the parameters of the original data distribution.
\begin{align}
    \vec{\mu} & = \frac{1}{\vert X \vert}\sum_{x \in X} \begin{pmatrix} c^\parallel(x) \\ c^\bot(x) \end{pmatrix} \\
    \mat{\Sigma} & = (\frac{1}{\vert X \vert} \sum_{x \in X}
        (\begin{pmatrix} c^\parallel(x) \\ c^\bot(x) \end{pmatrix} - \vec{\mu})
        (\begin{pmatrix} c^\parallel(x) \\ c^\bot(x) \end{pmatrix} - \vec{\mu})^T
        )^{\frac{1}{2}} \\
\end{align}

Using the Kullback-Leibler divergence for multivariate Gaussian distributions, the compliance with the data sets distribution can be checked:

\begin{align}
    \L_{\text{KL}} = \frac{1}{2}\left[\log\frac{|\Sigma|}{|\tilde{\Sigma}|} - d + \text{tr} ( \Sigma^{-1}\tilde{\Sigma} ) + (\mu - \tilde{\mu})^T \Sigma^{-1}(\mu - \tilde{\mu})\right]
\end{align}

with 
\begin{align}
    \tilde{\mu} & = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \\
    \tilde{\Sigma} & = \begin{pmatrix} \frac{1}{200} & 0 \\ 0 & \frac{1}{25} \end{pmatrix} \text{ see \eqref{eq:Sigma}}
\end{align}

since the origin of the projection is at $\vec{a}(x) = \frac{\vec{s}(x) + \vec{e}(x)}{2}$ and coincides with the center of the data distribution.

In theory, $L_{\text{KL}}$ and $L_{\text{bs}}$ should be able to capture any deviation from the source data and ensure that parameters stay within the training space of the renderer.
One problem that obviously could arise with this formulation is for patches with very similar brushstrokes that show a mean other than $\tilde{\mu}$ as well as a very low values inside the covariance matrix and non-zero off-diagonal values.
Thus it will be favorable to include as many strokes as possible when calculating this loss, ideally all strokes in the global parameter catalog.

\subsection{Style Transfer}
\labsubsec{opt:styletransfer}

One question that arises when discussing the optimization procedure is whether style transfer could be performed with this approach.
Especially, since the optimization procedure has explicitly been compared to the approach by \citeauthor*{gatys}~\cite{gatys}, it is natural to assume that such an approach could also be applicable in this case.

This mainly requires to introduce a style loss as it has been done by \citeauthor*{gatys}~\cite{gatys} since a content loss is already in place (see~\eqref{eq:percep}).
A style loss can then be implemented similarly by aggregating the activations of more layers and then calculating the gram matrices:
\begin{align}
    \L_{\text{style}} & = \sum_{l=0}^L \frac{w_l}{4 N^2_l M^2_l} \sum_{ij} (G_{ij}^l - A_{ij}^l)^2
    G_{ij}^l & = \sum_k F_{ik}^l F_{jk}^l
\end{align}

The problem, which arises when trying to apply style transfer with this particular approach is the partial update routine.
Since the gram-matrices are meant to catch global second-order statistics of the image, a small patch would be object to a wrongful assumption that the patch represents the whole image.
The only chance of dealing with a local patch in a global context in image space is a cached version of the whole current image in which the patch is embedded.

\subsection{Optimization Details}
\labsubsec{opt:details}

As it has been explained in \refsec{opt:partialupdates}, it is not possible to optimize brushstrokes for the whole image in parallel.
This is partly due to the neural renderer's memory requirements, which scales with the number of rendered patches.
The memory requirements of placing and blending, on the other hand, scales with the number of rendered brushstrokes as well as the number of surrounding fixed brushstrokes and the patch window size.
Experiments have shown that a combination of 256 rendered brushstrokes with 128 surrounding brushstrokes paired with a patch window size of 320x320 pixels occupies around $9.5 \si{\gibi \byte}$ of memory which leaves enough space for more advanced losses and tweaks to the network architecture.

The learning rate for the optimization procedure can be significantly larger than for training the neural renderer.
With a learning rate of $0.01$, the optimization procedure will converge significantly faster to a solution without any instability issues.

One choice, which has to be made individually per target image is how many brushstrokes will cover the image.
As larger images obviously require more brushstrokes than smaller images, what should remain the same is the \textbf{brushstrokes density}.
The brushstroke density will decide how many pixels on average should be covered by each brushstroke and thus be used during initialization.
A number of $100 \text{pixels}/\text{brushstroke}$ has produced the best results during the experiments.

Another important choice that goes along the choice of how dense the brushstrokes should be distributed is that of how many optimization steps each brushstroke will be object to.
As for too few steps, the training will not have converged, and for too many, optimization will take an unnecessary amount of time.
As this can vary between images, since some images require more time to converge, about $1500 \text{steps}/\text{brushstroke}$ have been empirically chosen.
There is no direct enforcement that each brushstroke is updated exactly this often, but it can be expected that due to the uniform sampling of render patches, there will be no major deviation for some brushstrokes.
Since 256 brushstrokes will be optimized in every step, the total number of optimization steps can be calculated together with the brushstroke density and the image's size.

Another minor detail is the fact that for each render patch, five consecutive optimization steps are performed as this safes memory bandwidth since data must be written and read from memory each time a different render patch is optimized.

\subsection{Results}
\labsubsec{results}

Results can be seen in \reffig{opt:results:starry_night} for Starry Night as the standard reference image of this thesis.
It took approximately 15,000 optimization steps, which corresponds to about $2\si{\hour}$.
The rendering consists of roughly 10,000 brushstrokes.

\reffig{opt:results:photo} shows the result for a natural photo as target image which took
also $~2\si{\hour}$ to compute with 8.000 brushstrokes and 12.000 optimization steps.

