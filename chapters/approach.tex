\setchapterpreamble[u]{\margintoc}
\chapter{Approach}
\labch{Approach}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Motivation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Motivation}
\labsec{motivation}

%previous approaches have several problems
%-very low resolution only (learning2paint)
%-no brush strokes (style transfer)
%-bad reconstruction(SPIRAL)
%-hand-crafted/do not generalize (painterly rendering)
%-bad brush storkes (brushs troke extraction)
%-train whole network for a single image(drawingnet)
%-too inefficient (genetic algorithms)
%
%pick an orthogonal approach to drawing networks
%seems as a step back but could show whether it is possible at all
%emphasize more on the decoder quality than others did
%
%Goal is twofold: extract brush strokes is first
%see whether this stylizes normal images as well.

This thesis main goal is to extract a set of brushstrokes, that collectively reconstructs the image of a given painting. \\
The way these brushstrokes have to fit together creates dependencies between them.
It is important where brushstrokes intersect or whether they run in parallel.
Therefore, a brushstroke can not be placed without considering other brushstrokes in the same region.

\todo{argue that placing one piece in tangram includes planning ahead. -> problem with sequential approaches.}
A simple yet similar example is the game of Tangram.
\begin{marginfigure}
    \includegraphics{tangram}
    \caption[]{An Example of Tangram.}
    \labfig{tangram}
\end{marginfigure}
Tangram is a Chinese puzzle game that has the objective of replicating a given silhouette with a set of 7 unique shapes.
All of these shapes have to be used and they may not overlay or be cut.
To solve the puzzle it is necessary to consider rather all parts together than a single part at a time.
Changing the position or rotation of one shape has consequences for all other shapes.\\
Quite similarly, the parameters of one brushstroke affects all nearby brushstrokes as well.
The fact that brushstrokes can overlap, have different colors and complex paths makes the matter even worse.

Thus, existing approaches are currently not capable of solving this complex task sufficiently.
\refch{ACV} presented many such methods which either aim at stylizing images or extracting brushstrokes.\\
Those which extract brushstrokes, are only capable of doing so for about half of all brushstrokes in a painting.
Such shortcomings are acceptable for further analysis, which prefers few accurate brushstrokes over many inaccurate ones.
However, it will not suffice for reconstructing the entire image.\\
Neural style transfer, which builds on filter-based approaches, is generally not suited for this problem.
Since a set of pixels is directly transformed into another set of pixels, there is no brushstroke data available.\\
Painterly renderings do not suffer from such problems.
They are inherently based on using brushstrokes to generate an entire image.
Most of these approaches focus on stylizing images, but few have tried to imitate paintings as well~\cite{paintbot,lpaintb}.\\
Both approaches trained recurrent neural networks specifically to reconstruct a single image.
The resulting approximations of paintings would resemble the original input, but not on a brushstroke level.
Even though they claim, that the trained neural network could be reused for style transfer, the use of a prediction network is likely a burden to the approach.
Ultimately, a set of parameters is sought and training a network to predict these parameters adds overhead in most scenarios.
Having such a network only really makes sense if it can be reused on many images.

This thesis will present an approach which is orthogonal to many recent implementations of drawings networks and painterly rendering.
Instead of training a network to predict the brushstroke parameters, the parameters are optimized directly.

Early painterly rendering approaches built on a similar set-up.
They would formulate an energy function to describe how well the parameters describe the image~\cite{hertzmann_review}.
Finding a minimum for these energy functions requires advanced searching algorithms like evolutionary algorithms~\cite{genetic_algo}.
\begin{marginfigure}
    \includegraphics{genetic_starry_night}
    \caption[]{'The Starry Night' approximated by a genetic algorithm using only circles. \url{https://effyfan.com/2018/03/02/w6-van-gogh-flowfield/}}
    \labfig{genetic}
\end{marginfigure}
Albeit being capable of reconstructing images using pretty much any shape (also called unit), they have a major draw-back.
Such search algorithms are often inefficient since they are based on random sampling.
The sampling becomes even more inefficient if the number of degrees of freedom per unit is large, as it is the case for brushstrokes.

A solution to this problem is featured in many recently published approaches that use drawing networks.
Some reference \citeauthor*{worldmodel}'s idea of a world model~\cite{neuralpainters, learning2paint, strokenet} and present what is called a \textbf{neural renderer}.\\
Such a neural renderer makes the process of placing a brushstroke on canvas differentiable.
Thereby, a prediction network can learn easier compared to an approach without such a neural renderer.\\
Combining a differentiable renderer with a direct optimization-based approach would mitigate the inefficiency which other approaches suffered from.
Instead of randomly searching for minima, it would become possible to follow the gradient towards a minimum.
Thus, the approach of this thesis will consist of two components. \\
\begin{enumerate}
    \item A differentiable renderer which can generate images of brushstrokes from a parameter representation.
    \item An optimization procedure that iteratively approximates an image through brushstroke representations.
\end{enumerate}
Notably, \citeauthor*{gatys}'s approach to style transfer uses a similar set-up but optimizes pixels instead of brushstroke parameters.

The following two sections will present how each component of the approach has been realized.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Neural Renderer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Neural Renderer}
\labsec{NeuralRend}

Generating brushstrokes artificially is a complex task.
There exist plenty of applications that are dedicated to this.
It is still possible to discern real brushstrokes from application-generated ones, even though some research is bridging this gap~\cite{wetbrush}.\\
It would be even harder to think of a differentiable way to generate brushstrokes from a set of parameters.

Neural networks solve this issue by finding a differentiable pipeline through training.
Previous works have shown that neural networks are capable of conditionally generating high-resolution and high-quality images and neural networks are inherently differentiable.
Therefore, it requires only data of brushstrokes and their parameters to train the network.\\
As previously stated, many already did so.
However, these approaches do not focus on the quality of the renderer.
Thus, they use simple painting applications like \citetitle{libmypaint} to generate the training data.
In this approach there shall be more emphasis on the render quality.

\subsection{Data Set}
\labsubsec{dataset}
There is no data sets openly available for this task
Therefore, a data set must be created explicitly for this approach, for which a painting tool (or something similar) has to be picked.\\
Three qualities are important when is comes to creating the data set:
\begin{enumerate}
    \item Suitable data format: As brushstrokes usually overlap in a painting, the data should already provide information about opacity or transparency, favorably in the common RGBA format.
    \item Data set size and variability: Only with enough different data points available, it will be possible to train a renderer reliably.
    \item Image quality: Brushstrokes should be as close to real-world brushstrokes as possible to ensure high-quality renderings.
\end{enumerate}
\marginnote{RGBA has a fourth channel besides the three color channels, which hold information about the opacity of every pixel.}

\subsubsection{Brushstroke Images}
There are some data sets with single scanned hand-drawn brushstrokes available on the internet~\cite{zolee_on_onlygfx}.
Some even come with the background removed on which they were drawn on.\\
Such data has the advantage that it consists of actual real-world brushstrokes.
On the other side, the data is very sparse.
Brushstrokes are very similar throughout the data set considering their path and color and come without any further information.
This would have the generator to interpolate data in the best case or not work for any unknown inputs in the worst case.\\
These limitations to the data make it unlikely that a generator could learn a coherent representation from it.

\subsubsection{Painting Libraries}
Painting libraries such as libmypaint are the de facto standard for neural renderer in drawing networks~\cite{neuralpainters, learning2paint, strokenet}.\\
They allow to fully control the output through parameters and thus make training for the renderer easier.
Still, this data falls short regarding the authenticity of rendered strokes, which are obviously computer generated.\\
This kind of data would allow to train a renderer, though it would likely cause the final images to look a bit ``flat''.

\subsubsection{Fluid Simulations}
Fluid simulations like those by \citeauthor*{wetbrush} give a better render quality compared to painting libraries. 
Unfortunately, only few implementations are openly available and if so are not as advanced.\\
\citeauthor*{SPIRAL} provide a fluid simulation package based on \citetitle{fluidpaint}~\cite{fluidpaint}.
It uses simple fluid dynamics to give artificial brushstrokes a more plastic look.
One problem, that comes with all simulations is that the brushstroke path not controlled directly, but by moving a virtual paintbrush.
Nevertheless, it is possible to partly account for this when creating the data set.\\
Another problem is the simulation time.
State-of-the-art approaches are just capable of rendering in real-time which is too slow for an entire data set.
Luckily, \citetitle{fluidpaint} is not as computationally expensive and can be parallelized to generate $100,000$ samples in about two hours.

All in all, \citetitle{fluidpaint} has been chosen as the tool to generate the data set.
The superior render quality justifies the additional work required to generate the data set.\\
There are other applications which are capable of simulating brushstrokes reasonably well like \citetitle{artrage}~\cite{artrage} and \citetitle{krita}~\cite{krita}.
Unfortunately, these application do not provide an interface to efficiently generate a data set.
\begin{marginfigure}
    \subfloat[Photograph]{%
        \includegraphics[width=.8\textwidth]{images/real_red_stroke}%
        \labfig{strokes:real}%
    }\qquad
    \subfloat[ArtRage]{%
        \includegraphics[width=.8\textwidth]{images/artrage_red_stroke}%
        \labfig{strokes:artrage}%
    }\qquad
    \subfloat[libmypaint]{%
        \includegraphics[width=.8\textwidth]{images/affinity_red_stroke}%
        \labfig{strokes:libmypaint}%
    }\qquad
    \subfloat[FluidPaint]{%
        \includegraphics[width=.8\textwidth]{images/fluidpaint_red_stroke}%
        \labfig{strokes:fluidpaint}%
    }
    \caption[]{Comparison of similar brushstrokes in each data set}
    \labfig{strokes}
\end{marginfigure}

\subsubsection{Brushstroke Formalism}
The means of brushstroke production are now seized and what is left is to formulate the parameters that define a brushstrokes.
These parameters must quantify the following three properties of brushstrokes:
\begin{itemize}
    \item color
    \item thickness/brush size
    \item path/curve
\end{itemize}

Naturally, color is approximated by three 8-bit numbers in the RGB format.\\
Path and thickness depend on the given coordinate scaling which is $[0, 1]$ in \citetitle{fluidpaint}.
Valid brush sizes are any value in $[0, 1]$ from an infinitely small brushstroke ($0$) to one as wide as the canvas ($1$).
Both these edge cases do not make sense in this approach, thus the range is constrained to $[.03, .2]$.
This range includes only brushstrokes that are visible but do not cover the whole canvas.\\

Quantifying the path is a little more tricky.
\citetitle{fluidpaint} is internally only able to move the brush in a straight line between $a$ and $b$ for a single time step $t \rightarrow t+1$.
Subsequently, any curved paths must be split into linear/straight segments that together approximate a curved line.
As more time steps mean longer simulation times and fewer steps mean edgy curves, a value of 20 time-steps per stroke emerged as a good compromise.\\
There are additional issues which stem from the fact that \citetitle{fluidpaint} gives only control over the brush's path.
Due to the length of the bristles an offset is introduced.
Also, the bristles need to be aligned such that a ``clean'' brushstroke is drawn.
Further information on how this has been implemented can be found in the appendix (see \refch{fluidpaint}).
\begin{marginfigure}
    \includegraphics{bezier_sample}
    \caption[]{Virtual bristles in \citetitle{fluidpaint} as they are initialized.}
\end{marginfigure}
\todo{add figure of bristles/fluidpaint bristles}

Another question is how to express a curved path in numbers for the neural renderer.
A straightforward representation would be a sequence of points that make up the curved path.
Such an approach allows for the highest versatility.
\begin{marginfigure}
    \includegraphics{bezier_sample}
    \caption[]{(a) typical path of a brushstroke in a painting. (b) a rarely observed path~\cite{onlygfx}.}
    \labfig{genetic}
\end{marginfigure}
At the same time it introduces a noticeable amount of parameters as each point consists of 2 coordinates, totaling 40 additional dimensions.
Also, each point is not independent of the other points but should lie withing a certain range and follow a reasonable path. 
Otherwise, the resulting brushstrokes would resemble more of a scribble than what a brushstroke is expected to look like.

Drawing networks often face the same issue and many rely on Bézier curves as a parametrization~\cite{SPIRAL, neuralpainters, learning2paint}.\\
\textbf{Bézier curves} parametrize curved paths and are often used in computer graphics.
Instead of linear segments, they define analytical paths for curves of different orders.
In this case, quadratic Bézier curves are already sufficient.\\
A curve is defined by its start and end point, as well as a control point (more points for higher orders).
These points are connected by lines such that control points have two lines connecting them, and start- and end points only one line.
Over the time interval $[0, T]$ a virtual point moves along each of these lines linearly.
\marginnote{The linear motion starts at the point with the lower index.
The start point has the lowest index, the end point the highest index.
Any control points in between get higher indices in the order they are placed.
Thus a virtual point will always start at the start point $t=0$ and end at the end point $t = T$.}\\
As two such points, in the quadratic case, give two paths a another line is introduced to connect these two virtual points.
This line will now change as the virtual points move linearly in $[0, T]$.
Another virtual point is then placed on this line and also moves linearly in $[0, T]$.
This point will follow a more complex path as the line on which it moves, is not static anymore.
Thus, the path of this point defines a curve as originally intended.\\
A quadratic Bézier curve will only bend into one direction or follow a straight path.\\
\marginnote{For higher orders, the displayed process can be applied iteratively and allows for more complex curves.}
As brushstrokes usually follow a rather simple path and fewer parameters are preferred, quadratic Bézier curves are used as parametrization.
This is also the case in many drawing networks~\cite{SPIRAL, neuralpainters, learning2paint}.
\begin{marginfigure}
    \includegraphics{bezier_sample}
    \caption[]{Sample of a 3rd degree Bezier curve, using the De-Casteljau-algorithm, \url{https:\/\/de.wikipedia.org\/wiki\/Bézierkurve\#\/media\/Datei:Bezier-cast-3.svg}}
    \labfig{genetic}
\end{marginfigure}

Ultimately, there will be ten values that are sufficient to parametrize brushstrokes with certain constraints:
\begin{itemize}
    \item Three 2D coordinates that define the Bezier curves (six values).
    \item One brush size parameter.
    \item Three values in RGB space.
\end{itemize}

\subsubsection{Data Constraints}
This section already mentioned that data cannot be generated in real-time for the training, instead it is generated beforehand.
A data set of $100,000$ brushstrokes is sampled in the ten-dimensional feature space that has been introduced.
However, the data still needs further constraints to facilitate the renderer's training.\\
Such constraints should limit the data to valid brushstrokes only.
``Valid'' brushstrokes will be defined as brushstrokes that resemble the shape of many real-world brushstrokes.
This primarily concerns two relations within a brushstroke:
\begin{itemize}
    \item Its width-to-length ratio.
    \item Its curvature.
\end{itemize}

Brushstrokes should be at least two times as long as they are wide.
\begin{align}
    \norm{\vec{s} - \vec{e}} \overset{!}{\leq}  2 \times (\text{brush size}) \labeq{bs}
\end{align}
Due to the simulation in \citetitle{fluidpaint}, shorter brushstrokes will show artifacts caused by the bristles' length.
The length of these bristles directly correlates to the brush size/width.\\
Another motivation is the intended use-case, which will focus on van Gogh paintings.
As van Gogh did not practice pointillism, most of his strokes have a length to them, which brings such a constraint in line with some characteristics of van Gogh's style.\\
Subsequently, $\vec{s}$, $\vec{e}$ and the brush size are sampled together such that $\vec{s}$ and $\vec{e}$ are further apart than two-times the brush size.\\
The brush size is sampled from a uniform distribution in $[.03, .2]$ and $\vec{s}$ and $\vec{e}$ are sampled in a uniform distribution in $[0, 1]^2$.
If the relation
\begin{align}
    \norm{\vec{s} - \vec{e}} \geq 2 \times \text{brush size}
\end{align}
does hold this set of parameters is included in the data set, otherwise the whole set is re-sampled.

Similar constriants apply to the curvature:
Most brushstrokes (especially those by van Gogh) have a certain ``flow'' or ``smoothness'' to them.
``Smooth'' brushstrokes do not follow a kinky path but feature curvatures with large radii.\\
\begin{marginfigure}
    %\includegraphics{patch_starrynight}
    \caption[]{Brushstrokes by van Gogh in a painting (The Starry Night).}
\end{marginfigure}
Thus, the data set will be restricted to brushstrokes that follow these descriptions.
To achieve this with random sampling in place, a multivariate Gaussian distribution is placed between start ($\vec{s}$) and end point ($\vec{e}$).
The Gaussian is parametrized by an offset $\vec{mu}$ and a covariance matrix $\mat{\Sigma}$.
$\vec{mu}$ is picked such that the distribution is centered between $\vec{s}$ and $\vec{e}$.
\begin{align}
    \vec{\mu} = \frac{\vec{s} + \vec{e}}{2}
\end{align}
One of the principal axes of $\mat{\Sigma}$ shall align with the directional vector between $\vec{s}$ and $\vec{e}$.
\begin{align}
    \vec{q}_1 & = \vec{s} - \vec{e} \\
    \hat{\vec{q}}_1 & = \frac{\vec{q}_1}{\norm{\vec{q}_1}} \\
    \hat{\vec{q}}_2 & = \mat{R} \hat{\vec{q}}_1 \text{ s.t. } \hat{\vec{q}}_2 \perp \hat{\vec{q}}_1 \\
    \mat{Q} = (\hat{\vec{q}}_1, \hat{\vec{q}}_2)
\end{align}
$\hat{\vec{q}}_1$ and $\hat{\vec{q}}_2$ build an orthonormal 2D basis which are the eigenvectors of the eigendecomposition of $\mat{\Sigma}$ .
The respective eigenvalues $\lambda_1$ and $\lambda_2$ are handpicked such that the distribution is narrow along $\hat{\vec{q}}_1$ but broad along $\hat{\vec{q}}_2$.
\begin{align}
    \lambda_1 & = \frac{1}{200} \\
    \lambda_2 & = \frac{1}{25} \\
    \mat{\Lambda} & = \begin{pmatrix} \lambda_1 & \\ & \lambda_2 \end{pmatrix}
\end{align}
Thus, the eigendecomposition of $\mat{\Sigma}$ is known and $\mat{\Sigma}$ can be constructed from it.
\begin{align}
    \mat{\Sigma} = \mat{Q} \mat{\Lambda} \mat{Q}^T
\end{align}
Figure \ref{fig:datageneration} shows samples from this distribution for an exemplary set of start and end point.
\begin{marginfigure}
    \resizebox{\textwidth}{!}{
        \input{images/scatter.pgf}
    }
    \caption[]{Exemplary scatter plot for given start and end point to visualize the covariance matrix}
    \labfig{scatter}
\end{marginfigure}

This distribution is intended to follow the expected distribution of brushstrokes as they appear in the real world.
The majority of brushstrokes will be straight or just slightly bent due to the maximum of the PDF of the Multivariate Gaussian being at the center $\vec{\mu}$.
Bent brushstrokes will mostly be symmetric as $\hat{\vec{q}}_2$ is the long axis of the multivariate Gaussian.
Thus, the region which is of most interest will be densely populated.\\
Still, some brushstrokes will have their bent slightly towards either end of the brushstroke and others will feature tight curvature.
\begin{align}
    p(\vec{c}| \vec{s}, \vec{e}) & = \mathcal{N}(\vec{\mu}, \mat{\Sigma}) \labeq{checkpoint} \\
\end{align}
If $\vec{c}$ lies outside $[0, 1]^2$ it is re-sampled.\\
$\vec{s}$, $\vec{e}$ and $\vec{c}$ are also scaled with the handpicked factor of $0.7$ to ensure the brushstrokes are rendered completely within the window and not cut by an edge of the render window.

The color of the brushstrokes is not constrained as it should be possible to fit any color distribution equally well.
Each RGB value is sampled from a uniform distribution as a color.

The resulting tuple of start, end and control point, brush size, and RGB color is then rendered.

The render canvas size was chosen to be 64x64 pixels for several reasons:
First, even with such a small canvas size, training of the renderer takes about one day.
Secondly, the larger the render canvas size becomes, the deeper the renderer needs to be, which results in more computational overhead not only in training but in the optimization procedure as well.
Lastly, there will be upwards of a thousand brushstrokes in a single image and increasing the canvas size to 128x128 would require four times as much memory per rendered image.
As 1000 brushstrokes would already account for $1000 x 64 x 64 x 4 \si{\byte} \approx 16.4 \si{\gibi \byte}$ a four-fold increase is significant.

After rendering, the data set is renormalized to the range $[-1, 1]$ for convenience and to facilitate training as well.

$100,000$ samples that follow these constraints make up the training data set.
The constraints helped in fact to train the renderer.

\subsection{Architecture}
The architecture of the brushstroke generator follows that of an inverse VGG network.
It is widely used and has shown that it should be capable of handling this task.\\
The architecture consists of three dense layers with 256 dimensions, leaky ReLU activation and instance normalization at the beginning.
The output is then spatially arranged in a 4x4 grid which is the base for the following convolutional blocks.\\
Each blocks applies a convolution with kernel size 3x3 and stride 1x1 followed by a leaky ReLU activation and instance normalization.
At the end of the block the activations are bilinearly upsampled.
The number of kernels decreases logarithmically with every layer.
Notably every convolution has two additional input channels with coordinates along each axis as values.
This is called CoordConv~\cite{coordconv}.
Two other additions to the convolutional block have been inspired by StyleGAN~\cite{stylegan}.
First, the activations are augmented with weighted noise inputs.
The scale of the additional noise is a trainable parameter in the network.
Secondly, a style modulation is applied to the normalized activations.
This means that an additional fully connected pipeline with four layers is implemented to give a set of activations.
These activations are then the inputs to single independent fully connected layers at every scale.
These FC layers predict the affine parameters of every instance normalization.\\
The last convolutional block has neither normalization nor upsampling but a hyperbolic tangent function to restrict the output to $[-1, 1]$.\\

The discriminator is similarly built.\\
It employs the same convolutional blocks with a few changes.
There is no style modulation or noise augmentation.
Instead, there are multi-scale inputs like in MS-GAN~\cite{MSG-GAN}.
Thus, the input image is downscaled to the resolution of each layer and then concatenated to the existing activations of that layer.
Also, instead of upscaling there is max-pool downscaling after each block.
The convolutions have the same kernel size, kernel number and stride in each layer as the generator.\\
When the activation reach a resolution of $4\times4$, they are flattened and serve as an input to a three-layer fully connected network.
These FC layers are again symmetrically configured to the generator.
The last FC layer consists of only eight unit, which act are predictors.

\reffig{architecture} shows a visual depiction of the architectures.

\todo{reinsert these figures}
%\begin{figure*}
%    \resizebox{1.5\textwidth}{!}{
%        \input{images/generator.tex}
%    }
%    \resizebox{1.5\textwidth}{!}{
%        \input{images/discriminator.tex}
%    }
%        \caption{Visualization of the generator and discriminator architectures}\labfig{genarch}
%        \labfig{architecture}
%\end{figure*}

\subsection{Training}
Training is set up in an adversarial fashion.
Thus, the generator predicts a brush stroke form parameters and the discriminator tries to tell is apart from a given sample from the data set.
RaGAN defines the adversarial objective during training.\\
Also the L2 distance is used in the first phase of training until it falls under a threshold of $0.2$.
This way e meaningful decoding is learned in the beginning but the is no blurring in later training stages.
L2 distance and the FID score serve as evaluation metrics during training.
The L2 distance does not qualify as a sufficient metric for later training stages due to the stochastic nature of the brushstrokes.
The FID score serves as a metric for the visual similarity to the data set as the visual comparison of the generated samples has proven difficult between different experimental runs.
\marginnote{The Frechet inception distance (FID) is calculated by comparing the statistics of InceptionNet~\cite{inception}-activations for two batches of samples from the dataset and generated samples~\cite{FID}.
This is somewhat similar to perceptual distance.}

A two-time-step update rule was implemented to stabilize training further.
This way, the discriminator and generator are updated at different frequencies.
If, for instance, the discriminator becomes too strong and the generator's loss increases, the generator is update more often than the discriminator to balance both networks again.

\subsection{Results}
\reffig{brushstroke} shows a generated sample compared to a rendered sample with the same parameters.
The images are split into their RGB channels and the alpha channel to allow evaluating the two independently.
\reffig{single_brushstroke} shows all four channels merged for a generated brushstroke.
It becomes immediately clear that the neural renderer is capable of imitating \citetitle{fluidpaint}.
\begin{marginfigure}
    \resizebox{\textwidth}{!}{
        \input{images/scatter.pgf}
    }
    \caption[]{Generated sample and the corresponding data set sample compared.}
    \labfig{scatter}
\end{marginfigure}
\begin{marginfigure}
    \resizebox{\textwidth}{!}{
        \input{images/scatter.pgf}
    }
    \caption[]{Generated brushstroke.}
    \labfig{scatter}
\end{marginfigure}

Further analysis of the results will be presented in the next chapter.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Stroke Approximation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Optimization Procedure}
\labsec{strokeapprox}
The goal of this thesis is to approximate painting images through many parametrized brushstrokes.
In detail, high-resolution input images should be approximated with realistic-looking brushstroke renderings by direct optimization.

In the previous section a way of training the required differentiable renderer was presented.
With the given differentiable renderer it is now possible to backpropagate gradients from the image space to the parameter space.
However, there are many questions concerning the optimization procedure that have to be answered.
Most important is the question how the brush strokes shall be optimized.
Namely, whether brushstrokes should be optimized sequentially or in parallel.\\
In the beginning of the chapter it has already been described how brushstrokes depend on one another.
The placement of a foreground brushstroke directly impacts the brushstrokes that it covers and vice versa.
\eg if a background brushstroke does not cover some canvas for whatever reason, a foreground stroke has to cover up this area later on.
Thus, ideally, when placing brushstrokes in the background, there is a plan in place, how to lay brushstrokes on top of them later on.\\
Drawing networks have to deal with the same issue which makes training them even harder.
Most of these approaches tend to rely on sequential placing of brushstrokes~\cite{SPIRAL, strokenet, learning2paint, neuralpainters, hertzmann}.\\
They often argue that artists place brushstrokes in the same sequential manner.
This is true as artists rarely use multiple brushes at the same time.
However, this leads to an issue when placing the first few brushstrokes in an image.\\
\begin{marginfigure}
    \caption{Series of brushstroke predictions with a sequential approach.}
\end{marginfigure}
It can be observed that these early brushstrokes are all very large and fill up the entire canvas.
This is due to the way these networks were trained.
Usually, L2-distance or a similar metric is minimized by the networks.
As the L2-distance is reduced the fastest by placing large brushstrokes with the mean color of the entire image, these networks tend to do just that.
Afterwards finer brushstrokes are placed, which usually become increasingly finer over time.
Arguably, this checks out with reality.
Artists would also paint a background first and then place foreground objects on top.
Nevertheless, artists usually do not cover the whole canvas in a single color but paint different backgrounds where necessary.
Also, few artists use giant brushes for this but sensible brush sizes which still give a lot of control over the paint.
%artists iterate over the image spatially, draw some lines here, they are all realted to each other.
\begin{marginfigure}
    \caption{Canvas in the middle of the painting process.}
\end{marginfigure}

Ultimately, all sequential approaches suffer from this shortcoming as planing this well ahead is really difficult for neural networks.
This problem becomes even more serious when the goal is to extract sensible brushstrokes.\\
Thus, a parallel optimization approach is preferred in this thesis.
In contrast to a sequential approach, a parallel approach places all brushstrokes on the canvas at once.
For an optimization based approach this means placing many brushstrokes on the canvas and then optimizing them such that they compose an image.

Another argument for parallel optimization is the focus on actually visible brushstrokes.
As early-placed brushstrokes are often covered up, it becomes a shot in the dark to guess what these first brushstrokes look like.
Subsequently, they are irrelevant to the reconstruction of the image and only introduces noise to the problem.
A parallel approach does not care how the background could have been drawn but only focuses on what is visible in the given image.\\

The decision of using a parallel approach brings with it many subsequent design decisions which will be explained in this section.
In the end there should be a working pipeline to approximate brushstroke parameters.

\subsection{Data Set}
In order to reconstruct brushstrokes in an image, the data set should consist of relatively high-resolution images.
At the same time, the renderer requires brushstrokes to fit into a 64x64 window.
Any larger strokes can not be approximated with a single rendering window and thus would falsely be approximated by multiple brushstrokes.
Too small brushstrokes would be ignored or combined with surrounding brushstrokes into a single set of parameters.

Thus, the image must be scaled to the right size for optimization.
Therefore, more information per image is required, than just the resolution.
The scale of the image is, hence, necessary information.
All images of paintings should then be rescaled such that the pixel density is equal among all images.
Previous brushstroke extraction approaches did the same and therefore requested high resolution images from galleries~\cite{rhythmic, lamberti}.\\
Through visual examination and experimental evaluation a pixel density of $13 \frac{\text{pixels}}{\si{\centi\meter}} \approx 33 \text{dpi}$ has been determined ideal.
\begin{marginfigure}
    \caption{Size comparison of an image patch and the render window size (taken from ``The Little Arlesienne'').}
\end{marginfigure}

Also, the renderer has explicitly been trained on painting brushstrokes, which requires the target painting to either be oil on canvas or acrylic.
The reason for this as well as the focus on van Gogh as an artist has been explained in the introduction.

It is possible to obtain all the required data from the internet.
Whenever it was possible the preferred source was the webpage of the Van Gogh Museum in Amsterdam~\cite{VG_museum}, as it provides high-resolution images as well as additional information the paintings.

\subsection{Rendering Layout}
The optimization approach has already been compared to the style transfer approach by \citeauthor*~{gatys} \cite{gatys}, with parameters optimized instead of pixels.
However, by going from pixels to brushstrokes, a major problem arises with the loss of apparent structure.
The grid-structure of pixels is already defined and well-known, but usually parameters come without any structure to them.
Especially, since a painting image ($1000\times1000$) is several times larger than the render window ($64\times64$).\\
If both had the same size, this would be easier, since render window and canvas would co-align.
In order to fill the canvas, several render windows can be stacked on top of each other.

This idea can be scaled to a larger canvas size, by placing many such stacks in a grid, which covers the entire canvas.
Each stack would then act like a giant pixel and could be optimized independently.\\
However, this raises a problem.
Since brushstrokes are confined in their render window the giant pixel structure would become apparent when looking at the canvas.
Also, the variability of a brushstroke is significantly limited, since the control point has to lie withing the render window.
Additionally, such a grid structure would cut brushstrokes that cross the edges between the grid cells in half.
All of this is undesirable behavior when the focus lies on approximating brushstrokes.
\begin{marginfigure}
    \caption{The result of testing a non-overlapping grid layout for render windows. }
\end{marginfigure}

An easy solution to this problem is a grid of render windows with a smaller lattice constant.
If the lattice constant is smaller than the render window size (\eg half the window size), then the render windows start to overlap.
As the render windows overlap, there is no need for the brushstrokes to be rendered near the edge of a render window.\\
This principle can be taken further by reducing the lattice constant even more.
At the same time one can go from stacking render windows in the same place to placing just a single render window at every position.
This way, the edges of the render windows are well-hidden.
\begin{marginfigure}
    \caption{Two different layout patterns illustrated.}
\end{marginfigure}

Nevertheless, this kind of initialization requires a very even distribution of brushstrokes.
Ideally there should be brushstroke at the center of each render window.
As this is usually not the case, and stroke densities will vary locally, this is a false assumption. \\
It could even lead to problems that affect the whole image:
The fixed grid structure is unable to account for local changes in density.
Starting with a single region of high-density strokes in the vicinity of one grid cell, the respective brushstroke should be narrow to achieve high accuracy.
Neighboring cells then have to shift their strokes towards the center of that render window to account for the free space that is not covered by the narrow brushstrokes.
These shifts also free up space in the neighboring cells, which their neighbors have to account for, and so on.
A chain reaction is the result which affects many brushstrokes. \\
It is even more likely that instead of having few shifts in the grid, no grid cells shift, since there are many areas of varying stroke density in paintings.
All these shift would then balance each other over the image, and each brushstroke just ignores finer structures.
Such behavior would cause uniform brushstrokes everywhere as all brushstrokes would interfere as little as possible.
Each brushstroke would then behave like weirdly shaped pixel, and the result would be a blurry image.

The core of this problem is the imposed lattice structure that could propagate local density shifts.

One possible solution is getting rid of the lattice structure and replacing it with a non-symmetric structure.
Superpixels are an easy way of evenly spacing brushstrokes without a regular structure.
\marginnote{Superpixels were popular in the 'pre-neural network era' of computer vision and were used in image segmentation tasks \cite{superpixel_segmentation}.
Each superpixel defines a groupd of pixels that is pairwise disjoint to all other superpixel \ie they do not overlap.
Superpixels-like algorithms are also a valid starting point for brushstroke extraction algorithms \cite{lamberti}.}

An ideal optimization procedure should be able to place strokes freely on the canvas, as this allows for an unbiased approximation.
It would allow allocating many small strokes in areas where the artist placed many strokes and use fewer and wider strokes in other areas.
Unfortunately, the render windows have to be explicitly placed on the canvas and such an operation is not differentiable.
Thus, there are no gradients which would allow to optimize the postioning.

\subsubsection{Initilization}
\labsubsubsec{init}
Since the rendering layout has now been explained, two additional parameters shall be added to the set of brushstroke parameters.
These two parameters define a basic position of the brushstroke on the canvas.
The final brushstroke will then lie in the vicinity of that position since $\vec{s}$, $\vec{c}$ and $\vec{e}$ define the exact position inside the render window.\\
All these parameters have to be initialized before the optimizer starts.\\
Ideally, the initialization should not introduce any bias to the optimization process.
At the same time, an initialization should facilitate training and accelerate convergence in the early stages of optimization.\\
The initialization of brushstrokes is closely related to the position of the render window.
For a given set of render window positions the color of each brushstroke can already be approximated.
The color of the closest pixel to the center of the window already gives and idea which color the brushstroke might have.
By using this information the optimization procedure can be accelerated as the color is possible already correct.
If it is not correct, it is still not worse than randomly initializing a color.\\
The brush size will be initialized with the minimum possible value.
This way, brushstrokes do not overlap at the beginning of optimization.\\
Other patch parameters, namely $\vec{s}$, $\vec{e}$ and $\vec{c}$, will be initialized using a narrow Gaussian distribution with $\sigma = .1$ and values clipped to $[-1, 1]$.
There is no prior information available on how the brushstrokes should be oriented, thus they should also not overlap, as it has been argued for the brush size.

\subsection{Rendering Order}
Another problem that will come up during the optimization procedure is the order in which strokes are rendered.
Real-world brushstrokes are also subject to the same issue as a later-painted brushstroke will always be placed on top of brushstrokes which were painted earlier.\\
Parallel optimization does not provide a hierarchy like sequential approaches do.
Thus it must be explicitly stated which strokes are in the foreground and which are in the background.\\
If this would randomly change, edges in the image might be obstructed, and gradients would continually change.
The optimizer could even oscillate between solutions where different strokes lie in the foreground.
Such outcomes would be unfavorable as it tends to produce worse results in the end.

One solution to calculate a brushstroke's accuracy.
The accuracy will be defined as the negative L2-distance of each stroke's visible pixels ($\alpha > 0$) to the corresponding pixels in the target image.
\begin{align}
    \text{accuracy} = - \frac{1}{N} \sum_{p \in \text{pixels}} \norm{\vec{x}_p - y_p}_2^2 * \alpha_p
\end{align}
for $\vec{x}$ the rendered stroke as a vector of RGB values, $\vec{y}$ the corresponding image patch and $\alpha$ the alpha-channel that belongs to $\vec{x}$.

The resulting value describes how well the pixels of the rendered stroke match their respective pixels in the target image.
Consequently, any brushstroke with higher accuracy will be more faithful to the target image than brushstrokes with lower accuracy.
Placing these brushstrokes in the foreground thus should result in a smaller L2-distance for the entire reconstruction.\\
Notably, the accuracy should not be included in the brushstroke's loss, as this would heavily influence the behavior towards very small brushstrokes.
Thus, the accuracy of each stroke will be calculated as it is rendered and placed.

\subsection[Placing \& Blending]{Image Placing \& Blending}
Since it has now been explained how the positions for render windows are determined, it is time present the methods behind placing and blending the brushstrokes.\\

After each brushstroke has been rendered in its $64\times64\si{\pixel}$ window, it needs to be placed according to the translation parameters.
Thus the render window is placed inside a larger zero-valued matrix with the size of the canvas accordingly.\\
By calculating each pixel's global position from the position parameters, it is possible to assign each pixels of the original rendered image into its correct position.
After this step the accuracy of each stroke can be easily calculated as the brushstroke is now in its final position.

The next step is to blend the canvas-sized rendered brushstrokes together (stitching them).
Due to the alpha channel, it is possible to blend only relevant information while the rest of the image will be ignored.
It is important to perform alpha blending according the order that is given by the calculated accuracies.\\
Two images are blended by multiplying each pixel value with the alpha value for the top-layer image while the background image's pixels are multiplied with the complement to the alpha value.
\begin{align}
    \vec{r}_p & = \vec{t}_p \times \vec{\alpha}_p + \vec{b}_p \times (1 - \vec{\alpha}_p) \forall p \in \text{pixels} \\
    \rightarrow \vec{r} & = \vec{t} \times \vec{\alpha} + \vec{b} \times (1 - \vec{\alpha}) \\
\end{align}
for $\vec{t}$ to top-image RGB vector, $\vec{b}$ the bottom-image RGB vector, $\vec{\alpha}$ the alpha channel image vector of the top-image, and $\vec{r}$ the resulting image vector.

For multiple layers, this process can be repeated in various fashions, after the strokes are ordered according to their accuracy.
Either one could start from the bottom and blend the two lowest strokes which generates a new bottom-image. 
This process is repeated until all images a blended together.
The same is imaginable starting with the top two images.
Both of these approaches would yield the same result and require $(n-1)$ blending operations for $n$ layers per pixel.

Blending brushstrokes in a sparse manner can reduce this number.\\
The majority of pixels for each padded brushstroke is non-informative, as the alpha value is zero.
This opens the possibility of go from blending \textit{all} pixels of \textit{all} brush strokes to blending just those pixels with non-zero alpha values.
It is only important to maintain the order which has been imposed before for every pixel during this operation.\\
Before there were many layers that represented one single brush stroke each, in the new representation the layers just correspond to the overall position in the hierarchy.
Each pixel of the top layer will thus be the first non-zero alpha value with its corresponding RGB values when searching through the layers from the top.
\marginnote{If alpha values would be one everywhere in the top-layer this would mean that the top layers is equal to resulting blended image.
Since the alpha values follow $0 < alpha < 1$ for many pixels at the border of each brushstroke, it requires more than one layer.}

Additionally, it is possible to avoid iterative blending.
By transforming the tensor which holds all alpha values for all layers, it is possible to perform alpha blending as a single matrix multiplication with the stack of image layers.
The necessary algorithm is described in \refch{alphablend}.

\subsection{Partial Updates}
When optimizing many brushstrokes in parallel with high-resolution image targets, memory limitations become a guaranteed issue.
Even if it might not be evident at first, the optimization procedure imposes a considerable requirement for memory on the graphics card, due to two parts of the training:\\
First, the number of brushstrokes can easily become very high with large images as input.
With a render window size of $64\times64$, the brushstrokes are relatively small, and paintings can consist of a few thousand brushstrokes.\\
Tests have shown that on the latest hardware with $\approx 12 \si{\gibi\byte}$ of memory, the maximum number of brushstrokes that can be rendered in parallel is $\approx 256$.
Obviously, this is at least one order of magnitude smaller than what would be required.\\
The other problem for large input images is that stitching of brushstrokes (as previously explained) takes up a considerable amount of memory as well.\\
If each brushstroke is placed on the virtual canvas individually, a couple thousand $1\si{\mega\pixel}$ images have to stored in memory temporarily.
As a single 1MP image carries roughly $1000 \times 1000 \times 4 \text{channels} \times 8 \frac{\si{\bit}}{channel} = 32 \times 10^{6} \si{\bit} = 4 \times 10^{6} Byte = 4 MB$ of information, a few thousand of these will easily exceed the memory of most graphics cards.\\
\marginnote{Images are typically saved as unsigned 8-Bit integers, thus $8 \si{\bit} = 1 \si{\byte}$ per channel.}
Still, there is a way around this bottleneck, by optimizing the image not as a whole but as smaller patches consisting of 256 brushstrokes at a time.
%reference the spatial predicitons of lpaintb and paintbot
%is not as easy since layout is not stacked.
\citeauthor*{paintbot} already introduced a similar approach as their network predict brushstrokes for a small window size.
This window then slides over the image and brushstrokes are spatially predicted.\\
\marginnote{Spatially predicting brushstrokes is also an alternative to sequentially predicting brushstrokes or predicting them in parallel.}
A similar approach is picked in this theses, only that instead of a sliding window a randomly jumping window is used.\\
First a patch window with a large enough size (\eg $256\times256$ pixels) is defined.
This patch window is then placed somewhere on the canvas.
A round the center of the window the 256 nearest brushstrokes according to their location are selected.
These 256 brushstrokes are then rendered from their parameters and ordered according to their accuracies.
Finally they are placed on the canvas and blended together.\\
The final rendered patch can then be compared to the respective image patch and the loss can be computed.
The resulting gradient is then back-propagated to update the parameters of the 256 selected brushstrokes.\\
This way a coherent set of brushstrokes is optimized together such that the parallel update routine can still work.

When restricting the number of brush strokes like that, bordercases become an issue.
Brushstrokes that lie at the perimeter of the patch are not fully surrounded by other brushstrokes.
The result is a discrepancy of what the gradient would have looked like if the brushstroke was in the middle of the patch.\\
A solution to this is laying a ring of already rendered brushstrokes around the patch.
The ring guarantees that all brush strokes that brushstrokes at the border of the patch are surrounded by neighboring brushstrokes.
Thus, reducing the impact that the position of the brushstroke in the patch has.\\
Notably, the brushstrokes that belong to the ring around the patch, serve as 'dummy' brushstrokes and are not optimized.
Subsequently, they do not need to be rendered as the other brushstrokes.
Instead, they previously rendered image is reused.\\
For this reason, a \textbf{render image catalog} saves the images for all rendered brushstrokes outside of GPU memory.
Analog to this, the collection of brushstrokes will be called a \textbf{parameter catalog}.\\
The images in the render images catalog are updated whenever their respective brushstroke parameters are updated.

Such a workaround solves both problems that were stated, as both, the number of brushstrokes rendered in parallel, as well as the size of the stitched patches have been reduced.\\

\subsection{Losses}

In order to get the best possible results, different losses are necessary\\
First off, the L2-distance or \textbf{mean squared error} is an obvious choice for this task.
\begin{align}
    L_{\text{MSE}} = \frac{1}{\# \text{pixels}} \sum_{p \in \text{pixels}} \norm{\vec{x}_p - \vec{y}_p}_2^2
\end{align}
with $\vec{x}$ the rendered image vector and $\vec{y}$ the target image vector.\\
The MSE loss focuses on minimizing the pixel-wise error between the target image and the rendered image, this will mainly affect the colors in the image.
However, MSE loss is prone to blurring, which results in washed out edges in the rendered image.
Therefore, this loss must be accompanied by additional losses to reduce the blurring.

A popular choice for preserving the content in an image, which is associated with preserving edges, is \textbf{perceptual loss}.
Perceptual loss has been presented together with \citetitle{gatys} in \refsec{ST}.
\begin{align}
    L_{\text{percep}}(\vec{x}, \vec{y}) = \sum_{i, j} (F(\vec{x})_{ij}^l - F(\vec{y})_{ij}^l)^2 = \norm{\tensor{F(\vec{x})}^l - \tensor{F(\vec{y})}^l}_2^2
    \labeq{percep}
\end{align}
The perceptual distance captures whether content in both images is the same, which includes how well edges between the two input images are preserved.
Together with MSE loss, a perceptual loss is often used to get better reconstructions than with MSE loss alone

As an extension to perceptual loss, \citeauthor*{lpips} introduced perceptual similarity or \textbf{LPIPS loss}, which specifically weighs many different layers of the pre-trained VGG-network in order to increase the effectiveness of perceptual distance between two images \cite{lpips}.
LPIPS loss is meant to preserve edges even better than perceptual loss does with a similar computational overhead.

Besides losses that operate in pixel space, it is also necessary to restrict the action space for each brushstroke.
As already explained in this section, the renderer has been trained on a limited data set, which puts constraints on how curved brushstrokes may be and how what the ratio between length and width may be.
These constraints must be enforced in the optimizing process as well.\\
Because the renderer has not been trained on data outside of the generated data set, the renderer would otherwise break if the input parameters lie too far outside the training space.
Such behavior can be seen in \refsec{ablation_renderer}.\\
Thus, one must think of an additional loss to confine the parameter space to the same space as the generated brushstroke data during optimization.
There are two ways of achieving this:
\begin{itemize}
    \item Discriminators
    \item Handcrafted losses
\end{itemize}

\paragraph{Discriminators} are a popular choice in this context because they require no knowledge of the data distribution beforehand.
A discriminator learns the distribution from the data and then penalizes parameters that do not fit into this distribution.\\
Still, a discriminator comes with a few compromises, as the target distribution will never be entirely learned rather than well approximated.
This leaves room for weaknesses in form of local minima in the discriminator's prediction, which would cause the generator to fit brushstrokes to these minima rather than the image data.\\
Usually, these weaknesses are made up for during adversarial training.
As the generator over-fits to such weaknesses, the discriminator will quickly penalize this solution as it comes up significantly more often that it does in data.
In an optimization routine, such behavior can not be observed as the parameters only change slightly, while a generator comes up with totally new data.
Thus, the discriminator can easily over-fit to the set of brushstroke parameters as it is only a few thousand brushstrokes big.
Thus only a pre-trained discriminator with its said weaknesses can be used in this case.\\
Consequently, a pre-trained discriminator would be required, which in turn has persistent local minima.

\paragraph{Handcrafted losses} require previous knowledge of the data distribution.
As the data distribution for the data set is known in this case, it is possible to compare the distribution of the optimized parameters against the data set.
The width constraint can be enforced by penalizing whenever the brushstroke's width $w_b$ is more than half the length $l_b$ between the start point $\vec{s}_b$ and the end point $\vec{e}_b$ of the brushstroke $b \in B$:
\begin{align}
    L_{\text{bs}} & = \frac{1}{\vert B \vert}\sum_{b \in B} \max(0, 2w_b - l_b) \\
    & = \frac{1}{\vert B \vert}\sum_{b \in B} \max(0, 2w_b - \norm{\vec{s_b} - \vec{e_b}}_2)
\end{align}

The constraint on the distribution of the control point $\vec{c}_b$ can also be checked.
As $\vec{c}$ was originally sampled from a multivariate Gaussian with fixed parameters in the data set, it should now follow a similar distribution.
This requires to account for the position of  $\vec{s}_b$ and $\vec{e}_b$ in relation to $\vec{c}_b$.\\
First, the orthonormal basis $(\hat{\vec{q}}^b_1, \hat{\vec{q}}^b_2)$ with respect to $\vec{s}_b$ and $\vec{e}_b$ must be calculated.
\begin{align}
    \hat{\vec{q}^b_1} & = \frac{\vec{s}_b - \vec{e}_b}{\norm{\vec{s}_b - \vec{e}_b}_2} \\
    \hat{\vec{q}^b_2} & = R_{\pi/2} \hat{\vec{q}^b_1} \\
    \text{with } R_{\pi/2} & =
    \begin{pmatrix}
        \cos \pi/2 & -\sin \pi/2 \\
        \sin \pi/2 & \cos \pi/2
    \end{pmatrix} \text{ s.t. } \hat{\vec{q}^b}_2 \perp \hat{\vec{q}^b}_1
\end{align}
Then $\vec{c}_b$ can be projected into the coordinate system spanned by  $\mat{Q} = (\hat{\vec{q}}^b_1, \hat{\vec{q}}^b_2)$:
\begin{align}
    \vec{c}'_b & = (\vec{c}_b - \vec{a}_b) \mat{Q}
    \vec{a}_b & = \frac{\vec{s}_b + \vec{e}_b}{2}
\end{align}
Now, the mean and the covariance matrix can be calculated for $b \in B$
\begin{align}
    \vec{\mu} & = \frac{1}{\vert X \vert}\sum_{x \in X} \begin{pmatrix} c^\parallel(x) \\ c^\bot(x) \end{pmatrix} \\
    \mat{\Sigma} & = (\frac{1}{\vert X \vert} \sum_{x \in X}
        (\begin{pmatrix} c^\parallel(x) \\ c^\bot(x) \end{pmatrix} - \vec{\mu})
        (\begin{pmatrix} c^\parallel(x) \\ c^\bot(x) \end{pmatrix} - \vec{\mu})^T
        )^{\frac{1}{2}} \\
\end{align}
Since the axes of $\Sigma_B[\vec{c}']$ co-align with the eigenvectors that were defined for the dataset, the Kullback-Leibler divergence can be calculated.
\begin{align}
    \L_{\text{KL}} = \frac{1}{2}\left[\log\frac{|\Sigma|}{|\tilde{\Sigma}|} - d + \text{tr} ( \Sigma^{-1}\tilde{\Sigma} ) + (\mu - \tilde{\mu})^T \Sigma^{-1}(\mu - \tilde{\mu})\right]
\end{align}
with 
\begin{align}
    \tilde{\mu} & = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \\
    \tilde{\Sigma} & = \begin{pmatrix} \frac{1}{200} & 0 \\ 0 & \frac{1}{25} \end{pmatrix}
\end{align}
with the previously chosen eigenvalues already inserted into the equation.
\marginnote{Kullback-Leibler (KL) divergence is a function that allows to measure how well a distribution $q$ matches a given distribution $p$.
For Gaussian distributions the KL divergence is well defined~\cite{KLdiv}}

In theory, $L_{\text{KL}}$ and $L_{\text{bs}}$ should be able to ensure that the optimized parameters fit the distribution of the data set.
Subsequently, parameters tend to stay within the training space of the renderer.\\
One problem that could arise from this formulation is penalization for uniform parameter sets.
For patches with very similar brushstrokes the covariance matrix could show non-zero off-diagonal values or very low variance in general.
The KL divergence would then force the network to unnecessarily diversify brushstrokes.
Thus it will be favorable to include as many strokes as possible when calculating this loss, ideally all strokes in the global parameter catalog.

\subsection{Optimization Details}
\labsubsec{opt:details}

The optimization procedure is based on standard backpropagation and an AdaM optimzier, although others (most notably \citeauthor*{gatys} use L-BGFS-B).

As it has been explained, it is not possible to optimize brushstrokes for the whole image in parallel.
This is partly due to the neural renderer's memory requirements, which scales with the number of rendered patches.
The memory requirement of placing and blending, on the other hand, scales with the number of rendered brushstrokes as well as the number of surrounding brushstrokes and the patch window size.\\
Experiments have shown that a combination of 256 rendered brushstrokes with 128 surrounding brushstrokes paired with a patch window size of 320x320 pixels occupies around $9.5 \si{\gibi \byte}$ of memory which leaves enough space for more advanced losses and tweaks to the network architecture.
It is important to choose a large-enough patch window size, because with too-small window size, the rendered brushstrokes will not fully fit into the patch window and stay without a gradient.

The learning rate for the optimization procedure can be significantly larger than for training the neural renderer.
With a learning rate of $0.02$, the optimization procedure will converge significantly faster to a solution without any instability issues.

One choice, which has to be made individually per target image is how many brushstrokes will cover the image.
As larger images require more brushstrokes than smaller images, what should remain the same is the \textbf{brushstrokes density}.
The inverse brushstroke density will decide how many pixels on average should be covered by each brushstroke.
A number of $100 \text{pixels}/\text{brushstroke}$ has produced the best results during the experiments.

Another important choice that goes along the choice of how dense the brushstrokes should be distributed is that of how many optimization steps each brushstroke will be object to.
As for too few steps, the training will not have converged, and for too many, optimization will take an unnecessary amount of time.\\
This number can vary between images, since some images require more time to converge, about $500 - 1000 \text{steps}/\text{brushstroke}$ have given constantly good results.\\
Albeit there is no direct enforcement that each brushstroke is updated exactly this often, it can be expected that due to the uniform sampling of the patch window position, brushstrokes are updated roughly equally often.
Since 256 brushstrokes will be optimized in every step, the total number of optimization steps can be calculated together with the brushstroke density and the image's size.

Another detail is the fact that for each render patch, five consecutive optimization steps are performed.
Thereby memory bandwidth can be saved, since data is written and read from memory each time a different render patch is optimized.

\subsection{Results}
\labsubsec{results}
Results show that the optimization procedure is indeed capable of approximating a given image.
It takes about two hours to optimize on a single image with $\approx 1 \si{\mega\pixel}$
At that size renderings consists of $\approx 10,000$ brushstrokes.

\subsection{Style Transfer}
\labsubsec{opt:styletransfer}

One question that arises when discussing the optimization procedure is whether style transfer could be performed with this approach.
Especially, since the optimization procedure has explicitly been compared to the approach by \citeauthor*{gatys}~\cite{gatys}, it is natural to assume that such an approach could also be applicable in this case.\\
This mainly requires to introduce a style loss as it has been done by \citeauthor*{gatys}~\cite{gatys} since a content loss is already in place.\\
The problem, which arises when trying to apply style transfer with this particular approach is the partial update routine.
Since the gram-matrices are meant to catch global second-order statistics of the image, a small patch would be object to a wrongful assumption that the patch represents the whole image.
This would require to stitch the whole image with a few thousand brushstrokes which is very costly.

\subsection{Pitfalls of Feed-Forward Approaches}
In the development of this thesis, there were experiments targeting a feed-forward approach before ultimately deciding to use an optimization-based approach.
The problems which came up and the reasoning behind discarding this approach shall be shortly discussed, as it would otherwise exceed the scope of this thesis.

First and foremost, the computational burden of a feed-forward approach is very high.
Existing feed-forward drawing networks compromise image resolution to realize their implementation.\\
It was possible to implement a feed-forward that worked on small scale images. 
However, it was not possible to scale this approach well to larger image sizes (especially not for fully convolutional networks).
Since compromising image resolution was not a viable option in this thesis, another approach has to be found.

This goes hand in hand with another problem that occurred: the placement of brushstrokes on the canvas.\\
As artists are not bound to the same pixel grid as computers typically are, they can place brushstrokes freely on the canvas.
More so, they can pack brushstrokes densely in one area while distributing them broadly in another.
Classic CNNs are not suited for such behavior as they always build on a grid layout.
Experiments with either displaceable grid cells or stacked render windows further complicated the manner of scaling to large images sizes.

Thus, an optimization-based approach became favorable as it offers good approximations at high resolutions with manageable computational overhead.


