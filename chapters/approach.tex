\setchapterpreamble[u]{\margintoc}
\chapter{Approach}
\labch{Approach}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Motivation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Motivation}
\labsec{motivation}

%previous approaches have several problems
%-very low resolution only (learning2paint)
%-no brush strokes (style transfer)
%-bad reconstruction(SPIRAL)
%-hand-crafted/do not generalize (painterly rendering)
%-bad brush storkes (brushs troke extraction)
%-train whole network for a single image(drawingnet)
%-too inefficient (genetic algorithms)
%
%pick an orthogonal approach to drawing networks
%seems as a step back but could show whether it is possible at all
%emphasize more on the decoder quality than others did
%
%Goal is twofold: extract brush strokes is first
%see whether this stylizes normal images as well.


The basic approach of this work consists of two steps:
\begin{enumerate}
    \item A differentiable renderer which can generate images of brushstrokes from a parameter representation.
    \item An optimization procedure that iteratively approximates an image through brushstrokes representations.
\end{enumerate}

Having two separate steps can be motivated by comparing the optimization procedure to the actual process of painting an image.
An artist will most likely not pick single color particles and then place them on canvas.
Instead, an artist uses a brush or other utilities (see Pollock or others) to place more paint with a single action.

\begin{marginfigure}
    \includegraphics{oil_painting_tools}
    \caption[]{A typical set of brushes and spatulas used for oil paintings.
    %Source:Kuznetcov_Konstantin/Shutterstock}
    }
    \labfig{painting_tools}
\end{marginfigure}
Doing so -- of course -- limits the control over each drop of paint but maintains enough control to still create very delicate details in paintings.
This trade-off depends on the brush's size, such that an artist must choose the brush size depending on the content.

An example would be the painting of a uniformly colored sky.
Using a large brush size, the artist can cover a lot of canvas in relatively little time as well as keep the color well distributed over the canvas because the brush spreads the color more or less evenly within the brushstroke.
On the other hand, if one were to draw a sky with the smallest brush available, not only would it take forever to paint, it would also be hard to keep the paint evenly distributed over multiple strokes.

Now, translating this onto the given problem of recreating/approximating an image through brushstrokes, it would mean to limit the process to only use what we would describe as brushstrokes. \todo{reformualte this}

A comparable example is the game of Tangram.

\begin{marginfigure}
    \includegraphics{tangram}
    \caption[]{An Example of Tangram.}
    \labfig{tangram}
\end{marginfigure}

Tangram is a Chinese puzzle game that has the objective of replicating a given silhouette only with a set of 7 unique shapes.
The shapes may not overlay or be cut or anything.
Quite similarly, the objective of an optimizer is to replicate an image by only using brushstrokes.


Genetic algorithms already perform a similar task in order to approximate images by other geometric shapes or even smaller photos (also known as the popular photo mosaic effect).
\paragraph{Genetic algorithms} follow a random sampling approach that 'evolves' as genomes do.
Starting with a random set of circles that are parameterized by their position, radius, and color, it then chooses the most successful samples and resamples in a region around these.
This process is repeated until a certain level of convergence is reached.

\begin{marginfigure}
    \includegraphics{genetic_starry_night}
    \caption[]{Starry Night approximated by a genetic algorithm using only circles. \url{https://effyfan.com/2018/03/02/w6-van-gogh-flowfield/}}
    \labfig{genetic}
\end{marginfigure}

As well as this does work, it is very much computationally expensive as most samples will not fit the image, thus searching for the small set of fitting shapes requires to evaluate all the wrong shapes as well.
Considering artworks, brushstrokes have many more degrees of freedom, and artworks usually consist of upwards of a few thousand brushstrokes.
Consequently, it would be considerably more challenging to apply to this problem until computational resources have become a few magnitudes more powerful.

This premise can be overcome, though, by using a differentiable renderer.
A \textbf{differentiable renderer} is capable of creating/rendering shapes in the pixel domain.
In contrast to conventional renderers, it does so by solely using differentiable operations.
Thus, the previously described task becomes feasible, as random sampling can be replaced by gradient descent.
Ordinary renderers usually do not rely on differentiable operations, as faster operations with more straightforward logic render images well enough already.

Nonetheless, it is theoretically possible to create a differentiable renderer \cite{something}.

When talking about rendering brushstrokes, it would be even harder to think of a differentiable pipeline to draw brushstrokes of reasonable quality from a set of parameters.

Neural networks turn this problem around.
As neural networks are inherently differentiable, the question becomes rather how to make an existing neural pipeline render images from parameters.
Previous works have shown that neural networks are capable of conditionally generating high-resolution and high-quality images.
Conditioning the generator on brushstroke parameters as well as some noise should then output an image of the respective brushstroke with some variability to it.

The basic idea was proposed by \citeauthor*{japaneseneuralrenderer}~\cite{japaneseneuralrenderer} as it facilitates training of reinforcement learning based networks.

Inspired by this, the approach becomes more apparent.
First, a neural network is trained as a differentiable renderer.
Then the same renderer is used by a gradient descent-based optimization procedure to approximate an artwork as a set of renderer input parameters.

Both steps require some tricks to avoid pitfalls like computational limitations, which are outlined in the following two sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Neural Renderer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Neural Renderer}
\labsec{NeuralRend}

\todo{move this into the related work section and shorten it further}
The neural renderer in \citeauthor*{japanese neural renderer}'s~\cite{japanese neural renderer} work improves an architecture based on SPIRAL \cite{SPIRAL}.
\citeauthor*{Learning2Paint}'s efforts used a differentiable renderer to facilitate deep reinforcement learning for drawing images \cite{Learning2Paint}.
As mentioned, the renderer is required to be differentiable and ideally require as few resources as possible.

\subsection{Data Set}
\labsubsec{dataset}

Unfortunately, there is no data set available for this task, which means that a data set must be created explicitly for this approach.

There are several sources for virtual and real brushstrokes, to choose from.
These resources will be evaluated in the following part.
The main focus lies on three qualities for each data source:
\begin{enumerate}
    \item Suitable data format: As brushstrokes usually overlap in a painting, the data should already provide information about opacity or transparency, favorably in the common RGBA format, which has a fourth alpha channel to hold opacity information.
    \item Data set size and variability: Only with enough different data available, it will be possible to train a renderer reliably.
    \item Image quality. Brushstrokes should be as close to real-world brushstrokes as possible to ensure high-quality renderings later on.
\end{enumerate}

\subsubsection{Brushstroke Images}
\labsubsubsec{bsimages}

There are multiple sets of hand-drawn brushstrokes available online.
Most notably, there is a set of various well-classified colors and brush styles created by 'zolee' \todo{reference this} on the platform \url{onlygfx.com}.
It consists of approximately 1000 brushstrokes that mostly follow rather straight horizontal paths.
These brushstrokes are mostly grouped by color and painting technique (oil, acrylic, watercolor...).
All images are in the PNG format with the background made transparent in a post-editing step.

This data set has the advantage that it consists of real-world brushstrokes that were painted under presumably reproducible conditions.
On the other side, brushstrokes are of mostly the same width throughout the data set and also do not come with information which path the brush took or any other non-visual information.
Also, the data is very sparse.
Many color shades are not represented, which means that the generator would have to interpolate them or simply would not be capable of rendering any brushstrokes in this color.

It seems that this data set would be suitable to replicate single real-world brushstrokes as images.
However, limitations to the data make it unlikely that a generator could learn a coherent representation from this.

\subsubsection{Painting Libraries}
\labsubsubsec{libmypaint}

The mentioned work of SPIRAL \cite{SPIRAL} relies on synthetic data instead of real-world images.
It used the painting library 'libmypaint' \cite{libmypaint} to generate brushstrokes from parameters in real-time during training.

The apparent advantage of this and other painting libraries is the fact that one can fully control the output through parameters.
As the whole space of input parameters for the renderer can be covered, it is much easier to avoid pitfalls like they were described in \refsec{bsimages}.

Still, this data set falls short regarding the authenticity of rendered strokes.
Especially the inner area of the stroke shows a uniform color, which is far from what real brushstrokes would look like.

This data set is better suited for our task than the images from which were described in the previous seciton are.
However, the synthetic data will tend to make all rendering look a bit 'cartoonish' or flat, which could, in turn, limit convergence during the latter optimization process.

\subsubsection{Fluid Simulation}
\labsubsubsec{fluidpaint}

Fluid Paint is a project by David Li \cite{fluidpaint} that uses simple
fluid dynamics to give artificial brushstrokes a more plastic look.
It has been implemented in JavaScript and OpenGL.

There is a C++-version in the git-repository of SPIRAL along with
Python bindings created by Yaroslav Ganin.

Using these Python bindings, it possible to generate brushstrokes locally outside of a web browser environment.

The quality and controllability of FluidPaint fall right in the middle of the two previously mentioned data sets.
The generated brushstrokes look distinctively better than those generated with libmypaint, but still lack the quality of the real-world images.
Concerning controllability, FluidPaint allows controlling the path of the paintbrush's handle rather than the path of the brushstroke itself.
As a result, there occur some offsets to a given path as opposed to libmypaint.

As a reasonable compromise between controllability, variability, and render quality, FluidPaint emerges as a sensible choice.
Although real-time data generation is not possible with this library, data generation can be parallelized.
Such a set-up still allows for the creation of large data sets in a reasonable time frame.

Even though this data set still has some weaknesses, it is probably the best choice for training a differentiable renderer because of the noted reasons.

Other 'honorable mentions' are painting programs such as \todo{these two weird software stuff things}, which allow for even more authentic brushstrokes but lack any well-documented interface in order to generate a vast number of brushstrokes.

\begin{marginfigure}
    \subfloat[Photograph]{%
        \includegraphics[width=.8\textwidth]{images/real_red_stroke}%
        \labfig{strokes:real}%
    }\qquad
    \subfloat[ArtRage]{%
        \includegraphics[width=.8\textwidth]{images/artrage_red_stroke}%
        \labfig{strokes:artrage}%
    }\qquad
    %\subfloat[Affinity Photo]{%
    %    \includegraphics[width=.8\textwidth]{images/affinity_red_stroke}%
    %    \labfig{strokes:affinity}%
    %}\qquad
    \subfloat[libmypaint]{%
        \includegraphics[width=.8\textwidth]{images/affinity_red_stroke}%
        \labfig{strokes:libmypaint}%
    }\qquad
    \subfloat[FluidPaint]{%
        \includegraphics[width=.8\textwidth]{images/fluidpaint_red_stroke}%
        \labfig{strokes:fluidpaint}%
    }
    \caption[]{Comparison of similar brushstrokes in each data set}
    \labfig{strokes}
\end{marginfigure}

\todo{talk about RGBA advantages}


\subsubsection{Brushstroke Formalism}
\labsubsubsec{formalism}
With the means of data set production seized \todo{cut this joke}, what is left is to formulate the parameters that define the brushstrokes.
These parameters must quantify the following three properties of brushstrokes:
\begin{itemize}
    \item color
    \item thickness
    \item path
\end{itemize}

The easiest of these three properties is quantifying the color.
Naturally, computer vision relies on the RGB format, which defines color as a set of three 8-Bit integer values between 0 and 255. 
As for path and thickness, these two properties depend on the given coordinate system.
FluidPaint represents the canvas as a 2D plane in the $[0, 1]$ range.
Thus, it makes sense to follow the same representation.

Thickness can be any value in $[0, 1]$  for each brushstroke, where 0 is an infinitely small brushstroke, and 1 is a brushstroke as wide as the canvas.
As both these edge cases do not make sense in this application, the range is constrained to $[.03, .2]$, which includes only brushstrokes that are visible and also do not cover the whole canvas.

Quantifying the path now is a little more tricky.
The fluid dynamics simulation that FluidPaint uses relies on internal time steps at which the equations are evaluated and subsequently rendered.
At the same time, each step allows only a linear motion of the brush handle between positions $a$ and $b$.
Subsequently, any curved paths must be split into linear/straight segments that together should resemble a curved line.
As more steps mean longer simulation times and fewer steps mean edgy movement, a value of 20 time-steps per stroke emerged as a good compromise \todo{this is actually different as the steps depend on the length of the path. Look this up}.
The same number of steps has been used in SPIRAL's implementation.

Another problem becomes how to express a curved path in numbers.
The most straightforward representation would be a sequence of points that make up any curved path.
Such an approach allows for the highest versatility, but at the same time introduces a noticeable amount of parameters as each point consists of 2 coordinates totaling to 40 values.
These values are also not independent of one another but should follow a reasonable path as otherwise, the resulting brushstroke would look somewhat like a random walk than an actual brushstroke.
Since works such as SPIRAL or 'Learning to Paint' face a quite similar obstacle, their solution should be applicable in this case as well.
Both used so-called 'Bezier curves' which parametrize curved paths by a limited set of numbers.
\begin{marginfigure}
    \includegraphics{bezier_sample}
    \caption[]{Sample of a 3rd degree Bezier curve, using the De-Casteljau-algorithm,
    \url{https:\/\/de.wikipedia.org\/wiki\/Bézierkurve\#\/media\/Datei:Bezier-cast-3.svg}}
    \labfig{genetic}
\end{marginfigure}

Bezier curves parametrize curved paths not as linear segments, but as analytical paths which can be used in computer models.
\todo{bezier curves margin note}
They can be of different orders, which allows them to follow more complicated paths.
In this case, the simplest form -- the first order Bezier curve-- is already sufficient.
It defines a curve through its start- and endpoint, as well as a control point.
The curve is then defined as the path of a point over the time interval $[0, T]$.
First, one connects the start- and endpoints with the control point to get two lines in return.
On these lines, lie two points that move linearly along their lines within a virtual time interval $[0, T]$.
Then these two points are connected in the same way as before by a third line.
Again, this line has a point moving along its path throughout $[0, T]$.
As the first two points that define the third line will move, the line's orientation will change as well, thus translating the linear movement of the point into a complex curved path.
The path that this point then takes in $[0, T]$ defines the Bezier curve.
A first-order Bezier curve will only bend into one direction or follow a straight path.
For higher orders, the displayed process can be applied iteratively and allows for more complex curves.
As brushstrokes usually follow a quite simple path and fewer parameters are preferred, Bezier curves of first-order are suitable as parametrization.

Ultimately, this gives ten values that are sufficient to parametrize brushstrokes with certain constraints:
\begin{itemize}
    \item Three 2D coordinates that define the Bezier curves (6 values).
    \item One thickness parameter.
    \item Three values in RGB space.
\end{itemize}


\subsubsection{Data Constraints}
\labsubsubsec{constraints}

Given the parameters listed in section \ref{sssec:formalism}, the data still needs further constraints to facilitate the generator's training even further.

Section \ref{sssec:fluidpaint} already hinted at the impracticality of online data generation.
A rough estimation by timing the rendering of 100.000 FluidPaint brushstrokes reveals that a dedicated CPU server is capable of generating 300 strokes per second.
\todo{experiment in appendix}
A neural network with batch size 32 is limited to $\approx 10$ iterations per second under these circumstances, which would mean a clear bottleneck.
Thus, it seems advisable to generate data beforehand with enough samples to cover the data space sufficiently.
It will allow for much faster access to data, as individual data samples are relatively small and can be stored in a binary data file such as HDF5.

Besides this constraint to the amount of data available, another set of constraints will be introduced to reduce the data space to 'valid' brushstrokes only.
'Valid' brushstrokes will be defined as brushstrokes that resemble real-world brushstrokes to a certain degree.
This primarily concerns two relations within a brushstroke:
\begin{itemize}
    \item Its width-to-length ratio.
    \item Its curvature.
\end{itemize}

The width-to-length ratio will be restricted to brushstrokes that are at least two times as long as they are wide.
\begin{equation}
\norm{\vec{s} - \vec{e}} \overset{!}{\leq}  2 \times (\text{brush size}) \labeq{bs}
\end{equation}
Due to the simulation background of FluidPaint shorter brushstrokes will show some artifacts due to the bristles' length in the simulation which depends on the width of the stroke.
Another reason for this is the intended use-case, which will focus on van Gogh paintings.
As van Gogh did not practice pointillism, most of his strokes have a length to them, which brings such a constraint in line with some characteristics of van Gogh's style.

The same argumentation applies to the curvature:
Most brushstrokes (especially those by van Gogh) have a certain 'flow' or 'smoothness' to them, which can be described by using strokes with large curvature radii and without any corners in the strokes' path.
Thus, the data set will also be restricted to strokes that follow these descriptions.
In order to achieve this with random sampling in mind, a multivariate Gaussian distribution is placed between start ($\vec{s}$) and end point ($\vec{e}$).
The two axes are rotated such that the short axis is in line with the vector
$\vec{a} = \vec{s} - \vec{e}$ while the other sits orthogonal.
Then both axes are scaled with $\norm{a}_2$ and also the handpicked values $\frac{1}{200}$ and $\frac{1}{25}$ for along $a$ and orthogonal to it, respectively.
Figure \ref{fig:datageneration} shows samples from this distribution for an exemplary brushstroke.
\begin{marginfigure}
    \resizebox{\textwidth}{!}{
        \input{images/scatter.pgf}
    }
    \caption[]{Exemplary scatter plot for given start and end point to visualize the covariance matrix}
    \labfig{scatter}
\end{marginfigure}
This distribution is intended to follow that of brushstrokes as they would appear in the real world.
The majority of brushstrokes will be straight or just slightly bent due to the maximum of the PDF being at the center of $s$ and $e$.
Bent brushstrokes will mostly be symmetric as the long axis of the multivariate Gaussian is orthogonal to $a$.
Still, there will be strokes that have their bent towards either end of the brushstroke as well as some strokes with a high curvature.
The area of interest, though, will be densely populated as intended.
\begin{align}
    p(\vec{c}| \vec{s}, \vec{e}) & = \mathcal{N}(\mu, \Sigma) \labeq{checkpoint} \\
    \mu & = \frac{\vec{s} + \vec{e}}{2} \\
    \Sigma & =
        \begin{pmatrix}
            a_x & 0 \\
            0 & a_y
        \end{pmatrix} \labeq{Sigma}\\
    \vec{a} & = \vec{s} - \vec{e}
\end{align}

The color of the brushstrokes is not constrained as the color distribution of the target data set is not known at this point. \todo{why not van Gogh color distribution?}


%is this number of parameters enough, too much?

\subsubsection{Data Set Creation}
\labsubsubsec{creation}

The data set will be created with 100.000 samples that follow the constraints that were presented in section \ref{sssec:constraints}.
As an underlying distribution, the uniform distribution is chosen as it allows a more evenly coverage of the data space.

First, a set of start and end points, as well as brush size, is drawn and checked against \eqref{eq:bs}.
If the constraint is not met, the set will be redrawn entirely.
In case the constraint is satisfied, a checkpoint is sampled according to \eqref{eq:checkpoint}.
If $\vec{c}$ lies outside the render window, the checkpoint will be resampled.
At last, an RGB set is sampled from a uniform distribution as a color.

The resulting tuple of start, end and control point, brush size, and RGB color is then added to the data set.
Before rendering starts, the values of $\vec{s}$, $\vec{e}$ and $\vec{c}$ are scaled with the handpicked factor of $0.7$ to ensure the brushstrokes are rendered completely within the window and not cut by an edge of the render window.
At last, the brushstrokes are rendered according to the data set and added as well.

The render canvas size was chosen to be 64x64 pixels for several reasons:
First, even with such a small canvas size, training for the renderer takes about one day.
Secondly, the larger the render canvas size becomes, the deeper the renderer needs to be, which results in more computational overhead in the optimization routine as well as more layer through which the gradient has to be propagated.
Lastly, as there will be upwards of a thousand brushstrokes in a single image, increasing the canvas size to 128x128 would require four times as much memory per rendered image. As 1000 brushstrokes would already account for $1000 x 64 x 64 x 4 \si{\byte} \approx 16.4 \si{\gibi \byte}$ a fourfold increase would be significant.


As the last step, the data set is renormalized to the range $[-1, 1]$ for convenience and to facilitate training as well.
\todo{explain why zero-centered data is good}


\subsection{Architecture}
\labsubsec{arch}
The architecture of the brushstroke generator follows that of an inverse VGG network.
It is widely used and has shown in previous works that it should be capable of handling this task.
\todo{check the details} The architecture consists of three dense layers at the beginning, followed by a two-times upsampling layer as well as three convolutional layers.
The same pipeline with a two-times upsampling layer, and three convolutional layers is repeated until the target size is reached.
After the last convolutional layer, a hyperbolic tangent function is applied to restrict the output to the $[-1, 1]$ range.
As part of the hyper-parameter search, different tweaks to the architecture have been tested:
\begin{itemize}
    \item An additional noise input at every layer with a size equal to that of the existing signal.
    \item Additional information about the position in the pixel grid in every layer, so-called CoordConv \cite{coordconv}.
    \item Various combinations of activation and normalization functions.
\end{itemize}

\todo{reinsert these figures}
%\begin{figure*}
%    \resizebox{1.5\textwidth}{!}{
%        \input{images/generator.tex}
%    }
%    \resizebox{1.5\textwidth}{!}{
%        \input{images/discriminator.tex}
%    }
%        \caption{Visualization of the generator and discriminator architectures}\labfig{genarch}
%\end{figure*}

The discriminator is designed after the same principles and resembles a VGG encoder network.
First, three convolutional layers are applied, followed by a downsampling /pooling layer.
This structure is repeated until a target resolution of 4x4 pixels is reached.
Then a set of three dense layers is applied to give one final prediction per sample.

\subsection{Training}
\labsubsec{train}
During training, the L2 distance and the FID score serve as evaluation metrics.
The FID score becomes necessary as the visual comparison of the generated samples has proven difficult between different experimental runs.
The L2 distance does not qualify as a sufficient metric for later training stages as the stochastic nature of the brushstrokes puts a lowerbound on the L2-distance.

A two-time-step update rule was implemented to stabilize training further.
\todo{write this down and look at the tricks that were used}

\subsection{Results}
\labsubsec{results}
\todo{pick some results and present whether they are any good}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Stroke Approximation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stroke Approximation}
\labsec{strokeapprox}

\subsection{Data Set}
\labsubsec{dataset}
The data set will be presented in this section due to the data defining demands for later networks.
Also, the data set for the optimization task should meet a few requirements.

In order to focus on the brushstrokes in an image, the data set should consist of relatively high-resolution images.
At the same time, most brushstrokes should fit into a 64x64 window.
Any larger strokes can not be approximated with a single rendering window and thus would falsely be approximated by multiple brushstrokes.

Thus, more information per image is required, than merely the resolution.
\todo{reference li and lamberti aaas they did the same}
The scale of the image, therefore, becomes inquired information in this task.
Ideally, each image should be accompanied by its measurement as this allows for rescaling the image accordingly, given that one knows how large a typical brushstroke is.

At last, the painting's technique must be oil on canvas or similar techniques as this is what the renderer has been trained for.

All these requirements on the data are met by data directly available from the Van Gogh Museum in Amsterdam.
Each high-resolution image is categorized by its technique as well as the period in which it was painted.
This is accompanied by information on the dimensions of the image.

\subsection{Optimization Algorithm}
The next stage of the approach takes the pre-trained neural renderer and uses it to approximate brushstrokes in images.
In theory, the neural renderer should allow gathering meaningful gradients in parameter space even though the losses are calculated in image space, as explained in section
\ref{sec:motivation}.
There, it was outlined already that the method of choice will be an optimization procedure that relies on the differentiability of the neural renderer.

Nevertheless, this is not the only possible approach to dissect images into sets of parameters.
As explained in \ref{sec:PR}, there exist a variety of approaches for this kind of task.
Some of these promise a fast inference of parameters from images, which makes the optimization approach in this work seem like a step back at first.
The following section is dedicated to justifying the chosen approach by comparing the existing state-of-the-art approaches.

The targeted task is to approximate the representation of an image with roughly 1,000,000 pixels = 1MP by means of $\approx 10.000$ brushstrokes (see section \ref{ssec:motivation}).

\subsubsection{Genetic Algorithms}
\labsubsubsec{genalg}
\todo{put much of this into the related work section}
Genetic algorithms are possibly the most straight-forward approach.
As it was laid out in sections \ref{sec:motivation} and \ref{sec:genetic}, genetic algorithms use random sampling and previous best results to approximate fitting solutions to the rendering problem, which was described in section \ref{sec:renderproblem}.

Current state-of-the-art solutions are capable of finding a solution in $\approx 1h$ when searching for an approximation of a 1MP image with simple geometric shapes like circles or triangles.
This corresponds to roughly $1 \times 10^{9}$ sampling steps.
The time frame and the number of sampling steps depend on target shape density, target accuracy, sampling rate per shape, and degrees of freedom (with the latter requiring more sampling).

Looking back at section \ref{sec:renderer}, each brushstroke has 10 degrees of freedom with some inter-dependencies between them.
Also it is known that the obtained neural renderer is capable of generating $\approx 300 \frac{\text{images}}{s}$.
This means that rendering $10^{9}$ sampled brushstrokes will take:
$$
\frac{10^{9} \text{samples}}{300 \frac{\text{images}}{s}} \approx 3.33 \times 10^{6} s
\approx 926 h \approx 38.6 d
$$
Clearly, this is an absurd amount of time to spend \textbf{per image}.
Furthermore, the sampling magnitude has not even been corrected for by the additional degrees of freedom that brushstrokes provide.

Ultimately, this means that genetic algorithms are not an option for this task, as they are too inefficient.

\subsubsection{Brushstroke Extraction}
\labsubsubsec{bse}
\todo{put much of this into the related work section}
Next in the line are algorithm-based approaches that extract brushstrokes from an image using standard computer vision techniques.

Besides texton based image characterization \cite{textons} and pure filter based approaches \cite{filters} there have been approaches to extract brushstrokes or some of their characteristics \cite{brushstrokecharacteristics} \cite{brushstrokeextraction}.

The latter would pose a valid option for the task at hand.
Unfortunately, the best existing techniques fail to detect brushstrokes reliably over the whole image but only identify the most significant ones (see figure \ref{fig brushstrokeextracted}) /todo{add example image}.
Other approaches are only able to extract a few characteristics, like the orientation of a brushstroke, which proves insufficient as well.
\todo{Other approaches will usually not get parameters but again pixels, which they analyze.}
\todo{show some of the extracted brushstrokes}

\subsubsection{Stroke Based Rendering}
\labsubsubsec{sbr}
Stroke Based Rendering or Painterly Rendering also uses algorithms to approximate an image through brushstrokes with the original goal to achieve some stylization along the way.
While early works relied on interactive approaches, later publications were then able to automate this process entirely.
Judging from tests by the authors, such an approach would take between \todo{this number} and \todo{this number} hours per image.
As figures \ref{fig:sbr1} and \ref{fig:sbr2} show, these approaches obtain similar results to genetic algorithms and tend to draw an image from a coarse scale to a more delicate scale over time instead of a locally coherent manner.

These approaches were not intended to be used to obtain brushstrokes from a painting but focus on stylizing images.
This renders such an approach unfit for the goal of this thesis as well.

\subsubsection{Drawing Networks}
\labsubsubsec{drawingnetworks}
\todo{put much of this into the related work section}
Lastly, drawing networks are the newest iteration of approaches in this field.
Beginning with \todo{which one was first?}, there have been approaches that make use of feed-forward or recurrent neural networks combined with either supervised training or deep reinforcement learning.
The best results of these approaches are shown in figure \ref{fig:drawingnets}.

Noticeably, all of these images have a maximum resolution of 256 pixels along their longest edge.
There have not been any approaches yet, which can go significantly beyond this limitation.
Notably, the high computational costs of training recurrent neural networks seem to be an obstacle when shooting for higher resolutions.
Such resolutions can not be deemed sufficient when looking at individual brushstrokes, as outlined in \ref{ssec:dataset}.

Consequently, drawing networks must also be ruled out.

\subsubsection{Combined Approach}
\labsubsubsec{combinedapproach}

Even though there have been plenty of previous approaches to the task of extracting brushstrokes from images or similar tasks like rendering images through brushstrokes, none of these quite meet all the requirements of this task.
Namely, high-resolution input images, brushstroke focussed rendering or retrieval, limited resources, and realistic depiction of brushstrokes.

Although brushstroke extraction is the topic of this chapter, stroke based rendering and drawing networks both seem capable of making brushstroke extraction more feasible with their inverse approach.
Drawing networks introduced differentiable renderers as a tool to facilitate training.
Stroke based renderers achieve a parametrization implicitly by focussing on replicating the entire image instead of extracting individual brushstrokes.

They can be combined into a unified approach that uses the differentiable renderer and the objective of recreating an entire image.
The resulting approach does not use re-sampling like genetic algorithms but relies on gradient descent to converge to a solution significantly faster.

The limited capabilities of such a renderer would also guarantee that any approximation is composed of only valid brushstrokes instead of single pixels as it would be the case with normal generative models.

All in all, this approach is probably the best suited for approximating a target image only through brushstrokes as closely as possible.


The decision of whether to use an optimization-based approach is also linked to another question: Whether to sequentially or parallelly place brushstrokes.

As an example, most drawing networks and stroke-based renderers rely on a sequential approach.
Intuitively, it makes sense to use a sequential approach as artists also place their brushstrokes sequentially on the canvas.
However, there is a significant difference in how existing computer vision approaches place strokes compared to artists.
As neural networks search for ways to reduce the loss function as much as possible and as fast as possible, images tend to be made up of large canvas-filling brushstrokes at early stages, which in turn reduce the L2 loss.

This is contrary to how an artist usually works.
Artists tend to fill the background based on content (\eg sea or sky is often painted first) and will often leave some blank canvas to paint foreground objects later on.
They also do not use giant brushes for this but normal-sized brushes with which they place many strokes.
Thus, sequential approaches differ significantly from real-world paint processes.

In contrast, parallel approaches predict the whole painting as one set of parameters.
They are not as widespread due to the computational pitfalls that come with predicting brushstrokes for a whole painting at once compared to predicting only a few at a time.
Also, parallel approaches still need to predict in which order brushstrokes should be placed on the canvas (this will be explained in section \ref{sssec:order}) such that brush strokes can overlap.
Nonetheless, the advantage to sequential approaches is the fact, that foreground an background strokes can influence one another.

An example would be a stroke in the foreground, changing its path and revealing the canvas beneath.
Then another stroke in the background should cover this up if the color matches.
For sequential approaches, the background stroke can not be changed afterward, and it is very complicated to formulate a loss that propagates this information.
As artists already plan their future brushstrokes when painting the background, it would require a drawing network to plan far ahead.
At the same time, an optimization-based approach will not need to do that.

Another argument for parallel optimization is the focus on actually visible brushstrokes.
As artists cover up previous strokes, again and again, it becomes a shot in the dark to guess what these first brush strokes might have looked like.
Subsequently, any such guess is ill-posed and only introduces noise to the problem.
A parallel approach does not care for how the background could have been drawn but only focus on what is visible in the given image.

\subsubsection{Pitfalls of Feed-Forward Approaches}
\labsubsubsec{ffapproaches}

In the development of this thesis, there were experiments targeting a feed-forward approach before ultimately deciding on an optimization-based approach that is now presented.
As it would exceed the scope of this thesis, the arising problems and cause for discarding this approach shall be discussed.

First, the computational burden of a feed-forward approach is very high.
Existing feed-forward drawing networks compromise image resolution to realize their implementation.
As it was possible to implement this for small scale data like the cMNIST data set, such an approach seems feasible at first. \todo{margin explain cmnist} \todo{add image that were drawn by ff network}
However, since compromising image resolution is not a viable option (see \refsec{dataset}, one must find ways around this problem.
Fully convolutional architectures are a popular solution, which allows training on small scale data and inference on large scale images.
Unfortunately, the problem of predicting many spatially brushstrokes with complex parameters has proven too difficult for fully convolutional approaches.

This goes hand in hand with another problem that occurred: the placement of brushstrokes on the canvas.
As artists are not bound to the same pixel grid as computers typically are, they can place brushstrokes freely on the canvas.
More so, they can pack brushstrokes densely in one area while distributing them broadly in another.
Classic CNNs are not able to allow for similar behavior as they always require a grid layout.
Experiments with either displaceable grid cells or stacked grids have proven did work on a small scale.
However, together with a fully convolutional architecture, the approach did not seem to scale to larger images.

Thus, an optimization-based approach became favorable as it offers good approximations at high resolutions with manageable computational overhead.

\subsection{Optimization Procedure}
\labsubsec{opt}
The previous section \ref{ssec:ffapproaches} already specified why an optimization-based approach is preferable over a feed-forward or recurrent approach.
This section aims to give more details about the optimization procedure.


\subsubsection{Rendering Layout}
\labsubsubsec{layout}
Fundamentally, the optimization procedure is inspired by stroke-based rendering procedures.
It could also be compared to the style transfer approach by \citeauthor*~{gatys} \cite{gatys}, with parameters optimized instead of pixels.
The difference to normal stroke-based renderers, though, is the limited size of a rendered brushstroke in this work's renderer.

This poses a significant challenge that might not be obvious at first.

Ideally, the optimization procedure should be able to place strokes freely on the canvas, as this allows for an unbiased approximation.
Furthermore, it would allow allocating many small strokes in areas where the artist placed many strokes and use fewer and wider strokes in other areas.
However, due to various limitations, which were explained in \ref{sec:NR}, the renderer is not able to render single brushstrokes in a 1MP frame. Similar approaches only perform on relatively small canvas sizes, likely due to this issue \todo{add ref}.

Dissecting the target image into many small patches and then running the optimization procedure on these individual patches is a workaround for this.
After the patches are rendered, they are then fused along their edges to give the full image.
A good comparison is a grid of renders where each grid cell is the center of many renderings at the same time \todo{add figure to this}.
The major issue of such an approach is the adjacent edges where the grid cells are joined.
Also, a grid structure will almost always differ from the inherent distribution of brushstrokes in an image;
grid edges will, more often than not, separate brushstrokes between two grid cells.
A simple solution to this problem is transitioning from a stacked grid structure to an overlapping grid. \todo{graphics!!!!!}

By overlapping the grid cell, obvious edges between render windows are hidden as every edge lies within the frame of a different render window.
Choosing a lattice vector size smaller than the render window's dimensions such a grid can easily be realized.

Still, this kind of initialization requires a very even distribution of brushstrokes, ideally with a brushstroke at the center of each cell.
As this is not the case, and stroke densities will vary locally, the grid layout is prone to erroneously enforce a grid-like layout of strokes where there is none.
This is due to the inability of the grid to account for local changes in density and the following propagation of error.
Starting with a single region of high-density strokes in the vicinity of one grid cell, this cell would ideally render a narrow stroke to achieve high accuracy.
Neighboring cells then have to shift their strokes towards the center of that grid cell to account for the free space that is not covered by the narrow brushstrokes.
This shift must then be accounted for by the next neighboring cells and so on, which will cause all strokes in a row or column to shift towards this one spot with a high stroke density.

Now, a painting usually has many such high-density regions, which possibly cancel the shift that is caused by other areas.
As a result, the renderings are likely to not shift at all, as shifting mostly cancels out over the whole image.
Subsequently, an area of high stroke density will not have enough strokes available in its local region and thus will be covered up by a single broad stroke as this minimizes the L2 loss.

The core of the problem is the previously imposed lattice structure that propagates local density shifts along its principal axes.

One possible solution is getting rid of the lattice structure and replacing it with a more random structure that also covers the image sufficiently.
This can be accomplished by using \textbf{super-pixels} \cite{superpixels}.
Superpixels were popular in the pre-neural network era of computer vision and were often used in image segmentation tasks (\cite{img segmentation with SP}).
Nevertheless, superpixels are also a popular starting point for brushstroke extraction algorithms \cite{brushs stroke extraction}.

Basically, superpixels are pairwise disjoint groups of pixels in an image that would usually join pixels with similar colors in a local region.
Straight away, it is obvious why this is interesting for brushstroke extraction.
The distribution of superpixels will not follow a grid-layout as the previous approach, and the location of superpixels should relate to the given color distribution in the image.
It is easy to imagine that the location of the superpixel centers would be a good prior for locations of render windows as well.
Also, as the colors of pixels inside a superpixel should be similar, one can use the mean color of a superpixel as an initialization for the color of the brushstroke.

Ultimately, a superpixel segmentation will be used to infer positions for render windows as well as the color initialization of each stroke.

\subsubsection{Rendering Order}
\labsubsubsec{order}
Another problem that will come up during the optimization procedure is the order in which strokes are rendered (already mentioned in \refsec{opt}).
Real-world brushstrokes are also subject to the same issue as the current brushstroke will always be placed on top of brushstrokes painted earlier, (see section \ref{sssec:combinedapproach}).

As the optimization-based approach relies on parallel optimization of brushstrokes, it must decide which strokes are in the foreground and which are in the background.
Otherwise, as this would randomly change, edges in the image might be obstructed, and optimization could oscillate between solutions where different strokes lie in the foreground.
It could also prevent brushstrokes from overlapping such that they cover disjoint areas.
All of these outcomes would be unfavorable as it tends to produce worse results in the end.

The solution which is presented in this thesis is an additional parameter that describes a brushstroke's accuracy.
The accuracy is defined as the L2 distance of each stroke's pixel to the corresponding pixel in the target image multiplied by this each pixel's alpha value.
This removes any pixels which are of no interest from the loss and focusses only on the rendered pixels.
\begin{align}
    \text{accuracy} = 1 - \frac{1}{N} \sum_{p \in \text{pixels}} \norm{p - p'}_2^2 \text{ with } \vec{p} = \vec{p'} 
\end{align}
    \todo{improve this, especially the p=p' part}

The resulting value describes how well the pixels of the rendered stroke match their respective pixels in the target image.
Consequently, any brushstroke with higher accuracy will be more faithful to the target image than strokes with lower accuracy.
Placing these brushstrokes in the foreground should thus result in a smaller L2 loss than the other way around.
Vice versa, brushstrokes that connect two same colored areas will aggregate a lower accuracy as the brushstroke is compared at the intersection as well.
A rendered brushstroke that fits the foreground brushstroke will not be affected by this, thus keeping its high accuracy and laying on top of the other brush stroke.

Notably, the accuracy should not be included in the brushstroke's loss, as this would prohibit background strokes from covering larger areas and result in behavior that is similar to the non-overlapping issue previously described.
Thus, the accuracy of each stroke will be calculated as it is rendered.


\subsubsection{Initilization}
\labsubsubsec{init}

The following section will focus on initialization details for all parameters of a brushstroke, their position, and the confidence value.

Besides the original ten parameters of each brushstroke, which were explained in section \ref{sssec:formalism}, the previous two sections introduced an accuracy parameter for ordering and two translation parameters that define the position of the render window along each axis.
All of these parameters must be initialized before the optimization procedure starts.
Ideally, the initialization should not introduce any bias to the optimization process.
At the same time, an initialization should facilitate training and accelerate convergence in the early stages of optimization.

Unfortunately, the placement of the render window will surely enforce a bias on the optimization (see section \ref{sssec:layout}).
Thus, a superpixel initialization was motivated for the translation parameters as well as the color of the brushstrokes.
Subsequently, the translation parameter for each render window will be equal to the position of the weighted mass center of its respective superpixel.

The initial color will be taken from the mean color value of the superpixel.

The brush size will be initialized with the minimum possible value.
This will let brushstrokes not overlap at the beginning of optimization.
Only when the brushstrokes already roughly fit their local region, they shall intersect and be ordered by their accuracy.
Therefore, the initial accuracy will be 0 everywhere, as the accuracy is recalculated after every optimization step.

Other patch parameters, notably $\vec{s}$, $\vec{e}$ and $\vec{c}$, will be initialised using a narrow Gaussian distribution with $\sigma = .1$ and values clipped to $[-1, 1]$ because there is no prior information available on how the brushstrokes are oriented.
Instead, this approach relies on the optimization procedure to be minimally biased by this initialization of the path variables.

\subsubsection{Partial Updates}
\labsubsubsec{partupdate}

When building an optimizer based on the information provided until now, GPU memory limitations become a guaranteed issue.
Even as it might not be evident at first, the optimization procedure imposes a considerable requirement for memory on the graphics card, due to two parts of the training:

First, the number of brushstrokes can easily become very high with large images as input.
With a render window size of 64x64, the brushstrokes are relatively small, and paintings can consist of a few thousand brushstrokes.
This would equate to a batch size of a few thousand for the brushstroke renderer.
Tests have shown that on the latest hardware with 12GB memory, the maximum number of brushstrokes rendered in parallel is $\approx 256$.
Obviously, this is one to two orders of magnitude smaller than what would be needed to optimize all strokes in the painting in parallel.
Still, there is a way around this bottleneck, by optimizing the image not as a whole but as smaller patches consisting of 256 brushstrokes at a time, giving a partial update routine.

Each patch comprises the 256 nearest render windows to a randomly sampled location on the canvas.
These 256 brushstrokes are then rendered from their parameters in order to obtain a gradient later on.
Then strokes are ordered according to their accuracy, placed on canvas ('padded') and blended ('stitched').
At last, the loss is calculated and back-propagated to update the parameters of the patch.

When restricting the number of brush strokes, bordercases do become an issue.
Brushstrokes that lie at the perimeter of the patch are not fully surrounded by other patches.
The result is a discrepancy of what the gradient to the brushstroke would look like if the brushstroke had laid in the middle of the patch.
A solution to this is laying a ring of already rendered brushstrokes around the patch.
The ring guarantees that all brush strokes that brushstrokes at the border of the patch are surrounded by neighboring brushstrokes the same way, as if they were in the middle of the patch.
Notably, the brushstrokes that belong to the ring around the patch, serve as 'dummy' brushstrokes and are optimized.
Without any need for optimization, these brushstrokes do not need to be rendered in this optimization step.
Instead, they could have been rendered earlier and the rendered image is just reused.
For this reason rendered brushstrokes for all parameters are saved in a \textbf{render image catalog}.
Equally, the collection of brushstrokes that shall later compose the image shall be called a \textbf{parameter catalog}.
The parameters as well as the images in these catalogs are updated whenever the respective brushstroke parameter has been updated.
With the render image catalog at hand, it is possible to use the previously rendered brushstrokes to stitch the image as a whole with the newly rendered brushstrokes of the image patch embedded.

As this will cause the border strokes to be surrounded by other brushstrokes to all sides (even if not all of them are freshly rendered), the effect of the partial update routine vanishes.

The other problem for huge input images is that the stitching of brushstrokes takes up a considerable amount of memory.
In detail, each brushstroke must be placed on the virtual canvas individually,  where the canvas' size is that of the input image.
This would equate to a couple of thousand 1MP images being stored in memory before they are stitched to a single 1MP images.
As a single 1MP image carries roughly $1000 \times 1000 \times 4 \text{channels} \times 8 Bit = 32 \times 10^{6} Bit = 4 \times 10^{6} Byte = 4 MB$ of information, a few thousand of these will easily exceed the memory of most graphics cards.
\marginnote{Images atypically saved as unsigned 8-Bit integers, thus 8-Bit per channel.}

Luckily, the previous workaround for optimizing only 256 brushstrokes will work as well without rendering the full image at every step.
Since most brushstrokes are not re-rendered and thus will not be supplied with a gradient, their main task is to regulate losses for edge strokes of the image patch.
As this does not need far away strokes, but only those close to the optimized strokes, a ring of pre-rendered strokes around the re-rendered patch will suffice.

\todo{approximate the number of brushstrokes needed to surround the patch}
This reduces the number of involved brushstrokes per optimization step from a few thousand down to a couple hundred.
Besides allowing for the partial update routine to be performed at all, it should also increase performance significantly compared to an approach that involves all brushstrokes.

\subsection[Placing \& Blending]{Image Placing \& Blending}
\labsubsec{placing_blending}
As the update and optimization procedure has now been explained thoroughly, it is now time to explain the process of placing and blending a rendered brushstroke a bit further.

After each stroke has been rendered, it needs to be placed according to the translation parameters (see \ref{sssec:layout}).
This requires dynamically placing each brushstroke inside a zero-filled tensor.

By calculating each pixel's global positioning in the rendered image individually, it is possible to scatter the pixels of the original rendered image into the larger zero-filled tensor and obtain a globally placed brushstroke.

The task of blending the resulting canvas-sized rendered brushstrokes together, or stitching them, is more complicated, though.
Due to the canvas' alpha channel, it is possible to blend only relevant information while the rest of the image will be ignored.

As far as conventional alpha blending goes, two images are blended by multiplying each pixel value with the alpha value of the top-layer image while the background image is multiplied with the complement to the alpha value:
\begin{align}
    p_{x,y} = p^{\text{top}}_{x, y} \times \alpha^{top}_{x, y}
    + p^{\text{bottom}}_{x, y} \times (1 - \alpha^{top}_{x, y})
    \forall (x, y) \in \mathcal{D}(\text{image})
\end{align}
\todo{correct this equation and make it nicer}

For multiple layers, this process can be repeated in various fashions, after the strokes are ordered according to their accuracy.
Either one could start from the bottom and blend the two back-most strokes, followed by the next third last strokes and so one, or one could start this process from the front with the two strokes in the very front being blended at first, then the stroke with the third-highest accuracy \etc.

Both of the approaches would yield the same result but differ only in the order in which they were blended.
Subsequently, both methods will have $(n-1)$ blending operations to compute per pixel.

Blending brushstrokes in a position-aware manner can reduce this number.
The majority of pixels for each padded brushstroke is non-informative, as the alpha value is zero (see \reffig{stroke_on_canvas}).
This opens the possibility of go from blending \textit{all} pixels of \textit{all} brush strokes to blending just those pixels with non-zero alpha values.
As before there were many layers that represented one single brush stroke each, few layers that do not represent brush strokes can achieve the same result.
Instead of representing brush strokes, these few layers represent the order in which pixels should be blended.
This means going from a layer-focussed approach to a rather pixel-focussed approach to alpha-blending.

The easiest way to accomplish this, is to first find the maximum blending-depth over all pixels, where the blending-depth $k$ is the number of layers where the alpha value is not zero.
\begin{align}
    k = \argmax_{p \in \text{pixels}} \sum_{i \in \# \text{layers}} \mathds{1}(\alpha_i > 0)
\end{align}

Then the top $k$ layer indices for each pixel are picked, which reduces the number of blending operations from $(n-1)$ to $(k-1)$.
Importantly, the top $k$ indices should not be ordered by their alpha values but remain in the order that was imposed by sorting according to the accuracy value.
Otherwise, the order will most certainly be mixed up, and the pixel with the highest alpha value will always lie on top instead of the pixel that belongs to the most accurate brushstroke.
Especially, as brushstroke renderings fade out towards their edges, this makes a significant difference.


Another way of accelerating the process of alpha-blending is \textbf{vectorizing}.
Instead of iteratively applying the computations, a vectorized operation can perform these computations in parallel.
Vectorizing makes it necessary to construct a tensor with the following properties:

For $\tensor{I} \in [0, 1]^{H \times W \times 4} $ the image target, the shape will be defined
as $\mathcal{S}(\tensor{I}) = (H, W, 4)$.
Each alpha channel will have the values $\alpha^{hw} \in [0, 1]$ for $h = 0, ..., H$ and $w = 0, ..., W$.

The set of rendered and padded brushstrokes $\tensor{J}$ will have the shape $(N, H, W, 4)$ with $N$ depicting the number of brushstrokes that ought to be stitched simultaneously.

Now, looking at each individual pixel in $\tensor{J}$, which is described by $(z^{hw}_n, \alpha^{hw}_n)$ for $n = 1, ..., N$ and $z^{hw}_n \in [0, 1]^{3}$, $z^{hw}$ describes the RGB values and $\alpha^{hw}$ the alpha-channel for a pixel at $(h, w)$.

A blending operation can then be defined by
\begin{align}
    z'^{hw} & = \tilde{\alpha}^{hw} \cdot z^{hw} \\
    \text{or} \\
    z'^{hw} & = \sum_{n=1}^N \tilde{\alpha}^{hw}_n  z^{hw}_n \\
\end{align}

with $z'^{hw}$ the resulting RGB values of the blended pixel and $\tilde{\alpha}^{hw}$ a vector that holds the merged alpha values for each pixel:
\begin{align}
    \tilde{\alpha}^{hw} & =
    \begin{pmatrix}
        \alpha^{hw}_1 & &\\
        \alpha^{hw}_2 & (1 - \alpha^{hw}_1) &\\
        \alpha^{hw}_3 & (1 - \alpha^{hw}_2) & (1 - \alpha^{hw}_1)\\
        \vdots & &\\
    \end{pmatrix}
    \\
    & = \alpha^{hw} \odot 
    \begin{pmatrix}
        1  &\\
        (1 - \alpha^{hw}_1) &\\
        (1 - \alpha^{hw}_2) & (1 - \alpha^{hw}_1)\\
        \vdots &\\
    \end{pmatrix}
    \\
    \rightarrow  \tilde{\alpha}^{hw}_n & = \alpha^{hw}_n \prod^{n-1}_{i=1} (1 - \alpha^{hw}_i)
\end{align}

with $\odot$ the element-wise product

What is left, is to find a way to construct $\tilde{\alpha}^{hw}$ from $\alpha^{hw}$.

For this an auxiliary matrix $\beta^{hw}$ is constructed:
\begin{align}
    \beta^{hw} = \alpha^{hw} \times \mathbb{1}_{1 \times N} = 
    \begin{pmatrix}
        \alpha^{hw}_1 & \alpha^{hw}_2 & \hdots & \alpha^{hw}_n\\
        \alpha^{hw}_1 & \alpha^{hw}_2 & \hdots & \alpha^{hw}_n\\
        \vdots & \vdots & \ddots & \vdots \\
        \alpha^{hw}_1 & \alpha^{hw}_2 & \hdots & \alpha^{hw}_n\\
    \end{pmatrix}
\end{align}
with
$$
\mathbb{1}_{1 \times N} = \begin{pmatrix}
        1 \\
        1 \\
        \vdots\\
        1 \\
    \end{pmatrix}^T
$$

Then $\beta^{hw}$ is strictly triangulated such that:
\begin{align}
    \gamma^{hw} & = \beta^{hw} \odot
    \begin{pmatrix}
        0 & 0 & 0 &\hdots & 0\\
        1 & 0 & 0 &\hdots & 0\\
        1 & 1 & 0 &\hdots & 0\\
        \vdots & \vdots & \ddots & \ddots & \vdots \\
        1 & 1 & \hdots & 1 & 0\\
    \end{pmatrix}
    \\ & = 
    \begin{pmatrix}
        0 & 0 & 0 & \hdots & 0\\
        \alpha^{hw}_1 & 0 & 0 & \hdots & 0\\
        \alpha^{hw}_1 & \alpha^{hw}_2 & 0 & \hdots & 0\\
        \vdots & \vdots & \ddots & \ddots & \vdots \\
        \alpha^{hw}_1 & \alpha^{hw}_2 & \hdots & \alpha^{hw}_{n-1} & 0\\
    \end{pmatrix}
    \\
    \rightarrow \delta^{hw} = 1 - \gamma^{hw} & = 
    \begin{pmatrix}
        1 & 1 & 1 & \hdots & 1\\
        (1 - \alpha^{hw}_1) & 1 & 1 & \hdots & 1\\
        (1 - \alpha^{hw}_1) & (1 - \alpha^{hw}_2) & 1 & \hdots & 1\\
        \vdots & \vdots & \ddots & \ddots & \vdots \\
        (1 - \alpha^{hw}_1) & (1 - \alpha^{hw}_2) & \hdots & (1 - \alpha^{hw}_{n-1}) & 1\\
    \end{pmatrix}
\end{align}

By multiplying the elements along each row in $\delta^{hw}$ one gets:

\begin{align}
    \epsilon^{hw}_i = \prod^N_{j=1} \delta^{hw}_{ij} & =
    \begin{pmatrix}
        1  &\\
        (1 - \alpha^{hw}_1) &\\
        \vdots &\\
        \prod^{N-1}_{j=1} (1 - \alpha^{hw}_j)
    \end{pmatrix}
    \\ \rightarrow \tilde{\alpha}^{hw} = \epsilon^{hw} \odot \alpha^{hw} & =
    \begin{pmatrix}
        \alpha^{hw}_1 & &\\
        \alpha^{hw}_2 & (1 - \alpha^{hw}_1) &\\
        \alpha^{hw}_3 & (1 - \alpha^{hw}_2) & (1 - \alpha^{hw}_1)\\
        & \vdots &\\
        \alpha^{hw}_N & \prod^{N-1}_{j=1} (1 - \alpha^{hw}_j)
    \end{pmatrix}
\end{align}

This vectorized version of alpha blending will introduce a new possible bottleneck as it is, since $\beta^{hw}$ will be a tensor of shape $(N, N, H, W)$, which will equate to
$$
256 \times 256 \times 256 \times 256 \times 4 \si{\byte} = 2^{36}\si{\byte} = 64\si{\gibi\byte}
$$
alone.

This is where the previous position-aware alpha blending tricks becomes useful.
By computing $\beta^{hw}$ only through the top $k$ values of $\alpha^{hw}$ instead of the full tensor $\alpha^{hw}$, the size will be reduced to
$$
k \times k \times 256 \times 256 \times 4 \si{\byte} = k^2 2^{22}\si{\byte} = k^2 \times 4\si{\mebi\byte}
$$
as the shape is reduced to $(k, k, H, W)$.

Ultimately, this accelerates optimization by a factor of $2-3$, as it will be shown in \ref{sec:exp:vectorization}.

It must be mentioned that the upper boundary for computational complexity in using this kind of alpha-blending is $\mathcal{O}(N \log k)$, since the top $k$ search is bound by this complexity.
\todo{calcualte this}

\subsubsection{Losses}
\labsubsubsec{losses}

In this section, the different kinds of losses for the optimization procedure will be discussed.

First off, the L2 loss or \textbf{mean squared error} is an obvious choice for this task.
\begin{align}
    L_{\text{MSE}} = \frac{1}{HW} \sum_{p \in \text{pixels}} \norm{z(p) - z'(p)}_2^2
\end{align}

Since the MSE loss focuses on minimizing the pixel-wise error between the target image and the fully rendered approximation by brushstrokes, it will cause the rendered image to match the target image mainly in color.
At the same time, MSE loss is prone to blurring, which results in washed out edges in the rendered image.
Thus, this loss is must be accompanied by additional losses to make up for the shortcoming of MSE loss.

A popular choice for preserving the content in an image, which is associated with preserving edges, is \textbf{perceptual loss}.
Perceptual loss is based on a VGG Network \cite{VGG} that is pre-trained on ImageNet \cite{ImageNet}.
To compute the loss, the activations of deep layers (usually the fourth convolutional block) of the pre-trained VGG network are inferred and then compared using MSE loss.
\begin{align}
    L_{\text{perceptual}} = \frac{1}{H_f, W_f} \sum_{p \in \text{pixels}} \norm{f(p) - f'(p)}_2^2
    \labeq{percep}
\end{align}
The resulting distance is meant to capture how well edges between the two input images are preserved which should be equal to whether the content in both images is the same.
Together with MSE loss, a perceptual loss is often used to get better reconstructions than with MSE loss alone, as edges of objects in the image are better preserved, prohibiting blurriness that would occur otherwise.

As an extension to perceptual loss, \citeauthor*{lpips} introduced perceptual similarity or \textbf{LPIPS loss}, which weighs the different layers of the pre-trained VGG-network differently in order to increase the effectiveness of perceptual distance between two images \cite{lpips}.
LPIPS loss is meant to preserve edges even better than perceptual loss does with a similar computational overhead.

Besides losses that operate in pixel space, it is also necessary to restrict the action space for each brushstroke.
As explained in section \ref{ssec:render:dataset}, the renderer has been trained on a limited data set, which puts constraints on how curved brushstrokes may be and how the ratio between length and width ought to look like.
These constraints must be enforced in the optimizing process as well.
Because the renderer has not been trained on data outside of the generated data set, the renderer will likely break if the input parameters lie too far outside the training space.
The results would then be renderings with no output, distorted brushstrokes, or just noisy output, as seen in \ref{ssec:ablation:renderer}.

Thus, one must think of an additional loss to confine the parameter space to the same space as the generated brushstroke data during optimization.
There are two ways of achieving this:
\begin{itemize}
    \item Discriminators
    \item Explicitly coded losses
\end{itemize}

\paragraph{Discriminators} are a popular choice in this context because even if the data distribution is not known beforehand, a discriminator is still able to learn the distribution from data and thus point out wrong parameter combinations in this case.
Still, a discriminator comes with a few compromises, as the target distribution will never be entirely learned rather than well approximated by it.
This leaves room for weaknesses as well as local minima in the discriminator's prediction, which would result in worse quality for this task.
Usually, these weaknesses are made up for during adversarial training as if the generator overfits to such weaknesses, and the discriminator will quickly penalize such a solution.
In the optimization routine, which is employed for this problem, it is not possible to train the discriminator online as the limited amount of brushstrokes will allow the discriminator to overfit the problem easily.
Thus only a pre-trained discriminator with its said weaknesses can be used in this case.

\paragraph{Handpicked losses} As the data distribution for the generated data set is actually known in this case (see \ref{sssec:creation}), it is also possible to manually define losses that confine the brushstrokes.
The width constraint -- as a first example -- can easily be enforced by penalizing whenever the brushstroke's width $w(x)$ is more than half the length $l(x)$ between the start point $\vec{s}(x)$ and the end point $\vec{e}(x)$ of the brushstroke $x$:
\begin{align}
    L_{\text{bs}} & = \frac{1}{\vert X \vert}\sum_{x \in X} \max(0, 2w(x) - l(x)) \\
    & = \frac{1}{\vert X \vert}\sum_{x \in X} \max(0, 2w(x) - \norm{\vec{s(x)} - \vec{e(x)}}_2)
\end{align}

This is a bit more complicated regarding the limitation that is introduced to the control point $\vec{c}(x)$.
As $\vec{c}(x)$ was sampled from a multivariate Gaussian with fixed parameters in the data set, it should now follow a similar distribution in relation to the direction $\vec{s}(x) - \vec{e}(x)$ of each stroke.

This can be achieved by first defining two orthonormal basis vectors which are either parallel $\vec{n}_{se}^\parallel(x)$ or orthogonal $\vec{n}_{se}^\bot(x)$ to the directional vector $\vec{s}(x) - \vec{e}(x)$:
\begin{align}
    \vec{n}_{se}^\parallel(x) & = \frac{\vec{s}(x) - \vec{e}(x)}{\norm{\vec{s}(x) - \vec{e}(x)}_2} \\
    \vec{n}_{se}^\bot(x) & = R_{\pi/2} \frac{\vec{s}(x) - \vec{e}(x)}{\norm{\vec{s}(x) - \vec{e}(x)}_2} \\
    \text{with } R_{\pi/2} & =
    \begin{pmatrix}
        \cos \pi/2 & -\sin \pi/2 \\
        \sin \pi/2 & \cos \pi/2
    \end{pmatrix}
\end{align}

Then $\vec{c}(x)$ can be projected into the coordinate system spanned by  $\vec{n}_{se}^\parallel(x)$ and $\vec{n}_{se}^\bot(x)$:
\begin{align}
    c^\parallel(x) & = (\vec{c}(x) - \vec{a}(x)) \cdot \vec{n}_{se}^\parallel(x)  \\
    c^\bot(x) & = (\vec{c}(x) - \vec{a}(x)) \cdot \vec{n}_{se}^\bot(x)  \\
    \vec{a}(x) & = \frac{\vec{s}(x) + \vec{e}(x)}{2}
\end{align}

Now, the axes of the original multivariate distribution co-align with $\vec{c}^\parallel(x)$ and $\vec{c}^\bot(x)$.
By calculating the mean and standard deviation along these projections, they can be compared to the parameters of the original data distribution.
\begin{align}
    \vec{\mu} & = \frac{1}{\vert X \vert}\sum_{x \in X} \begin{pmatrix} c^\parallel(x) \\ c^\bot(x) \end{pmatrix} \\
    \mat{\Sigma} & = (\frac{1}{\vert X \vert} \sum_{x \in X}
        (\begin{pmatrix} c^\parallel(x) \\ c^\bot(x) \end{pmatrix} - \vec{\mu})
        (\begin{pmatrix} c^\parallel(x) \\ c^\bot(x) \end{pmatrix} - \vec{\mu})^T
        )^{\frac{1}{2}} \\
\end{align}

Using the Kullback-Leibler divergence for multivariate Gaussian distributions, the compliance with the data sets distribution can be checked:

\begin{align}
    \L_{\text{KL}} = \frac{1}{2}\left[\log\frac{|\Sigma|}{|\tilde{\Sigma}|} - d + \text{tr} ( \Sigma^{-1}\tilde{\Sigma} ) + (\mu - \tilde{\mu})^T \Sigma^{-1}(\mu - \tilde{\mu})\right]
\end{align}

with 
\begin{align}
    \tilde{\mu} & = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \\
    \tilde{\Sigma} & = \begin{pmatrix} \frac{1}{200} & 0 \\ 0 & \frac{1}{25} \end{pmatrix} \text{ see \eqref{eq:Sigma}}
\end{align}

since the origin of the projection is at $\vec{a}(x) = \frac{\vec{s}(x) + \vec{e}(x)}{2}$ and coincides with the center of the data distribution.

In theory, $L_{\text{KL}}$ and $L_{\text{bs}}$ should be able to capture any deviation from the source data and ensure that parameters stay within the training space of the renderer.
One problem that obviously could arise with this formulation is for patches with very similar brushstrokes that show a mean other than $\tilde{\mu}$ as well as a very low values inside the covariance matrix and non-zero off-diagonal values.
Thus it will be favorable to include as many strokes as possible when calculating this loss, ideally all strokes in the global parameter catalog.

\subsection{Style Transfer}
\labsubsec{opt:styletransfer}

One question that arises when discussing the optimization procedure is whether style transfer could be performed with this approach.
Especially, since the optimization procedure has explicitly been compared to the approach by \citeauthor*{gatys}~\cite{gatys}, it is natural to assume that such an approach could also be applicable in this case.

This mainly requires to introduce a style loss as it has been done by \citeauthor*{gatys}~\cite{gatys} since a content loss is already in place (see~\eqref{eq:percep}).
A style loss can then be implemented similarly by aggregating the activations of more layers and then calculating the gram matrices:
\begin{align}
    \L_{\text{style}} & = \sum_{l=0}^L \frac{w_l}{4 N^2_l M^2_l} \sum_{ij} (G_{ij}^l - A_{ij}^l)^2
    G_{ij}^l & = \sum_k F_{ik}^l F_{jk}^l
\end{align}

The problem, which arises when trying to apply style transfer with this particular approach is the partial update routine.
Since the gram-matrices are meant to catch global second-order statistics of the image, a small patch would be object to a wrongful assumption that the patch represents the whole image.
The only chance of dealing with a local patch in a global context in image space is a cached version of the whole current image in which the patch is embedded.

\subsection{Optimization Details}
\labsubsec{opt:details}

As it has been explained in \refsec{opt:partialupdates}, it is not possible to optimize brushstrokes for the whole image in parallel.
This is partly due to the neural renderer's memory requirements, which scales with the number of rendered patches.
The memory requirements of placing and blending, on the other hand, scales with the number of rendered brushstrokes as well as the number of surrounding fixed brushstrokes and the patch window size.
Experiments have shown that a combination of 256 rendered brushstrokes with 128 surrounding brushstrokes paired with a patch window size of 320x320 pixels occupies around $9.5 \si{\gibi \byte}$ of memory which leaves enough space for more advanced losses and tweaks to the network architecture.

The learning rate for the optimization procedure can be significantly larger than for training the neural renderer.
With a learning rate of $0.01$, the optimization procedure will converge significantly faster to a solution without any instability issues.

One choice, which has to be made individually per target image is how many brushstrokes will cover the image.
As larger images obviously require more brushstrokes than smaller images, what should remain the same is the \textbf{brushstrokes density}.
The brushstroke density will decide how many pixels on average should be covered by each brushstroke and thus be used during initialization.
A number of $100 \text{pixels}/\text{brushstroke}$ has produced the best results during the experiments.

Another important choice that goes along the choice of how dense the brushstrokes should be distributed is that of how many optimization steps each brushstroke will be object to.
As for too few steps, the training will not have converged, and for too many, optimization will take an unnecessary amount of time.
As this can vary between images, since some images require more time to converge, about $1500 \text{steps}/\text{brushstroke}$ have been empirically chosen.
There is no direct enforcement that each brushstroke is updated exactly this often, but it can be expected that due to the uniform sampling of render patches, there will be no major deviation for some brushstrokes.
Since 256 brushstrokes will be optimized in every step, the total number of optimization steps can be calculated together with the brushstroke density and the image's size.

Another minor detail is the fact that for each render patch, five consecutive optimization steps are performed as this safes memory bandwidth since data must be written and read from memory each time a different render patch is optimized.

\subsection{Results}
\labsubsec{results}

Results can be seen in \reffig{opt:results:starry_night} for Starry Night as the standard reference image of this thesis.
It took approximately 15,000 optimization steps, which corresponds to about $2\si{\hour}$.
The rendering consists of roughly 10,000 brushstrokes.

\reffig{opt:results:photo} shows the result for a natural photo as target image which took
also $~2\si{\hour}$ to compute with 8.000 brushstrokes and 12.000 optimization steps.

