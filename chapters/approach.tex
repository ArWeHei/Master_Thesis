\setchapterpreamble[u]{\margintoc}
\chapter{Approach}
\labch{Approach}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Motivation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Motivation}
\labsec{motivation}

\todo{two stage approach be Ha and Schmidhuber 'world model'}

The basic approach of this work consists of two steps:
\begin{enumerate}
    \item A differentiable renderer which can generate images of brushstrokes from a parameter representation.
    \item An optimization procedure that iteratively approximates an image through brushstrokes representations.
\end{enumerate}

Having two separate steps can be motivated by comparing the optimization procedure to the actual process of painting an image.
An artist will most likely not pick single color particles and then place them on canvas.
Instead, an artist uses a brush or other utilities (see Pollock or others) to place more paint with a single action.

\begin{marginfigure}
    \includegraphics{oil_painting_tools}
    \caption[]{A typical set of brushes and spatulas used for oil paintings.
    %Source:Kuznetcov_Konstantin/Shutterstock}
    }
    \labfig{painting_tools}
\end{marginfigure}
Doing so -- of course -- limits the control over each drop of paint but maintains enough control to still create very delicate details in paintings.
This trade-off depends on the brush's size, such that an artist must choose the brush size depending on the content.

An example would be the painting of a uniformly colored sky.
Using a large brush size, the artist can cover a lot of canvas in relatively little time as well as keep the color well distributed over the canvas because the brush spreads the color more or less evenly within the brushstroke.
On the other hand, if one were to draw a sky with the smallest brush available, not only would it take forever to paint, it would also be hard to keep the paint evenly distributed over multiple strokes.

Now, translating this onto the given problem of recreating/approximating an image through brushstrokes, it would mean to limit the process to only use what we would describe as brushstrokes. \todo{reformualte this}

A comparable example is the game of Tangram.

\begin{marginfigure}
    \includegraphics{tangram}
    \caption[]{An Example of Tangram.}
    \labfig{tangram}
\end{marginfigure}

Tangram is a Chinese puzzle game that has the objective of replicating a given silhouette only with a set of 7 unique shapes.
The shapes may not overlay or be cut or anything.
Quite similarly, the objective of an optimizer is to replicate an image by only using brushstrokes.


Genetic algorithms already perform a similar task in order to approximate images by other geometric shapes or even smaller photos (also known as the popular photo mosaic effect).
\paragraph{Genetic algorithms} follow a random sampling approach that 'evolves' as genomes do.
Starting with a random set of circles that are parameterized by their position, radius, and color, it then chooses the most successful samples and resamples in a region around these.
This process is repeated until a certain level of convergence is reached.

\begin{marginfigure}
    \includegraphics{genetic_starry_night}
    \caption[]{Starry Night approximated by a genetic algorithm using only circles. \url{https://effyfan.com/2018/03/02/w6-van-gogh-flowfield/}}
    \labfig{genetic}
\end{marginfigure}

As well as this does work, it is very much computationally expensive as most samples will not fit the image, thus searching for the small set of fitting shapes requires to evaluate all the wrong shapes as well.
Considering artworks, brushstrokes have many more degrees of freedom, and artworks usually consist of upwards of a few thousand brushstrokes.
Consequently, it would be considerably more challenging to apply to this problem until computational resources have become a few magnitudes more powerful.

\begin{figure}
    \includegraphics{photomosaic_starry_night}
    \caption[]{Photo mosaic of Starry Night using only images by the Hubble Space Telescope. \url{http://www.astro.uvic.ca/~alexhp/new/figures/starrynight_HST.001.jpg}}
    \labfig{photomosaic}
\end{figure}

This premise can be overcome, though, by using a differentiable renderer.
A \textbf{differentiable renderer} is capable of creating/rendering shapes in the pixel domain.
In contrast to conventional renderers, it does so by solely using differentiable operations.
Thus, the previously described task becomes feasible, as random sampling can be replaced by gradient descent.
Ordinary renderers usually do not rely on differentiable operations, as faster operations with more straightforward logic render images well enough already.

Nonetheless, it is theoretically possible to create a differentiable renderer \cite{something}.

When talking about rendering brushstrokes, it would be even harder to think of a differentiable pipeline to draw brushstrokes of reasonable quality from a set of parameters.

Neural networks turn this problem around.
As neural networks are inherently differentiable, the question becomes rather how to make an existing neural pipeline render images from parameters.
Previous works have shown that neural networks are capable of conditionally generating high-resolution and high-quality images.
Conditioning the generator on brushstroke parameters as well as some noise should then output an image of the respective brushstroke with some variability to it.

The basic idea was proposed by \authorcite{japanese neural renderer}~\cite{japanese neural renderer} as it facilitates training of reinforcement learning based networks.

Inspired by this, the approach becomes more apparent.
First, a neural network is trained as a differentiable renderer.
Then the same renderer is used by a gradient descent-based optimization procedure to approximate an artwork as a set of renderer input parameters.

Both steps require some tricks to avoid pitfalls like computational limitations, which are outlined in the following two sections.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Neural Renderer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Neural Renderer}
\labsec{NeuralRend}

The neural renderer in \authorcite{japanese neural renderer}'s~\cite{japanese neural renderer} work improves an architecture based on SPIRAL \cite{SPIRAL}.
\citeauthor*{Learning2Paint}'s efforts used a differentiable renderer to facilitate deep reinforcement learning for drawing images \cite{Learning2Paint}.
As mentioned, the renderer is required to be differentiable and ideally require as few resources as possible.

\subsection{Data Set}
\labsubsec{dataset}

Unfortunately, there is no data set available for this task, which means that a data set must be created explicitly for this approach.

There are several sources for virtual and real brushstrokes, to choose from.
These resources will be evaluated in the following part.
The main focus lies on three qualities for each data source:
\begin{enumerate}
    \item Suitable data format: As brushstrokes usually overlap in a painting, the data should already provide information about opacity or transparency, favorably in the common RGBA format, which has a fourth alpha channel to hold opacity information.
    \item Data set size and variability: Only with enough different data available, it will be possible to train a renderer reliably.
    \item Image quality. Brushstrokes should be as close to real-world brushstrokes as possible to ensure high-quality renderings later on.
\end{enumerate}

\subsubsection{Brushstroke Images}
\labsubsubsec{bsimages}

There are multiple sets of hand-drawn brushstrokes available online.
Most notably, there is a set of various well-classified colors and brush styles created by 'zolee' \todo{reference this} on the platform \url{onlygfx.com}.
It consists of approximately 1000 brushstrokes that mostly follow rather straight horizontal paths.
These brushstrokes are mostly grouped by color and painting technique (oil, acrylic, watercolor...).
All images are in the PNG format with the background made transparent in a post-editing step.

This data set has the advantage that it consists of real-world brushstrokes that were painted under presumably reproducible conditions.
On the other side, brushstrokes are of mostly the same width throughout the data set and also do not come with information which path the brush took or any other non-visual information.
Also, the data is very sparse.
Many color shades are not represented, which means that the generator would have to interpolate them or simply would not be capable of rendering any brushstrokes in this color.

It seems that this data set would be suitable to replicate single real-world brushstrokes as images.
However, limitations to the data make it unlikely that a generator could learn a coherent representation from this.

\subsubsection{Painting Libraries}
\labsubsubsec{libmypaint}

The mentioned work of SPIRAL \cite{SPIRAL} relies on opposite data to real-world images.
It used the painting library 'libmypaint' \cite{libmypaint} to generate brushstrokes from parameters in real-time during training.

The apparent advantage of this and other painting libraries is the fact that one can fully control the output through parameters.
As the whole space of input parameters for the renderer can be covered, it is much easier to avoid pitfalls like they were described in \refsec{bsimages}.

Still, this data set falls short regarding the authenticity of rendered strokes.
Especially the inner area of the stroke shows a uniform color, which is far from what real brushstrokes would look like.

This data set is better suited for our task than the given images are but will tend to make all rendering look a bit 'cartoonish' or flat, which could, in turn, limit convergence during the latter optimization process.

\subsubsection{Fluid Simulation}
\labsubsubsec{fluidpaint}

Fluid Paint is a project by David Li \cite{fluidpaint} that uses simple
fluid dynamics to give artificial brushstrokes a more plastic look.
It has been implemented in JavaScript and OpenGL.

There is a C++-version in the git-repository of SPIRAL along with
Python bindings created by Yaroslav Ganin.

Using these Python bindings, it possible to generate brushstrokes locally outside of a web browser environment.

The quality and controllability of FluidPaint fall right in the middle of the two previously mentioned data sets.
The generated brushstrokes look distinctively better than those generated with libmypaint, but still lack the quality of the real-world images.
Concerning controllability, FluidPaint allows controlling the path of the paintbrush's handle rather than the path of the brushstroke itself.
As a result, there occur some offsets to a given path as opposed to libmypaint.

As a reasonable compromise between controllability, variability, and render quality, FluidPaint emerges as a sensible choice.
Although real-time data generation is not possible with this library, data generation can be parallelized.
Such a set-up still allows for the creation of large data sets in a reasonable time frame.

Even though this data set still has some weaknesses, it is probably the best choice for training a differentiable renderer because of the noted reasons.

Other 'honorable mentions' are painting programs such as \todo{these two weird software stuff things}, which allow for even more authentic brushstrokes but lack any well-documented interface in order to generate a vast number of brushstrokes.

\begin{figure*}
    \subfloat[Photograph]{%
        \includegraphics[width=.29\textwidth]{images/real_red_stroke}%
        \labfig{strokes:real}%
    }\qquad
    \subfloat[ArtRage]{%
        \includegraphics[width=.29\textwidth]{images/artrage_red_stroke}%
        \labfig{strokes:artrage}%
    }
    \subfloat[Affinity Photo]{%
        \includegraphics[width=.29\textwidth]{images/affinity_red_stroke}%
        \labfig{strokes:affinity}%
    }
    \subfloat[libmypaint]{%
        \includegraphics[width=.29\textwidth]{images/affinity_red_stroke}%
        \labfig{strokes:libmypaint}%
    }
    \subfloat[FluidPaint]{%
        \includegraphics[width=.29\textwidth]{images/fluidpaint_red_stroke}%
        \labfig{strokes:fluidpaint}%
    }
    \caption[]{Comparison of similar brushstrokes in each data set}
    \labfig{strokes}
\end{figure*}

\todo{talk about RGBA advantages}


\subsubsection{Brushstroke Formalism}
\labsubsubsec{formalism}
With the means of data set production seized \todo{cut this joke}, what is left is to formulate the parameters that define the brushstrokes.
These parameters must quantify the following three properties of brushstrokes:
\begin{itemize}
    \item color
    \item thickness
    \item path
\end{itemize}

The easiest of these three properties is quantifying the color.
Naturally, computer vision relies on the RGB format, which defines color as a set of three 8-Bit integer values between 0 and 255. 
As for path and thickness, these two properties depend on the given coordinate system.
FluidPaint represents the canvas as a 2D plane in the $[0, 1]$ range.
Thus, it makes sense to follow the same representation.

Thickness can be any value in $[0, 1]$  for each brushstroke, where 0 is an infinitely small brushstroke, and 1 is a brushstroke as wide as the canvas.
As both these edge cases do not make sense in this application, the range is constrained to $[.03, .2]$, which includes only brushstrokes that are visible and also do not cover the whole canvas.

Quantifying the path now is a little more tricky.
The fluid dynamics simulation that FluidPaint uses relies on internal time steps at which the equations are evaluated and subsequently rendered.
At the same time, each step allows only a linear motion of the brush handle between positions $a$ and $b$.
Subsequently, any curved paths must be split into linear/straight segments that together should resemble a curved line.
As more steps mean longer simulation times and fewer steps mean edgy movement, a value of 20 time-steps per stroke emerged as a good compromise \todo{this is actually different as the steps depend on the length of the path. Look this up}.
The same number of steps has been used in SPIRAL's implementation.

Another problem becomes how to express a curved path in numbers.
The most straightforward representation would be a sequence of points that make up any curved path.
Such an approach allows for the highest versatility, but at the same time introduces a noticeable amount of parameters as each point consists of 2 coordinates totaling to 40 values.
These values are also not independent of one another but should follow a reasonable path as otherwise, the resulting brushstroke would look somewhat like a random walk than an actual brushstroke.
Since works such as SPIRAL or 'Learning to Paint' face a quite similar obstacle, their solution should be applicable in this case as well.
Both used so-called 'Bezier curves' which parametrize curved paths by a limited set of numbers.
\begin{marginfigure}
    \includegraphics{bezier_sample}
    \caption[]{Sample of a 3rd degree Bezier curve, using the De-Casteljau-algorithm,
    \url{https:\/\/de.wikipedia.org\/wiki\/Bézierkurve\#\/media\/Datei:Bezier-cast-3.svg}}
    \labfig{genetic}
\end{marginfigure}

Bezier curves parametrize curved paths not as linear segments, but as analytical paths which can be used in computer models.
\todo{bezier curves margin note}
They can be of different orders, which allows them to follow more complicated paths.
In this case, the simples form -- the first order Bezier curve-- is already sufficient.
It defines a curve through its start- and endpoint, as well as a control point.
The curve is then defined as the path of a point over the time interval $[0, T]$.
First, one connects the start- and endpoints with the control point to get two lines in return.
On these lines, lie two points that move linearly along their lines within a virtual time interval $[0, T]$.
Then these two points are connected in the same way as before by a third line.
Again, this line has a point moving along its path throughout $[0, T]$.
As the first two points that define the third line will move, the line's orientation will change as well, thus translating the linear movement of the point into a complex curved path.
The path that this point then takes in $[0, T]$ defines the Bezier curve.
A first-order Bezier curve will only bend into one direction or follow a straight path.
For higher orders, the displayed process can be applied iteratively and allows for more complex curves.
As brushstrokes usually follow a quite simple path and fewer parameters are preferred, Bezier curves of first-order are suitable as parametrization.

Ultimately, this gives ten values that are sufficient to parametrize brushstrokes with certain constraints:
\begin{itemize}
    \item Three 2D coordinates that define the Bezier curves (6 values).
    \item One thickness parameter.
    \item Three values in RGB space.
\end{itemize}


\subsubsection{Data Constraints}
\labsubsubsec{constraints}

Given the parameters listed in section \ref{sssec:formalism}, the data still needs further constraints to facilitate the generator's training even further.

Section \ref{sssec:fluidpaint} already hinted at the impracticality of online data generation.
A rough estimation by timing the rendering of 100.000 FluidPaint brushstrokes reveals that a dedicated CPU server is capable of generating 300 strokes per second.
\todo{experiment in appendix}
A neural network with batch size 32 is limited to $\approx 10$ iterations per second under these circumstances, which would mean a clear bottleneck.
Thus, it seems advisable to generate data beforehand with enough samples to cover the data space sufficiently.
It will allow for much faster access to data, as individual data samples are relatively small and can be stored in a binary data file such as HDF5.

Besides this constraint to the amount of data available, another set of constraints will be introduced to reduce the data space to 'valid' brushstrokes only.
'Valid' brushstrokes will be defined as brushstrokes that resemble real-world brushstrokes to a certain degree.
This primarily concerns two relations within a brushstroke:
\begin{itemize}
    \item Its width-to-length ratio.
    \item Its curvature.
\end{itemize}

The width-to-length ratio will be restricted to brushstrokes that are at least two times as long as they are wide.
\begin{equation}
\norm{\vec{s} - \vec{e}} \overset{!}{\leq}  2 \times (\text{brush size}) \labeq{bs}
\end{equation}
Due to the simulation background of FluidPaint shorter brushstrokes will show some artifacts due to the bristles' length in the simulation which depends on the width of the stroke.
Another reason for this is the intended use-case, which will focus on van Gogh paintings.
As van Gogh did not practice pointillism, most of his strokes have a length to them, which brings such a constraint in line with some characteristics of van Gogh's style.

The same argumentation applies to the curvature:
Most brushstrokes (especially those by van Gogh) have a certain 'flow' or 'smoothness' to them, which can be described by using strokes with large curvature radii and without any corners in the strokes' path.
Thus, the data set will also be restricted to strokes that follow these descriptions.
In order to achieve this with random sampling in mind, a multivariate gaussian distribution is placed between start ($\vec{s}$) and end point ($\vec{e}$).
The two axes are rotated such that the short axis is in line with the vector
$\vec{a} = \vec{s} - \vec{e}$ while the other sits orthogonal.
Then both axes are scaled with $\norm{a}_2$ and also the handpicked values $\frac{1}{200}$ and $\frac{1}{25}$ for along $a$ and orthogonal to it, respectively.
Figure \ref{fig:datageneration} shows samples from this distribution for an exemplary brushstroke.
\begin{marginfigure}
    \resizebox{\textwidth}{!}{
        \input{images/scatter.pgf}
    }
    \caption[]{Exemplary scatter plot for given start and end point to visualize the covariance matrix}
    \labfig{scatter}
\end{marginfigure}
This distribution is intended to follow that of brushstrokes as they would appear in the real world.
The majority of brushstrokes will be straight or just slightly bent due to the maximum of the PDF being at the center of $s$ and $e$.
Bent brushstrokes will mostly be symmetric as the long axis of the multivariate gaussian is orthogonal to $a$.
Still, there will be strokes that have their bent towards either end of the brushstroke as well as some strokes with a high curvature.
The area of interest, though, will be densely populated as intended.
\begin{align}
    p(\vec{c}| \vec{s}, \vec{e}) & = \mathcal{N}(\mu, \Sigma) \labeq{checkpoint} \\
    \mu & = \frac{\vec{s} - \vec{e}}{2} + \vec{e} \\
    \Sigma & =
        \begin{pmatrix}
            a_x & 0 \\
            0 & a_y
        \end{pmatrix} \labeq{Sigma}\\
    \vec{a} & = \vec{s} - \vec{e}
\end{align}

The color of the brushstrokes is not constrained as the color distribution of the target data set is not known at this point. \todo{why not van Gogh color distribution?}


%is this number of parameters enough, too much?

\subsubsection{Data Set Creation}
\labsubsubsec{creation}

The data set will be created with 100.000 samples that follow the constraints that were presented in section \ref{sssec:constraints}.
As an underlying distribution, the uniform distribution is chosen as it allows a more evenly coverage of the data space.

First, a set of start and end points, as well as brush size, is drawn and checked against \eqref{eq:bs}.
If the constraint is not met, the set will be redrawn entirely.
In case the constraint is satisfied, a checkpoint is sampled according to \eqref{eq:checkpoint}.
If $\vec{c}$ lies outside the render window, the checkpoint will be resampled.
At last, an RGB set is sampled from a uniform distribution as a color.

The resulting tuple of start, end and control point, brush size, and RGB color is then added to the data set.
Before rendering starts, the values of $\vec{s}$, $\vec{e}$ and $\vec{c}$ are scaled with the handpicked factor of $0.7$ to ensure the brushstrokes are rendered completely within the window and not cut by an edge of the render window.
At last, the brushstrokes are rendered according to the data set and added as well.

The render canvas size was chosen to be 64x64 pixels for several reasons:
First, even with such a small canvas size, training for the renderer takes about one day.
Secondly, the larger the render canvas size becomes, the deeper the renderer needs to be, which results in more computational overhead in the optimization routine as well as more layer through which the gradient has to be propagated.
Lastly, as there will be upwards of a thousand brushstrokes in a single image, increasing the canvas size to 128x128 would require four times as much memory per rendered image. As 1000 brushstrokes would already account for $1000 x 64 x 64 x 4 \si{\byte} \approx 16.4 \si{\gibi \byte}$ a fourfold increase would be significant.


As the last step, the data set is renormalized to the range $[-1, 1]$ for convenience and to facilitate training as well.


\subsection{Architecture}
\labsubsec{arch}
The architecture of the brushstroke generator follows that of an inverse VGG network.
It is widely used and has shown in previous works that it should be capable of handling this task.
\todo{check the details} The architecture consists of three dense layers at the beginning, followed by a two-times upsampling layer as well as three convolutional layers.
The same pipeline with a two-times upsampling layer, and three convolutional layers is repeated until the target size is reached.
After the last convolutional layer, a hyperbolic tangent function is applied to restrict the output to the $[-1, 1]$ range.
As part of the hyper-parameter search, different tweaks to the architecture have been tested:
\begin{itemize}
    \item An additional noise input at every layer with a size equal to that of the existing signal.
    \item Additional information about the position in the pixel grid in every layer, so-called CoordConv \cite{coordconv}.
    \item Various combinations of activation and normalization functions.
\end{itemize}

\begin{figure*}
    \resizebox{1.5\textwidth}{!}{
        \input{images/vgg16.tex}
    }
    \resizebox{1.5\textwidth}{!}{
        \input{images/vgg16.tex}
    }
        \caption{Visualization of the generator and discriminator architectures}\labfig{genarch}
\end{figure*}

The discriminator is designed after the same principles and resembles a VGG encoder network
First, three convolutional layers are applied, followed by a downsampling /pooling layer.
This structure is repeated until a target resolution of 4x4 pixels is reached.
Then a set of three dense layers is applied to give one final prediction per sample.

\subsection{Training}
\labsubsec{train}
During training, the L2 distance and the FID score serve as evaluation metrics.
The FID score becomes necessary as the visual comparison of the generated samples has proven difficult between different runs.
The L2 distance does not qualify as a sufficient metric for later training stages as the stochastic nature of the brushstrokes puts a threshold on how short the L2-distance can become.

A two-time-step update rule was implemented to stabilize training further.
\todo{write this down and look at the tricks that were used}

\subsection{Results}
\labsubsec{results}
\todo{pick some results and present whether they are any good}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Stroke Approximation %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Stroke Approximation}
\labsec{strokeapprox}

\subsection{Data Set}
\labsubsec{dataset}
First off, for this section the data set will be presented.
This is due to the data defining some demands for the networks later on.
The data set for the optimization task should also meet a few requirements.

In order to be able to focus on the brushstrokes in an image the data set should consist of relatively high resolution images.
This becomes necessary as most brushstrokes should fit into a 64x64 window any larger strokes could not be approximated with a single renderer and thus would falsely be constructed of multiple brushstrokes.

This requires more information per image than simply the resolution as the scale of the image plays into this requirement as well.
Ideally each image should be accompanied by its size as this allows to rescale all images accordingly, given that one knows how large a typical brushstroke is.

At last the painting's technique must be oil on canvas or similar techniques as this is what the renderer has been trained for.

All these requirements on the data can be met by using data directly from the Van Gogh Museum in Amsterdam that is freely available online.
Each high resolution image is categorized by its technique as well as the period in which it was painted accompanied by information on the measurements of the image.

\subsection{Weapon of Choice}
The next stage of training takes the now pre-trained neural renderer and combines it with the task of approximating brushstrokes in images.
In theory the neural renderer should allow to gather gradients in parameter space even though meaningful losses are calculated in image space, as explained in section
\ref{sec:motivation}.
There, it was outlined already that the weapon of choice will be an optimization procedure that relies on the capabilities of the neural renderer.

But this is not the only possible approach to dissecting images into sets op parameters.
As explained in \ref{sec:PR} there exist a variety of approaches for this kind of task.
Some of these promise fast generation of parameters from images which makes the optimization approach in this work seem like a step back at first.
The following section is dedicated to justifying the chosen approach by comparing state of the art implementations of existing approaches.

The targeted task is to approximate the representation of an image with roughly 1MP by the means of $\approx 10.000$ brushstrokes (see section \ref{ssec:motivation}).

\subsubsection{Genetic Algorithms}
\labsubsubsec{genalg}
Genetic algorithms are possibly the simplest approaches.
As it was laid out in sections \ref{sec:motivation} and \ref{sec:genetic}, genetic algorithms use random sampling and previous best results to approximate fitting solutions to the rendering problem which was described in section \ref{sec:renderproblem}.

Current state of the art solutions are capable of finding a solution in $\approx 1h$, when searching for an approximation of a 1MP image with simple geometric shapes like circles or triangles.
This accounts for roughly $1 \times 10^{9}$ sampling steps.
This time frame varies depending on target shape density, target accuracy, sampling runs per shape and degrees of freedom (with the latter requiring more sampling runs).

Looking back at seciton \ref{sec:renderer} each brushstrokes has 10 degrees of freedom with some inter-dependencies between them.
Also it is known that the obtained neural renderer is capable of generating $\approx 300 \frac{\text{images}}{s}$.
This means that rendering the same amount of sampled images will take:
$$
\frac{10^{9} \text{samples}}{300 \frac{\text{images}}{s}} \approx 3.33 \times 10^{6} s
\approx 926 h \approx 38.6 d
$$
Which is an impossible amount of time to spend \textbf{per image}.

At this point the number of samples has not been corrected for by the higher number of degrees of freedom of this problem.

Ultimately this means that genetic algorithms are not an option for this task.

\subsubsection{Brush Stroke Extraction}
\labsubsubsec{bse}
Next in the line of approaches are algorithm based approaches that use standard computer vision techniques to extract brushstrokes from an image and parametrize them.

Besides texton based image characterization \cite{textons} and pure filter based approaches \cite{filters} there have been approaches to extract brushstrokes or some of their characteristics \cite{brushstrokecharacteristics} \cite{brushstrokeextraction}.

The latter would pose a valid option for the main goal of this thesis if it were to characterize brushstrokes reliably.
Unfortunately the best existing techniques fail to detect brushstrokes equally  over the whole image but only identify the most significant ones (see figure \ref{fig brushstrokeextracted}).
Other approaches are only able to extract only few characteristics like the orientation of a brushstroke, which proves insufficient as well.
\todo{show some of the extracted brushstrokes}

\subsubsection{Stroke Based Rendering}
\labsubsubsec{sbr}
A different field that also uses algorithm to approximate an image through brushstrokes and also wants to achieve some stylization along the way is Stroke Based Rendering or Painterly Rendering.
While early works relied on interactive approaches, later publications then were able to fully automate this process.
Judging from tests by the authors themselves such an approach would take between \todo{this number} and \todo{this number} hours per image to fully render it this way.
As figures \ref{fig:sbr1} and \ref{fig:sbr2} show, these approaches obtain similar results to genetic algorithms and tend to draw and image from a coarse scale to a finer scale instead of locally coherent.

At last these approaches were not intended to be used to obtain brushstrokes from a painting but rather focus on stylizing images.
This renders such an approach unfit for the goal of this thesis as well.

\subsubsection{Drawing Networks}
\labsubsubsec{drawingnetworks}
Lastly, drawing networks are the newest iteration of approaches in this field.
Beginning with \todo{which one was first?} there have been approaches the make use
of feed-forward or recurrent neural networks combined with either supervised training
or deep reinforcement learning.
The best results by these approaches are shown in figure \ref{fig:drawingnets}.

Noticeably, all of these images have a maximum resolution of 256 pixels along the
longer edge.
There have not been any approaches yet, which are able to go significantly beyond
this limitation.
Especially, the high computational costs of training recurrent neural networks seems
to be an obstacle when shooting for higher resolutions.
As it will be outlined in more detail in \ref{ssec:dataset} such resolutions can not
be deemed sufficient when looking at individual brushstrokes.

This, also rules out drawing networks.

\subsubsection{Combined Approach}
\labsubsubsec{combinedapproach}

Even though there have been plenty of previous approaches to the task of extracting brushstrokes from images or to similar task like rendering images through brushstrokes; none of these quite meet all the requirements that have been posed for this task.
Namely, high resolution input images, brushstroke focussed rendering and/or retrieval, limited resources and realistic depiction of brushstrokes.

Even though brushstroke extraction seems to be most fit for the main asset of this thesis, stroke based rendering and drawing networks both bring features into the mix, that seem capable of making the task at hand more feasible.
Drawing networks for once introduced differentiable renderers as a tool to facilitate training.
Stroke based renderers achieve a parametrization implicitly by focussing on replicating the entire image instead of extracting brushstroke by brushstroke.

This can be combined into a unified approach that uses the differentiable renderer and the objective of recreating an entire image.
The resulting approach does not use re-sampling like genetic algorithms do, but can rely on gradient descent to converge to a solution significantly faster.

The limited capabilities of such a renderer would also guarantee that any approximation is composed of only valid brushstrokes instead of single pixels it would be the case with normal generative models.

All in all such an approach fits best to the goal of this thesis as it will generate an approximation of a target image that ideally resembles the target as closely as possible while being limited to the use of brushstrokes.



The decision of whether to use an optimization based approach is also linked to another question: Whether to use a sequential or parallel placement of brushstrokes.

As an example most drawing networks rely on a sequential approach as do some stroke based renderers.
Intuitively it also makes sense to use a sequential approach as artists also place their brushstrokes sequentially on the canvas.
Yet there is a major difference in how existing computer vision based approaches place their strokes compared to artists.
Due to the loss functions that are used when training drawing networks or stroke based renderers the resulting images tend to be made up entirely of large canvas-filling brushstrokes which reduces the L2 loss very fast.

This is contrary to an artist which would maybe start with some sketches first but then fills the canvas not with a brush that is roughly the size of the canvas -- as these algorithms do -- but with a significantly smaller brush that requires many individual strokes (often with some pattern) to fill large areas.
Thus, sequential approaches do actually differ significantly from how an artist would paint.

In contrast to this parallel approaches which predict the whole painting in one step are not wide spread due to the computational pitfalls that come with predicting brushstrokes for a whole painting at once compared to predicting only a few at a time.
Also, parallel approaches require to deal explicitly with the order in which brushstrokes should be placed on the canvas, as it will be explained in section \ref{sssec:order}.
Still, if one considers the optimization procedure instead of a policy network, it becomes clear that the there are interactions between strokes that are locally close.
An example would be one stroke in the foreground changing its path and revealing the canvas beneath, then another stroke in the background should cover this up if the color matches.

As usually artists plan their future brushstrokes when painting the background it would require a drawing network to plan far head which is impossible to do with an optimization based approach.

Another argument for parallel optimization is the actual reliance on actual visual proof for approximating brushstrokes.
Sequential approaches will again an again cover up previous brushstrokes until maybe only a few pixels of the background brushstroke are visible.
Even if these few pixels match the target image well it will be a shot in the dark whether this is really how the artist has drawn the background, which in return introduces a great deal of noise to the result.
A parallel approach on the other hand should only place brushstrokes where there is visual evidence that there is a brushstroke and concentrate less on possible background arrangements.

\subsubsection{Pitfalls of Feed-Forward Approaches}
\labsubsubsec{ffapproaches}

In the course of this thesis there were experiments targeting a feed forward approach before ultimately tending toward the optimization based approach that is presented.

The main reason for this change in direction can be pinpointed to two problems that emerged.
The computational burden that of a feed-forward approach can be approximated when looking at existing feed-forward approaches of drawing networks.
Without compromising resolution and this images quality significantly, training a network can hardly be realized.
Still it was possible to show a basic implementation of this for very simple data like the cMNIST data set. \todo{margin explain cmnist} \todo{add image that were drawn by ff network}

Another problem that occurred was the placement of brushstrokes on the canvas.
As artists are not bound to the same pixel grid as images are, they can place brushstrokes freely on the canvas.
More so, they can pack brushstrokes densely in one area while distributing them broadly in another.

As most neural networks in computer vision are CNNs, they are not able to allow for a similar behaviour as they will always follow a grid layout of various resolutions.

Repeated experiments with either displaceable grid cells or stacked signals have proven too complicated to manage for a convolutional network architecture and also seemed to scale badly when implemented in fully convolutional manner.

Thus an optimization based approximation was chosen as it offers good approximations at high resolutions with manageable computational overhead.

\subsection{Optimization Procedure}
\labsubsec{opt}
Since the previous section \ref{ssec:ffapproaches} already specified why an optimization based approach has been chosen over a feed forward or recurrent approach, this section is meant to explain a little more about the optimization procedure itself.


\subsubsection{Rendering Layout}
\labsubsubsec{layout}
Fundamentally, the optimization procedure is inspired by stroke based rendering procedures.
Also, it could be compared to the style transfer approach by Gatys \etal \cite{gatys}, where,  instead of pixels there a parameters optimized.
The difference to normal stroke based renderers, though, is the limited size of a rendered brushstroke in this work's renderer.

This poses a significant challenge that might not be obvious at first.

Ideally the optimization procedure should be able to place strokes freely on canvas, as this allows for an unbiased approximation.
Furthermore, this allows to allocate many small strokes in areas where the artist placed many strokes as well and use fewer and wider strokes in other area during the optimization procedure.
But due to various limitations which were explained in \ref{sec:NR}, the renderer is not able to render single brushstrokes in a 1MP frame. Similar approaches only perform on relatively small canvas sizes, likely due to this issue \todo{add ref}.

One work-around for this would be to dissect the target image into many smaller patches and then run the optimization procedure on these individual patches.
This could be compared to a grid of renders where each grid cell is the center of many renderings at the same time \todo{add figure to this}.
Right away, a major issue with this becomes obvious, which are the edges where the grid cells are joined.

As a grid structure will almost always always differ from the inherent distribution of brushstrokes in an image, grid edges will more often than not, separate brushstrokes between two grid cells.
A simple solution to this problem is going from a stacked grid structure to an overlapping grid. \todo{graphics!!!!!}

This hides obvious edges between render windows as every edge coincides with the center of a different render window.
It is easily realized by choosing a lattice vector size smaller than the render window's dimensions.

Still, this kind of initialization requires a very even distribution of brushstrokes with only small dislocations at each point in the lattice.
As this is not the case and stroke densities will vary locally, the grid layout is prone to lead to enforcing a grid-like layout of strokes were there isn't any.
Mainly due to the inability of the grid to account for local changes in density and the following propagation of error.
This would start with a single region of high density strokes in the vicinity of one grid cell which would ideally result in a narrow stroke to achieve high accuracy.
Neighboring cells then have to shift their strokes towards the center of that grid cell to account for the free space that is not covered by the narrow brushstrokes.
This shift must then be accounted for by the next neighboring cells and so on, which will cause all strokes in a raw or column to shift towards this one spot with a high stroke density.

Now, a painting usually has many such high density areas which would require the optimization procedure to balance the shift that is caused by these areas.
As a result the strokes are likely to not shift at all as shifting will equal out for many such areas of high density.
Subsequently, an area of high stroke density will be not have enough strokes available in its local region and thus will be covered up by a single broad stroke as this minimizes the L2 loss.

The core of the problem is the previously imposed lattice structure that propagates local density shifts along its principle axis.

One possible solution to this is getting rid of the lattice structure and replacing it with a more random structure that also covers the image sufficiently.
This can be accomplished by using \textbf{super-pixels} \cite{superpixels}.
Super pixels come from an earlier era of computer vision and are often used in image segmentation tasks (\cite{img segmentation with SP}) but super-pixels are also a popular starting point for brushstroke extraction algorithms \cite{brushs stroke extraction}.

Basically, super-pixels are disjoint groups of pixels in an image that would usually combine pixels with similar colors in a local region.
Straight away, it is obvious why this is interesting for brushstroke extraction.
In theory the distribution of super-pixels will not follow a grid-layout as the previous approach and the location of super-pixels should relate to the given color distribution in the image.
It is easy to imagine that the location of the super-pixel centers would be a good prior of locations of render windows as well.
Also, as the colors of pixels inside a super-pixel should be similar, one can use the mean color of a super-pixel as an initialization for the color of the brushstroke.

Ultimately, a super-pixel segmentation will be used to infer positions for render windows as well as the color initialization of each strokes.

\subsubsection{Rendering Order}
\labsubsubsec{order}
Another problem that will come up during the optimization procedure is the order in which strokes are rendered.
Real world brushstrokes are also subject to the same issue as later brushstrokes will always be placed on top of earlier brushstrokes, with no way of changing this (see section \ref{sssec:combinedapproach}).

As the optimization based approach relies on parallel optimization of brushstrokes, it must be decided which strokes are in the foreground and which are in the background.
Otherwise, as this would randomly change, edges in the image might be obstructed and optimization could oscillate between solutions where different strokes lie in the foreground.
It could also lead to brushstrokes not overlapping but covering disjoint areas.
All of these outcomes would be unfavorable as it tends to produce worse results in the end.

The solution which is presented in this thesis is an additional parameter that describes a brushstroke's accuracy.
The accuracy is defined as the L2 distance of each stroke's pixel to the corresponding pixel in the target image multiplied by this each pixel's alpha value.
This removes any pixels which are of no interest from the loss and focusses only on the rendered pixels.
\begin{align}
    \text{accuracy} = 1 - \frac{1}{N} \sum_{p \in \text{pixels}} \norm{p - p'}_2^2 \text{ with } \vec{p} = \vec{p'} 
\end{align}
    \todo{improve this}

The resulting value describes how well the pixels of the rendered stroke match their respective pixels in the target image.
Consequently, any brushstroke with a higher accuracy will be more faithful to the target image than strokes with a lower accuracy.
Placing these brushstrokes in the foreground should thus result in a smaller L2 loss than the other way around.
Vice versa, brushstrokes which connect two same colored areas will aggregate a lower accuracy as the brushstroke is compared at the intersection as well.
A rendered brushstroke which fits to the foreground brushstroke will not be affected by such a penalty thus getting a higher accuracy and laying on top of the other brushstroke.

Notably, the accuracy should not be included in the brushstroke's loss, as this would prohibit background strokes from covering larger areas and result in behaviour that is similar to non-overlapping issue previously described.
Thus, the accuracy of each stroke will be calculated as it is rendered.


\subsubsection{Initilization}
\labsubsubsec{init}

The following section will focus on initialization details for all parameters of a brushstroke, their position and the confidence value.

Besides the original 10 parameters of each brushstroke, which were explained in section \ref{sssec:formalism}, the previous two section introduced an accuracy parameter for ordering and two translation parameters which define the position of the render window along each axis.
All of these parameters must be initialized before the optimization procedure starts.
Ideally, the initialization should not introduce any bias to the optimization process.
At the same time an initialization should facilitate training and accelerate convergence in early stages of optimization.

Unfortunately, the placement of the render window will surely enforce a bias on the optimization as hinted in section \ref{sssec:layout}.
There a super-pixel initialization was motivated and for the translation parameters as well as the color of the brushstrokes.
Subsequently, the translation parameter for each render window will be equal to the position of the weighted mass center of its respective super-pixel.

The initial color will be taken from the mean color value over the super pixel.

The brush size will be initialized with the minimum possible value.
This will let brushstrokes not overlap at the beginning of optimization.
Only when the brushstrokes already roughly fit their local region they shall intersect and be ordered by their accuracy.
Therefore, the initial accuracy will be 0 everywhere, as the accuracy is recalculated after every optimization step.

Other patch parameter, notably $\vec{s}$, $\vec{e}$ and $\vec{c}$, will be initialised using a narrow normal distribution with $\sigma = .1$ and values clipped to $[-1, 1]$.
This is done, as there is no prior information available on how the brushstrokes are oriented.
Instead, this approach relies on the optimization procedure to be minimally biased by this initialization of the path variables.

\subsubsection{Partial Updates}
\labsubsubsec{partupdate}

One major problem that emerges when building an optimizer based on the information provided up until now, is the limitations of GPU memory.
Even as it might not be obvious at first, the optimization procedure imposes a huge requirement for memory on the graphics card.
This is mainly due to two parts of the training:

First, the number of brushstrokes can easily become very large, if large enough images are used as input.
As the render window size of 64x64 already suggests the brushstrokes are relatively small and mentioned in section \ref{sec:motivation}, a painting can consist of a few thousand brushstrokes.
This would equate to a batch size of a few thousand for the brushstroke renderer.
Tests have shown that on an NVIDIA GTX 2080 Ti with 12GB GDDR5 VRAM the maximum number of brushstrokes rendered in parallel is $\approx 256$.
Obviously this is one to two orders of magnitude smaller than what would be needed to optimize all stroke in the painting in parallel.
Still, there is a way around this bottleneck, by optimizing the image not as a whole but as smaller patches consisting of 256 brushstrokes at a time, giving a partial update routine.

Each patch comprises the 256 nearest render windows to a randomly sampled location on the canvas.
These 256 brushstrokes are then rendered from their parameters in order to obtain a gradient later on.
Then the strokes are placed on the canvas or 'padded' and than blended or 'stitched' after they were ordered according to their accuracy.
At last, the loss is calculated and backpropagated to update the parameters of the patch.

As this will not make a difference for any brushstrokes at the center of the patch, border brushstrokes (those which do not solely neighbor brushstrokes that are also optimized) will be affected by this.
This is the reason why it makes sense to safe the rendered brushstrokes for all parameters at any time forming a \textbf{render image catalogue}.
Equally the collection of brushstrokes that shall later compose the image shall be called a \textbf{parameter catalogue}.
With the render image catalogue at hand, it is possible to use the previously rendered brushstrokes to render the image as whole with the newly rendered brushstrokes of the image patch embedded.

As this will cause the border strokes to be surrounded by other brushstrokes to all sides (even if not all of them are freshly rendered) the effect of the partial update routine vanishes.

The other problem that will occur for very large input images, is that the stitching of brushstrokes itself takes up a huge mount of RAM.
Going into detail, each brushstroke must be placed on the virtual canvas individually,  where the canvas' size is that of the input image.
This would equate to a couple thousand 1MP images being stored in RAM before they are stitched to a single 1MP images.
As a single 1MP image carries roughly $1000 \times 1000 \times 4 \text{channels} \times 8 Bit = 32 \times 10^{6} Bit = 4 \times 10^{6} Byte = 4 MB$ of information, a few thousand of these will easily eat up the RAM of most graphics cards.

Luckily, the previous work-around for optimizing only 256 brushstrokes will work as well without rendering the full image at every step.
Since most brushstrokes are not re-rendered anyway and thus will not be supplied with a gradient, their main task is to regulate losses for edge strokes of the image patch.
As this does not need far away strokes but only those close the strokes that are optimized, a ring of pre-rendered strokes around the re-rendered patch will suffice.

\todo{approximate the amount of brushstrokes needed to surround the patch}
This allows to reduce the amount of involved brushstrokes per optimization step from a few thousand down to a couple hundred.
Besides allowing for the partial update routine to be performed at all, it also should increase performance significantly compared to an approach that involves all brushstrokes at any time.

\subsection{Image Placing \& Blending}
\labsubsec{placing_blending}
As the update and optimization procedure has now been explained thoroughly, it is now time to explain the process of placing and blending a rendered brushstroke a bit further.

After each stroke has been rendered it needs to be placed according to the translation parameters that were introduced in section \ref{sssec:layout}.
This requires dynamically placing each brushstroke inside zero-filled tensor.

By calculating the global position of each pixel in the rendered image individually, it is possible to scatter the pixels of the original rendered image into the larger zero-filled tensor and obtain a globally placed brushstroke.

What is more complicated though is the task of blending the resulting canvas-sized renders together or stitching them.
Due to the canvas' alpha channel it is possible to blend only relevant information while the rest of the image will be ignored.

As far as conventional alpha blending goes, two images are blended by multiplying each pixel value with the alpha value of the top-layer image while the background image is multiplied with the complement to the alpha value:
\begin{align}
    p_{x,y} = p^{\text{top}}_{x, y} \times \alpha^{top}_{x, y}
    + p^{\text{bottom}}_{x, y} \times (1 - \alpha^{top}_{x, y})
    \forall (x, y) \in \mathcal{D}(\text{image})
\end{align}
\todo{correct this equation and make it nicer}

For multiple layers this process can be repeated in various fashions, after the strokes are ordered according to their accuracy.
Either one could start from the bottom and blend the two back-most strokes, followed by the next third last strokes and so one, or one could start this process from the front with the two strokes in the very front being blended at first, then the stroke with the third highest accuracy \etc.

Both of the approaches would yield the same result but differ only in the order in which they were blended.
Subsequently, both methods will have $(n-1)$ blending operations to compute per pixel.

By moving from the linear approaches to a tree-based approach this number will not decrease any further.

What makes it possible to reduce this number, though, is blending the strokes in  a content aware fashion.
As looking at figure \ref{fig:stroke_on_canvas} shows, the majority of pixels for each padded brushstroke is non-informative, as the alpha value is zero.
This hints at possibility to turn the process of blending around and instead of merging layers subsequently as a whole, merging all layers at the same time by picking relevant layers for each pixel.
Then all non-zero layers are merged in the same way as ordinary alpha blending works but with the number of layers being reduced and information inside a layer not being coherent anymore.


The easiest way to accomplish this is to first find the maximum depth over all pixels, where the depth $k$ is the number of layers where the alpha value is not zero.
\begin{align}
    k = \argmax_{p \in \text{pixels}} \sum_{i \in \# \text{layers}} \mathds{1}(\alpha_i > 0)
\end{align}

Then the top $k$ layer indices for each pixel are picked, which reduces the number of blending operations from $(n-1)$ to $(k-1)$.
Importantly, the top $k$ indices should not be ordered by their alpha values but remain in the order that was imposed by sorting according to the accuracy value.
Otherwise, the order will most certainly be mixed up and the pixel with the highest alpha value will always lie on top instead of the pixel that belongs to the most accurate brushstroke.
Especially as brushstroke renderings fade out towards their edges, this makes a significant difference.


Another way of accelerating the process of alpha-blending is vectorizing the process instead of iteratively applying the computations.
To achieve this it is necessary to construct a tensor with the following properties:

For $\tensor{I} \in [0, 1]^{H \times W \times 4} $ the image target, the shape will be defined
as $\mathcal{S}(\tensor{I}) = (H, W, 4)$.
Each alpha channel will have the values $\alpha^{hw} \in [0, 1]$ for $h = 0, ..., H$ and $w = 0, ..., W$.

The set of rendered and padded brushstrokes $\tensor{J}$ will have the shape $(N, H, W, 4)$ with $N$ depicting the number of brushstrokes that ought to be stitched simultaneously.

Now, looking at each individual pixel in $\tensor{J}$, which is described by $(z^{hw}_n, \alpha^{hw}_n)$ for $n = 1, ..., N$ and $z^{hw}_n \in [0, 1]^{3}$, $z^{hw}$ describes the RGB values and $\alpha^{hw}$ the alpha-channel for a pixel at $(h, w)$.

A blending operation can then be defined by
\begin{align}
    z'^{hw} & = \tilde{\alpha}^{hw} \cdot z^{hw} \\
    \text{or} \\
    z'^{hw} & = \sum_{n=1}^N \tilde{\alpha}^{hw}_n  z^{hw}_n \\
\end{align}

with $z'^{hw}$ the resulting RGB values of the blended pixel and $\tilde{\alpha}^{hw}$ a vector that holds the merged alpha values for each pixel:
\begin{align}
    \tilde{\alpha}^{hw} & =
    \begin{pmatrix}
        \alpha^{hw}_1 & &\\
        \alpha^{hw}_2 & (1 - \alpha^{hw}_1) &\\
        \alpha^{hw}_3 & (1 - \alpha^{hw}_2) & (1 - \alpha^{hw}_1)\\
        \vdots & &\\
    \end{pmatrix}
    \\
    & = \alpha^{hw} \odot 
    \begin{pmatrix}
        1  &\\
        (1 - \alpha^{hw}_1) &\\
        (1 - \alpha^{hw}_2) & (1 - \alpha^{hw}_1)\\
        \vdots &\\
    \end{pmatrix}
    \\
    \rightarrow  \tilde{\alpha}^{hw}_n & = \alpha^{hw}_n \prod^{n-1}_{i=1} (1 - \alpha^{hw}_i)
\end{align}

Where $\odot$ describes the element-wise product.

What is left, is to find a way to construct $\tilde{\alpha}^{hw}$ from $\alpha^{hw}$.

For this an auxiliary matrix $\beta^{hw}$ is constructed:
\begin{align}
    \beta^{hw} = \alpha^{hw} \times \mathbb{1}_{1 \times N} = 
    \begin{pmatrix}
        \alpha^{hw}_1 & \alpha^{hw}_2 & \hdots & \alpha^{hw}_n\\
        \alpha^{hw}_1 & \alpha^{hw}_2 & \hdots & \alpha^{hw}_n\\
        \vdots & \vdots & \ddots & \vdots \\
        \alpha^{hw}_1 & \alpha^{hw}_2 & \hdots & \alpha^{hw}_n\\
    \end{pmatrix}
\end{align}
with
$$
\mathbb{1}_{1 \times N} = \begin{pmatrix}
        1 \\
        1 \\
        \vdots\\
        1 \\
    \end{pmatrix}^T
$$

Then $\beta^{hw}$ is strictly triangulated such that:
\begin{align}
    \gamma^{hw} & = \beta^{hw} \odot
    \begin{pmatrix}
        0 & 0 & 0 &\hdots & 0\\
        1 & 0 & 0 &\hdots & 0\\
        1 & 1 & 0 &\hdots & 0\\
        \vdots & \vdots & \ddots & \ddots & \vdots \\
        1 & 1 & \hdots & 1 & 0\\
    \end{pmatrix}
    \\ & = 
    \begin{pmatrix}
        0 & 0 & 0 & \hdots & 0\\
        \alpha^{hw}_1 & 0 & 0 & \hdots & 0\\
        \alpha^{hw}_1 & \alpha^{hw}_2 & 0 & \hdots & 0\\
        \vdots & \vdots & \ddots & \ddots & \vdots \\
        \alpha^{hw}_1 & \alpha^{hw}_2 & \hdots & \alpha^{hw}_{n-1} & 0\\
    \end{pmatrix}
    \\
    \rightarrow \delta^{hw} = 1 - \gamma^{hw} & = 
    \begin{pmatrix}
        1 & 1 & 1 & \hdots & 1\\
        (1 - \alpha^{hw}_1) & 1 & 1 & \hdots & 1\\
        (1 - \alpha^{hw}_1) & (1 - \alpha^{hw}_2) & 1 & \hdots & 1\\
        \vdots & \vdots & \ddots & \ddots & \vdots \\
        (1 - \alpha^{hw}_1) & (1 - \alpha^{hw}_2) & \hdots & (1 - \alpha^{hw}_{n-1}) & 1\\
    \end{pmatrix}
\end{align}

By multiplying the elements along each row in $\delta^{hw}$ one gets:

\begin{align}
    \epsilon^{hw}_i = \prod^N_{j=1} \delta^{hw}_{ij} & =
    \begin{pmatrix}
        1  &\\
        (1 - \alpha^{hw}_1) &\\
        \vdots &\\
        \prod^{N-1}_{j=1} (1 - \alpha^{hw}_j)
    \end{pmatrix}
    \\ \rightarrow \tilde{\alpha}^{hw} = \epsilon^{hw} \odot \alpha^{hw} & =
    \begin{pmatrix}
        \alpha^{hw}_1 & &\\
        \alpha^{hw}_2 & (1 - \alpha^{hw}_1) &\\
        \alpha^{hw}_3 & (1 - \alpha^{hw}_2) & (1 - \alpha^{hw}_1)\\
        & \vdots &\\
        \alpha^{hw}_N & \prod^{N-1}_{j=1} (1 - \alpha^{hw}_j)
    \end{pmatrix}
\end{align}

This vectorized version of alpha blending will introduce a new possible bottleneck as it is, since $\beta^{hw}$ will be a tensor of shape $(N, N, H, W)$, which will equate to
$$
256 \times 256 \times 256 \times 256 \times 4 \si{\byte} = 2^{36}\si{\byte} = 64\si{\gibi\byte}
$$
alone.

This is where the previous content aware alpha blending tricks becomes useful.
By computing $\beta^{hw}$ only through the top $k$ values of $\alpha^{hw}$ instead of the full tensor $\alpha^{hw}$, the size will be reduced to
$$
k \times k \times 256 \times 256 \times 4 \si{\byte} = k^2 2^{22}\si{\byte} = k^2 \times 4\si{\mebi\byte}
$$
As the shape is reduced to $(k, k, H, W)$.

Ultimately, this accelerates optimization by a factor of $2-3$ as it will be shown in \ref{sec:exp:vectorization}.

It must be mentioned that the upper boundary for computational complexity in using this kind of alpha-blending is $\mathcal{O}(N \log k)$ as the top $k$ search is bound by this complexity.
\todo{calcualte this}

\subsubsection{Losses}
\labsubsubsec{losses}

In this section the different kinds of losses for the optimization procedure will be discussed.

First off, the L2 loss or \textbf{mean squared error} is an obvious choice for this task.
\begin{align}
    L_{\text{MSE}} = \frac{1}{HW} \sum_{p \in \text{pixels}} \norm{z(p) - z'(p)}_2^2
\end{align}

Since the MSE loss focuses on minimizing the pixel-wise error between the target image and the fully rendered approximation by brushstrokes, it will cause the rendered image to match the target image mainly in color.
At the same time MSE loss is prone to blurring which results in washed out edges in the rendered image.
Thus, this loss is must be accompanied by additional losses to make up for the shortcoming of MSE loss.

A popular choice for preserving content in an image which, which is associated with preserving edges, is \textbf{perceptual loss}.
Perceptual loss is based on a VGG Network \cite{VGG} that is pre-trained on ImageNet \cite{ImageNet}.
To compute the loss, the activations of deep layers (usually the fourth convolutional block) of the pre-trained VGG network are inferred and then compared using MSE loss.
\begin{align}
    L_{\text{perceptual}} = \frac{1}{H_f, W_f} \sum_{p \in \text{pixels}} \norm{f(p) - f'(p)}_2^2
    \labeq{percep}
\end{align}
The resulting distance is meant to capture how well edges between the two input images are preserved which should be equal to whether the content in both images is the same.
Together with MSE loss perceptual loss is often used to get better reconstructions than with MSE loss alone, as edges of object in the image are better preserved, prohibiting blurriness that would occur otherwise.

As an evolutionary step to perceptual loss, Zhang \etal \cite{lpips} introduced  perceptual similarity or \textbf{LPIPS loss} which weighs the different layers of the pre-trained VGG-network differently in order to increase the effectiveness of perceptual distance between two images.
LPIPS loss is meant to preserve edges even better than perceptual loss does with a similar computational overhead.

Besides losses that operate in pixel space, it is also necessary to restrict the action space for each brushstroke.
As it has been explained in section \ref{ssec:render:dataset} the renderer has been trained on a limited data set which put constraints on how curved brushstrokes may be and how the ration between length and width ought to look like.
These constraints must be enforced in the optimizing process as well.
Because the renderer has nit been trained on data that lies outside of the generated data set, it is very likely that that the renderer will break if the input parameters lie to far apart from the space it has been trained in.
The results would then be renderings with no output, distorted brushstrokes or just noisy output, as it can be seen in \ref{ssec:ablation:renderer}.

Thus, one must think of an additional loss to confine the parameter space to the same space as the generated brushstroke data during optimization.
There are two ways of achieving this:
\begin{itemize}
    \item Discriminators
    \item Explicitly coded losses
\end{itemize}

\paragraph{Discriminators} are a popular choice in this context because even if the data distribution is not known beforehand, a discriminator is still able to learn the distribution from data and thus point out wrong parameter combinations in this case.
Still, a discriminator comes with a few compromises, as the target distribution will never be perfectly learned rather than well approximated by a discriminator.
This leaves room for weaknesses as well as local minima in the discriminator's prediction which would result in worse quality for this task.
Usually, these weaknesses are made up for during adversarial training as if the generator over fits to such weaknesses the discriminator will quickly penalize such a solution.
In the case of the optimization routine which is employed for this problem, it is not possible to train the discriminator online as the limit amount of brushstrokes will allow the discriminator to over-fit the problem easily.
Thus only a pre-trained discriminator with its said weaknesses can be used in this case.

\paragraph{Handpicked losses} As the data distribution for the generated data set is actually known in this case (see \ref{sssec:creation}), it is also possible to manually define losses that confine the brushstrokes.
The width constraint -- as a first example -- can easily be enforced by penalizing whenever the brushstroke's width $w(x)$ is more than half the length $l(x)$ between the start point $\vec{s}(x)$ and the end point $\vec{e}(x)$ of the brushstroke $x$:
\begin{align}
    L_{\text{bs}} & = \frac{1}{\vert X \vert}\sum_{x \in X} \max(0, 2w(x) - l(x)) \\
    & = \frac{1}{\vert X \vert}\sum_{x \in X} \max(0, 2w(x) - \norm{\vec{s(x)} - \vec{e(x)}}_2)
\end{align}

This is a bit more complicated regarding the limitation that is introduced to the control point $\vec{c}(x)$.
As $\vec{c}(x)$ was sampled from a multivariate gaussian with fixed parameters in the data set, it should now follow a similar distribution in relation to the direction $\vec{s}(x) - \vec{e}(x)$ of each stroke.

This can be achieved by first defining two orthonormal basis vectors which are either parallel $\vec{n}_{se}^\parallel(x)$ or orthogonal $\vec{n}_{se}^\bot(x)$ to the directional vector $\vec{s}(x) - \vec{e}(x)$:
\begin{align}
    \vec{n}_{se}^\parallel(x) & = \frac{\vec{s}(x) - \vec{e}(x)}{\norm{\vec{s}(x) - \vec{e}(x)}_2} \\
    \vec{n}_{se}^\bot(x) & = R_{\pi/2} \frac{\vec{s}(x) - \vec{e}(x)}{\norm{\vec{s}(x) - \vec{e}(x)}_2} \\
    \text{with } R_{\pi/2} & =
    \begin{pmatrix}
        \cos \pi/2 & -\sin \pi/2 \\
        \sin \pi/2 & \cos \pi/2
    \end{pmatrix}
\end{align}

Then $\vec{c}(x)$ can be projected into the coordinate system spanned by  $\vec{n}_{se}^\parallel(x)$ and $\vec{n}_{se}^\bot(x)$:
\begin{align}
    c^\parallel(x) & = (\vec{c}(x) - \vec{a}(x)) \cdot \vec{n}_{se}^\parallel(x)  \\
    c^\bot(x) & = (\vec{c}(x) - \vec{a}(x)) \cdot \vec{n}_{se}^\bot(x)  \\
    \vec{a}(x) & = \frac{\vec{s}(x) + \vec{e}(x)}/2
\end{align}

Now, the axes of the original multivariate distribution co-align with $\vec{c}^\parallel(x)$ and $\vec{c}^\bot(x)$.
By calculating the mean and standard deviation along these projections, they can be compared to the parameters of the original data distribution.
\begin{align}
    \vec{\mu} & = \frac{1}{\vert X \vert}\sum_{x \in X} \begin{pmatrix} c^\parallel(x) \\ c^\bot(x) \end{pmatrix} \\
    \mat{\Sigma} & = (\frac{1}{\vert X \vert} \sum_{x \in X}
        (\begin{pmatrix} c^\parallel(x) \\ c^\bot(x) \end{pmatrix} - \vec{\mu})
        (\begin{pmatrix} c^\parallel(x) \\ c^\bot(x) \end{pmatrix} - \vec{\mu})^T
        )^{\frac{1}{2}} \\
\end{align}

Using the Kullback-Leibler divergence for multivariate normal distributions, the compliance with the data sets distribution can be checked:

\begin{align}
    \L_{\text{KL}} = \frac{1}{2}\left[\log\frac{|\Sigma|}{|\tilde{\Sigma}|} - d + \text{tr} ( \Sigma^{-1}\tilde{\Sigma} ) + (\mu - \tilde{\mu})^T \Sigma^{-1}(\mu - \tilde{\mu})\right]
\end{align}

with 
\begin{align}
    \tilde{\mu} & = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \\
    \tilde{\Sigma} & = \begin{pmatrix} \frac{1}{200} & 0 \\ 0 & \frac{1}{25} \end{pmatrix} \text{ see \eqref{eq:Sigma}}
\end{align}

since the origin of the projection is at $\vec{a}(x)$, thus in the middle between $\vec{s}(x)$ and $\vec{e}(x)$ and coincides with the center of the data distribution.

In theory, $L_{\text{KL}}$ and $L_{\text{bs}}$ should be able to capture any deviation from the source data and ensure that parameters stay withing the training space of the renderer.
One problem that obviously could arise with this formulation is for patches with very similar brushstrokes that show a mean other than $\tilde{\mu}$ as well as  a very low values inside the covariance matrix and non-zero off-diagonal values.
Thus is will be favorable to include as many strokes as possible when calculating this loss, ideally all strokes in the global parameter catalogue.

\subsection{Style Transfer}
\labsubsec{opt:styletransfer}

One question that that arises when talking about the optimization procedure, is whether there could be style transfer performed with this approach as well.
Especially, since the optimization procedure has explicitly been compared to the approach by \citeauthor*{Gatys}~\cite{Gatys}, it is just a short way of thought to assume that such an approach could also be applicable in this case.

This mainly requires to introduce a style loss as it has been done by \citeauthor*{Gatys}~\cite{Gatys}, since a content loss is already in place (see~\eqref{eq:percep}).
A style loss can then be implemented similarly by aggregating the activations of more layers and then calculating the gram matrices:
\begin{align}
    \L_{\text{style}} & = \sum_{l=0}^L \frac{w_l}{4 N^2_l M^2_l} \sum_{ij} (G_{ij}^l - A_{ij}^l)^2
    G_{ij}^l & = \sum_k F_{ik}^l F_{jk}^l
\end{align}

The problem, which arises when trying to apply style transfer with this particular approach is the partial update routine.
Since the gram-martices are meant to catch global second order statistics of the image, a small patch would be object to a wrongful assumption that the patch represents the whole image.
The only chance of dealing with a local patch in a global context in image space, is a cached version of the current whole image in which the patch is embedded.

\subsection{Optimization Details}
\labsubsec{opt:details}

As it has been explained in \refsec{opt:partialupdates}, it is not possible to optimize brushstrokes for the whole image in parallel.
This is partly due to the memory requirements of the neural renderer which scales with the number of rendered patches.
And partly due to the memory requirements of the placing and blending method, which scales with the number of rendered brushstrokes as well as the number of surrounding fixed brushstrokes and the patch window size.
Experiments have shown that a combination of 256 rendered brushstrokes with 128 surrounding brushstrokes paired with a patch window size of 320x320 pixels occupies around $9.5 \si{\gibi \byte}$ of memory which leaves enough space for more advanced losses and tweaks to the network architecture.

The learning rate for the optimization procedure can be significantly larger than for training the neural renderer.
With a learning rate of $0.01$ the optimization procedure will converge significantly faster to a solution without any instability issues.

One choice, which has to be made individually per target image is how many brushstrokes will cover the image.
As larger images obviously require more brushstrokes than smaller images, what should remain the same is the \textbf{brushstrokes density}.
The brushstroke density will decide how many pixel on average should be covered by each brushstroke and thus be used during initialization.
A number of $100 \frac{\text{pixels}}{\text{brushstroke}}$ has produced the best results during the experiments.

Another important choice that goes along the choice of how dense the brushstrokes should be distributed, is that of how many optimization steps each brushstroke will be object to.
As for too few steps the training will not have converged and for too many optimization will take an unnecessary amount of time.
As this can vary between images, since some images require more time to converge, about $1500 \frac{\text{steps}}{\text{brushstroke}}$ give constantly good results.
There is no direct enforcement that each brushstrokes is updated exactly this often but it can be expected that due to the uniform sampling of render patches, there will be no major deviation for some brushstrokes.
Since 256 brushstrokes will be optimized in every step, the total number of optimization steps can be calculated together with the brushstroke density and the image's size.

Another minor detail is the fact that for each render patch five consecutive optimization steps are performed as this safes memory bandwidth, since data must be written and read from memory each time a different render patch is optimized.

\subsection{Results}
\labsubsec{results}

Results can be seen in \reffig{opt:results:starry_night} for Starry Night as the standard reference image of this thesis.
It took approximately 15,000 optimization steps which corresponds to about $2\si{\hour}$.
The rendering consists of roughly 10,000 brushstrokes.

\reffig{opt:results:photo} shows the result for natural photo as target image which took
also $~2\si{\hour}$ to compute with 8.000 brushstrokes and 12.000 optimization steps.
