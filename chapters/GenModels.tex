\setchapterpreamble[u]{\margintoc}
\chapter{Generative Models}
\labch{GenModels}
\todo{think about including htis into previous chapter as well as optimization -> save some space?}
After the previous two chapters focused the basic ideas and mathematics behind neural networks, this chapter will introduce basic architectures and building blocks of many recent neural networks.
Not every new publication reinvents the wheel
most works nowadays rely on established architectures and losses and only change parts of these
introduce some common language and concepts in CV

\section{Discriminators \& Classifiers}
Discriminators and Classifiers are some of the most basic structures
Discriminators often act as binary classifier
also as critiques, how well does this fit?

classifiers tend to classify mutliple classes
one-hot encoding
softmax loss

both structures are subject to challenges such as imagenet
perform better than humans nowadays \cite{something}
one very popular structure is that of VGG with ImageNet weights
many classes and lots of natural images
other architecture is resnet with its residual blocks

\section{Generators \& Decoders}
given some low-level input, decoders/generators 
words are often used interchangebly, sometimes generators generate content from noise see next section, decoders specifically decode low-level information into specific high-level representation 1-to-1 pairing exists.

very often use inverted VGG architecture
hard to train if paired data is not available

\section{Autoencoders}
Autoencoders are combiantion of encoders which are very simialr to classifiers and decoders in row
main goal is reconstoruction 
other goal is feature space which should have some nice properties
E.G change somehting in feature space and look at reconstruction what happens
position in feature space should represent something in image space as well
have some specific losses for this -> metric learning(jsut reference it)

\section{Generative Adversarial Networks}
GANs \citeauthor{GAN} \cite{GAN} are a combination of generator and discriminator
\marginnote{SChmidhuber claims its his idea, cite him to be on the safe side}
generate sample from noise
discrimiantor judges sample (does it fit into the distribuion that the discrimiantor has been shown so far/expects)
equillibrium problem

got the Turing award and is one of THE major gamechangers in the last decade
many applications such as AIs driving or alpha Go more or less are based on this principle

\subsection{RS- \& RA-GAN}
is used in this paper thus explain it


\subsection{Flavors of GANS}
condition genration on some input liek decoder cGAN
improve loss funciton WGAN
combine with autoencoder AEGAN
MSGGAN and ProGAN
jsut some ideas...


