@article{undefined, 
author = {Lin, Xudong and Duan, Yueqi and Dong, Qiyuan and Lu, Jiwen and Zhou, Jie}, 
title = {{Deep Variational Metric Learning}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_Xudong_Lin_Deep_Variational_Metric_ECCV_2018_paper.pdf_Unknown_Unknown.pdf}
}
@phdthesis{grad, 
title = {{Deep Learning for Information Extraction in the Biomedical Domain}}, 
author = {Suárez-Paniagua, Víctor}, 
year = {2019}
}
@book{NN_book, 
author = {Geron, A.}, 
title = {{Neural Networks and Deep Learning}}, 
url = {https://books.google.de/books?id=5pm6tQEACAAJ}, 
publisher = {O'Reilly}, 
year = {2018}
}
@misc{turingaward, 
author = {}, 
title = {{Fathers of the Deep Learning Revolution Receive ACM A.M. Turing Award}}
}
@article{resnet, 
author = {He, Kaiming and Yahng, Xiangyu and Ren, Shaoqing and Sun, Jian}, 
title = {{Deep Residual Learning for Image Recognition}}, 
url = {http://arxiv.org/abs/1512.03385}, 
volume = {abs/1512.03385}, 
journal = {CoRR}, 
year = {2015}
}
@article{alphago, 
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Driessche, George van den and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis}, 
title = {{Mastering the game of Go with deep neural networks and tree search}}, 
issn = {0028-0836}, 
doi = {10.1038/nature16961}, 
pmid = {26819042}, 
abstract = {{The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away. A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence. The victory in 1997 of the chess-playing computer Deep Blue in a six-game series against the then world champion Gary Kasparov was seen as a significant milestone in the development of artificial intelligence. An even greater challenge remained — the ancient game of Go. Despite decades of refinement, until recently the strongest computers were still playing Go at the level of human amateurs. Enter AlphaGo. Developed by Google DeepMind, this program uses deep neural networks to mimic expert players, and further improves its performance by learning from games played against itself. AlphaGo has achieved a 99\% win rate against the strongest other Go programs, and defeated the reigning European champion Fan Hui 5–0 in a tournament match. This is the first time that a computer program has defeated a human professional player in even games, on a full, 19 x 19 board, in even games with no handicap.}}, 
pages = {484--489}, 
number = {7587}, 
volume = {529}, 
journal = {Nature}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2016_Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search_Silver_Hassabis.pdf}, 
year = {2016}
}
@book{deeplearning, 
author = {Heaton, Jeff}, 
title = {{Ian Goodfellow, Yoshua Bengio, and Aaron Courville: Deep learning: The MIT Press, 2016, 800 pp, ISBN: 0262035618}}, 
volume = {19}, 
series = {Genetic Programming and Evolvable Machines}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2017_Ian%20Goodfellow,%20Yoshua%20Bengio,%20and%20Aaron%20Courville-%20Deep%20learning-%20The%20MIT%20Press,%202016,%20800%20pp,%20ISBN-%200262035618_Heaton_Heaton.pdf}, 
year = {2017}
}
@article{LPIPS, 
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver}, 
title = {{The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}}, 
eprint = {1801.03924}, 
abstract = {{While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.}}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2018_The%20Unreasonable%20Effectiveness%20of%20Deep%20Features%20as%20a%20Perceptual%20Metric_Zhang_Wang.pdf}, 
year = {2018}
}
@article{lendu, 
author = {Du, Len}, 
title = {{How Much Deep Learning does Neural Style Transfer Really Need? An Ablation Study}}, 
doi = {10.1109/wacv45572.2020.9093537}, 
abstract = {{Neural style transfer has been a "killer app" for deep learning, drawing attention from and advertising the effectiveness to both the academic and the general public. However, we have found by ablative experiments that optimizing an image in the way neural style transfer does, while the objective functions (or more precisely, the functions to transform raw images to corresponding feature maps being compared) are constructed without pretrained weights or biases, worked almost as well. We can even factor out the deepness (multiple layers of alternating linear and nonlinear transformations) alltogether and have neural style transfer working to a certain extent. This raises the question how much of the the current success of deep learning in computer vision should be attributed to training, structure or simply spatially aggregating the image.}}, 
pages = {3139--3148}, 
volume = {00}, 
journal = {2020 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_Du_How_Much_Deep_Learning_does_Neural_Style_Transfer_Really_Need_WACV_2020_paper.pdf_Unknown_Unknown.pdf}, 
year = {2020}
}
@article{10.1007/s11263-020-01303-4, 
author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor}, 
title = {{Deep Image Prior}}, 
doi = {10.1007/s11263-020-01303-4}, 
eprint = {1711.10925}, 
abstract = {{Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, super-resolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs. Apart from its diverse applications, our approach highlights the inductive bias captured by standard generator network architectures. It also bridges the gap between two very popular families of image restoration methods: learning-based methods using deep convolutional networks and learning-free methods based on handcrafted image priors such as self-similarity. Code and supplementary material are available at https://dmitryulyanov.github.io/deep\_image\_prior .}}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1711.10925.pdf_Unknown_Unknown.pdf}, 
year = {2017}
}
@article{undefined, 
author = {He, Kun and Wang, Yan and Hopcroft, John}, 
title = {{A Powerful Generative Model Using Random Weights for the Deep Image Representation}}, 
eprint = {1606.04801}, 
abstract = {{To what extent is the success of deep visualization due to the training? Could we do deep visualization using untrained, random weight networks? To address this issue, we explore new and powerful generative models for three popular deep visualization tasks using untrained, random weight convolutional neural networks. First we invert representations in feature spaces and reconstruct images from white noise inputs. The reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture. Next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network. Third, by recasting the content of an image in the style of various artworks, we create artistic images with high perceptual quality, highly competitive to the prior work of Gatys et al. on pretrained networks. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization.}}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_6568-a-powerful-generative-model-using-random-weights-for-the-deep-image-representation.pdf_Unknown_Unknown.pdf}, 
year = {2016}
}
@article{undefined, 
author = {Monti, Federico and Boscaini, Davide and Masci, Jonathan and Rodolà, Emanuele and Svoboda, Jan and Bronstein, Michael M}, 
title = {{Geometric deep learning on graphs and manifolds using mixture model CNNs}}, 
eprint = {1611.08402}, 
abstract = {{Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph- and 3D shape analysis and show that it consistently outperforms previous approaches.}}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1611.08402.pdf_Unknown_Unknown.pdf}, 
year = {2016}
}
@article{10.1109/msp.2017.2693418, 
author = {Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre}, 
title = {{Geometric deep learning: going beyond Euclidean data}}, 
doi = {10.1109/msp.2017.2693418}, 
eprint = {1611.08097}, 
abstract = {{Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.}}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1611.08097.pdf_Unknown_Unknown.pdf}, 
year = {2016}
}
@article{undefined, 
author = {Salimans, Tim and Kingma, Diederik P}, 
title = {{Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks}}, 
eprint = {1602.07868}, 
abstract = {{We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1602.07868.pdf_Unknown_Unknown.pdf}, 
year = {2016}
}
@article{undefined, 
author = {Ishfaq, Haque and Liu, Ruishan}, 
title = {{TVAE: Deep Metric Learning Approach for Variational Autoencoder}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_108.pdf_Unknown_Unknown.pdf}, 
rating = {4}
}
@article{undefined, 
author = {Hjelm, R Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua}, 
title = {{Learning deep representations by mutual information estimation and maximization}}, 
eprint = {1808.06670}, 
abstract = {{In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1808.06670.pdf_Unknown_Unknown.pdf}, 
year = {2018}
}
@article{undefined, 
author = {Sohn, Kihyuk and Yan, Xinchen and Lee, Honglak}, 
title = {{Learning Structured Output Representation
using Deep Conditional Generative Models}}, 
keywords = {cVAE}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_5775-learning-structured-output-representation-using-deep-conditional-generative-models.pdf_Unknown_Unknown.pdf}, 
year = {2015}
}
@article{undefined, 
author = {Siddharth, N and Paige, Brooks and Meent, Jan-Willem van de and Desmaison, Alban and Goodman, Noah D and Kohli, Pushmeet and Wood, Frank and Torr, Philip H S}, 
title = {{Learning Disentangled Representations with Semi-Supervised Deep Generative Models}}, 
eprint = {1706.00400}, 
abstract = {{Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_7174-learning-disentangled-representations-with-semi-supervised-deep-generative-models.pdf_Unknown_Unknown.pdf}, 
year = {2017}
}
@article{undefined, 
author = {Opitz, Michael and Waltner, Georg and Possegger, Horst and Bischof, Horst}, 
title = {{Deep Metric Learning with BIER: Boosting Independent Embeddings Robustly}}, 
eprint = {1801.04815}, 
abstract = {{Learning similarity functions between image pairs with deep neural networks yields highly correlated activations of embeddings. In this work, we show how to improve the robustness of such embeddings by exploiting the independence within ensembles. To this end, we divide the last embedding layer of a deep network into an embedding ensemble and formulate training this ensemble as an online gradient boosting problem. Each learner receives a reweighted training sample from the previous learners. Further, we propose two loss functions which increase the diversity in our ensemble. These loss functions can be applied either for weight initialization or during training. Together, our contributions leverage large embedding sizes more effectively by significantly reducing correlation of the embedding and consequently increase retrieval accuracy of the embedding. Our method works with any differentiable loss function and does not introduce any additional parameters during test time. We evaluate our metric learning method on image retrieval tasks and show that it improves over state-of-the-art methods on the CUB 200-2011, Cars-196, Stanford Online Products, In-Shop Clothes Retrieval and VehicleID datasets.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1801.04815.pdf_Unknown_Unknown.pdf}, 
year = {2018}
}
@article{10.1007/s00371-019-01726-2, 
author = {Zhao, Hui-Huang and Rosin, Paul L. and Lai, Yu-Kun and Wang, Yao-Nan}, 
title = {{Automatic semantic style transfer using deep convolutional neural networks and soft masks}}, 
issn = {0178-2789}, 
doi = {10.1007/s00371-019-01726-2}, 
abstract = {{This paper presents an automatic image synthesis method to transfer the style of an example image to a content image. When standard neural style transfer approaches are used, the textures and colours in different semantic regions of the style image are often applied inappropriately to the content image, ignoring its semantic layout and ruining the transfer result. In order to reduce or avoid such effects, we propose a novel method based on automatically segmenting the objects and extracting their soft semantic masks from the style and content images, in order to preserve the structure of the content image while having the style transferred. Each soft mask of the style image represents a specific part of the style image, corresponding to the soft mask of the content image with the same semantics. Both the soft masks and source images are provided as multichannel input to an augmented deep CNN framework for style transfer which incorporates a generative Markov random field model. The results on various images show that our method outperforms the most recent techniques.}}, 
pages = {1--18}, 
journal = {The Visual Computer}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2019_Automatic%20semantic%20style%20transfer%20using%20deep%20convolutional%20neural%20networks%20and%20soft%20masks_Zhao_Wang.pdf}, 
year = {2019}
}
@article{undefined, 
author = {Srivatsan, Akshay and Barron, Jonathan T and Klein, Dan and Berg-Kirkpatrick, Taylor}, 
title = {{A Deep Factorization of Style and Structure in Fonts}}, 
eprint = {1910.00748}, 
abstract = {{We propose a deep factorization model for typographic analysis that disentangles content from style. Specifically, a variational inference procedure factors each training glyph into the combination of a character-specific content embedding and a latent font-specific style variable. The underlying generative model combines these factors through an asymmetric transpose convolutional process to generate the image of the glyph itself. When trained on corpora of fonts, our model learns a manifold over font styles that can be used to analyze or reconstruct new, unseen fonts. On the task of reconstructing missing glyphs from an unknown font given only a small number of observations, our model outperforms both a strong nearest neighbors baseline and a state-of-the-art discriminative model from prior work.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1910.00748.pdf_Unknown_Unknown.pdf}, 
year = {2019}
}
@article{undefined, 
author = {Upchurch, Paul and Gardner, Jacob and Pleiss, Geoff and Pless, Robert and Snavely, Noah and Bala, Kavita and Weinberger, Kilian}, 
title = {{Deep Feature Interpolation for Image Content Changes}}, 
eprint = {1611.05507}, 
abstract = {{We propose Deep Feature Interpolation (DFI), a new data-driven baseline for automatic high-resolution image transformation. As the name suggests, it relies only on simple linear interpolation of deep convolutional features from pre-trained convnets. We show that despite its simplicity, DFI can perform high-level semantic transformations like "make older/younger", "make bespectacled", "add smile", among others, surprisingly well - sometimes even matching or outperforming the state-of-the-art. This is particularly unexpected as DFI requires no specialized network architecture or even any deep network to be trained for these tasks. DFI therefore can be used as a new baseline to evaluate more complex algorithms and provides a practical answer to the question of which image transformation tasks are still challenging in the rise of deep learning.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1611.05507.pdf_Unknown_Unknown.pdf}, 
year = {2016}
}
@article{undefined, 
author = {Li, Mu and Zuo, Wangmeng and Zhang, David}, 
title = {{Deep Identity-aware Transfer of Facial Attributes}}, 
eprint = {1610.05586}, 
abstract = {{This paper presents a Deep convolutional network model for Identity-Aware Transfer (DIAT) of facial attributes. Given the source input image and the reference attribute, DIAT aims to generate a facial image that owns the reference attribute as well as keeps the same or similar identity to the input image. In general, our model consists of a mask network and an attribute transform network which work in synergy to generate a photo-realistic facial image with the reference attribute. Considering that the reference attribute may be only related to some parts of the image, the mask network is introduced to avoid the incorrect editing on attribute irrelevant region. Then the estimated mask is adopted to combine the input and transformed image for producing the transfer result. For joint training of transform network and mask network, we incorporate the adversarial attribute loss, identity-aware adaptive perceptual loss, and VGG-FACE based identity loss. Furthermore, a denoising network is presented to serve for perceptual regularization to suppress the artifacts in transfer result, while an attribute ratio regularization is introduced to constrain the size of attribute relevant region. Our DIAT can provide a unified solution for several representative facial attribute transfer tasks, e.g., expression transfer, accessory removal, age progression, and gender transfer, and can be extended for other face enhancement tasks such as face hallucination. The experimental results validate the effectiveness of the proposed method. Even for the identity-related attribute (e.g., gender), our DIAT can obtain visually impressive results by changing the attribute while retaining most identity-aware features.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1610.05586.pdf_Unknown_Unknown.pdf}, 
year = {2016}
}
@article{10.1109/access.2019.2941272, 
author = {Tao, Rentuo and Li, Ziqiang and Tao, Renshuai and Li, Bin}, 
title = {{ResAttr-GAN: Unpaired Deep Residual Attributes Learning for Multi-Domain Face Image Translation}}, 
doi = {10.1109/access.2019.2941272}, 
abstract = {{Facial attributes edit can be seen as an image-to-image translation problem, whose goal is to transfer images from the source domain to the target domain. Specially, facial attributes edit aims at changing some semantic attributes of a given face image while keeping the contents of unrelated area unchanged. The great challenge for this problem lies on the lacking of paired data, i.e. we do not have paired face images that only differ on particular attributes. Moreover, to train a good attributes editing model, there always needs a great amount of train data which labeled by hand. If the train data amount was reduced, then the editing performance would decrease accordingly. Strong intelligent systems should be able to learn knowledge from less data samples (similar idea with few-shot learning). To mitigate this limitation, in this paper, we proposed a Siamese-Network based residual attributes learning model to learn the attributes difference in the high-level latent space. Compared to existing models that perform attributes editing based on an attributes classifier, the proposed deep residual attributes learning model utilized relatively weaker information of attribute differences for face image translation. Sufficient qualitative and quantitative experiments conducted on CelebA dataset proved the effectiveness of our proposed method, moreover, we also adopt the proposed residual attributes learning model in two state-of-the-art models under different data usage percentage to show the effectiveness of the proposed model on boosting attribute editing performance under limited data usage. The experiment results proved that the proposed method can improve data utilization efficiency and thus can boost the editing performance when the train data was limited.}}, 
pages = {132594--132608}, 
volume = {7}, 
journal = {IEEE Access}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_08836502.pdf_Unknown_Unknown.pdf}, 
year = {2019}
}
@article{learning2paint, 
author = {Huang, Zhewei and Heng, Wen and Zhou, Shuchang}, 
title = {{Learning to Paint With Model-based Deep Reinforcement Learning}}, 
eprint = {1903.04411}, 
abstract = {{We show how to teach machines to paint like human painters, who can use a small number of strokes to create fantastic paintings. By employing a neural renderer in model-based Deep Reinforcement Learning (DRL), our agents learn to determine the position and color of each stroke and make long-term plans to decompose texture-rich images into strokes. Experiments demonstrate that excellent visual effects can be achieved using hundreds of strokes. The training process does not require the experience of human painters or stroke tracking data. The code is available at https://github.com/hzwer/ICCV2019-LearningToPaint.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1903.04411.pdf_Unknown_Unknown.pdf}, 
year = {2019}, 
rating = {3}
}
@article{undefined, 
author = {Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich, Andrew}, 
title = {{GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks}}, 
eprint = {1711.02257}, 
abstract = {{Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter \$\textbackslashalpha\$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1711.02257.pdf_Unknown_Unknown.pdf}, 
year = {2017}
}
@article{undefined, 
author = {Devarakonda, Aditya and Naumov, Maxim and Garland, Michael}, 
title = {{AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks}}, 
eprint = {1712.02029}, 
abstract = {{Training deep neural networks with Stochastic Gradient Descent, or its variants, requires careful choice of both learning rate and batch size. While smaller batch sizes generally converge in fewer training epochs, larger batch sizes offer more parallelism and hence better computational efficiency. We have developed a new training approach that, rather than statically choosing a single batch size for all epochs, adaptively increases the batch size during the training process. Our method delivers the convergence rate of small batch sizes while achieving performance similar to large batch sizes. We analyse our approach using the standard AlexNet, ResNet, and VGG networks operating on the popular CIFAR-10, CIFAR-100, and ImageNet datasets. Our results demonstrate that learning with adaptive batch sizes can improve performance by factors of up to 6.25 on 4 NVIDIA Tesla P100 GPUs while changing accuracy by less than 1\% relative to training with fixed batch sizes.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1712.02029.pdf_Unknown_Unknown.pdf}, 
year = {2017}
}
@article{undefined, 
author = {Hou, Xianxu and Shen, Linlin and Sun, Ke and Qiu, Guoping}, 
title = {{Deep Feature Consistent Variational Autoencoder}}, 
eprint = {1610.00291}, 
abstract = {{We present a novel method for constructing Variational Autoencoder (VAE). Instead of using pixel-by-pixel loss, we enforce deep feature consistency between the input and the output of a VAE, which ensures the VAE's output to preserve the spatial correlation characteristics of the input, thus leading the output to have a more natural visual appearance and better perceptual quality. Based on recent deep learning works such as style transfer, we employ a pre-trained deep convolutional neural network (CNN) and use its hidden features to define a feature perceptual loss for VAE training. Evaluated on the CelebA face dataset, we show that our model produces better results than other methods in the literature. We also show that our method can produce latent vectors that can capture the semantic information of face expressions and can be used to achieve state-of-the-art performance in facial attribute prediction.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2016_Deep%20Feature%20Consistent%20Variational%20Autoencoder_Hou_Qiu.pdf}, 
year = {2016}
}
@article{undefined, 
author = {}, 
title = {{Efficient Deep Representation Learning by Adaptive Latent Space Sampling.pdf}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_Efficient%20Deep%20Representation%20Learning%20by%20Adaptive%20Latent%20Space%20Sampling.pdf_Unknown_Unknown.pdf}
}
@article{undefined, 
author = {Dean, Jeffrey and Corrado, Greg S. and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V. and Mao, Mark Z. and Ranzato, Marc’Aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Ng, Andrew Y.}, 
title = {{Large Scale Distributed Deep Networks}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_Dean%20et%20al.%20-%202012%20-%20Large%20Scale%20Distributed%20Deep%20Networks.pdf_Unknown_Unknown.pdf}
}
@article{undefined, 
author = {Glorot, Xavier and Bengio, Yoshua}, 
title = {{Understanding the difficulty of training deep feedforward neural networks}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_Understanding%20the%20difficulty%20of%20training%20deep%20feedforward%20neural%20networks_Glorot_Bengio.pdf}
}
@article{undefined, 
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, 
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}}, 
eprint = {1502.01852}, 
abstract = {{Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2015_Delving%20Deep%20into%20RectifiersUnknown%20Surpassing%20Human-Level%20Performance%20on%20ImageNet%20Classification_He_Sun.pdf}, 
year = {2015}
}
@article{undefined, 
author = {Mahendran, Aravindh and Vedaldi, Andrea}, 
title = {{Understanding Deep Image Representations by Inverting Them}}, 
eprint = {1412.0035}, 
abstract = {{Image representations, from SIFT and Bag of Visual Words to Convolutional Neural Networks (CNNs), are a crucial component of almost any image understanding system. Nevertheless, our understanding of them remains limited. In this paper we conduct a direct analysis of the visual information contained in representations by asking the following question: given an encoding of an image, to which extent is it possible to reconstruct the image itself? To answer this question we contribute a general framework to invert representations. We show that this method can invert representations such as HOG and SIFT more accurately than recent alternatives while being applicable to CNNs too. We then use this technique to study the inverse of recent state-of-the-art CNN image representations for the first time. Among our findings, we show that several layers in CNNs retain photographically accurate information about the image, with different degrees of geometric and photometric invariance.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2014_Understanding%20Deep%20Image%20Representations%20by%20Inverting%20Them_Mahendran_Vedaldi.pdf}, 
year = {2014}
}
@article{undefined, 
author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio}, 
title = {{Learning Deep Features for Discriminative Localization}}, 
eprint = {1512.04150}, 
abstract = {{In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2\% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2015_Learning%20Deep%20Features%20for%20Discriminative%20Localization_Zhou_Torralba.pdf}, 
year = {2015}
}
@article{undefined, 
author = {Martinez, Manuel and Tapaswi, Makarand and {Stiefelhagen, and Rainer}}, 
title = {{A Closed-form Gradient for the 1D Earth
Mover’s Distance for Spectral Deep Learning
on Biological Data}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_A%20Closed-form%20Gradient%20for%20the%201D%20EarthUnknownMover’s%20Distance%20for%20Spectral%20Deep%20LearningUnknownon%20Biological%20Data_Unknown_Unknown.pdf}, 
year = {2016}
}
@article{10.1109/tpami.2015.2439281, 
author = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou}, 
title = {{Image Super-Resolution Using Deep Convolutional Networks}}, 
issn = {0162-8828}, 
doi = {10.1109/tpami.2015.2439281}, 
pmid = {26761735}, 
abstract = {{We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.}}, 
pages = {295--307}, 
number = {2}, 
volume = {38}, 
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2015_Image%20Super-Resolution%20Using%20Deep%20Convolutional%20Networks_Dong_Tang.pdf}, 
year = {2015}
}
@article{10.1109/cvpr.2016.182, 
author = {Kim, Jiwon and Lee, Jung Kwon and Lee, Kyoung Mu}, 
title = {{Accurate Image Super-Resolution Using Very Deep Convolutional Networks}}, 
doi = {10.1109/cvpr.2016.182}, 
abstract = {{We present a highly accurate single-image super-resolution (SR) method. Our method uses a very deep convolutional network inspired by VGG-net used for ImageNet classification [19]. We find increasing our network depth shows a significant improvement in accuracy. Our final model uses 20 weight layers. By cascading small filters many times in a deep network structure, contextual information over large image regions is exploited in an efficient way. With very deep networks, however, convergence speed becomes a critical issue during training. We propose a simple yet effective training procedure. We learn residuals only and use extremely high learning rates (104 times higher than SRCNN [6]) enabled by adjustable gradient clipping. Our proposed method performs better than existing methods in accuracy and visual improvements in our results are easily noticeable.}}, 
pages = {1646--1654}, 
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2016_Accurate%20Image%20Super-Resolution%20Using%20Very%20Deep%20Convolutional%20Networks_Kim_Lee.pdf}, 
year = {2016}
}
@article{10.1109/cvpr.2016.181, 
author = {Kim, Jiwon and Lee, Jung Kwon and Lee, Kyoung Mu}, 
title = {{Deeply-Recursive Convolutional Network for Image Super-Resolution}}, 
doi = {10.1109/cvpr.2016.181}, 
abstract = {{We propose an image super-resolution method (SR) using a deeply-recursive convolutional network (DRCN). Our network has a very deep recursive layer (up to 16 recursions). Increasing recursion depth can improve performance without introducing new parameters for additional convolutions. Albeit advantages, learning a DRCN is very hard with a standard gradient descent method due to exploding/vanishing gradients. To ease the difficulty of training, we propose two extensions: recursive-supervision and skip-connection. Our method outperforms previous methods by a large margin.}}, 
pages = {1637--1645}, 
journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2016_Deeply-Recursive%20Convolutional%20Network%20for%20Image%20Super-Resolution_Kim_Lee.pdf}, 
year = {2016}
}
@article{undefined, 
author = {Kingma, Diederik P and Rezende, Danilo J and Mohamed, Shakir and Welling, Max}, 
title = {{Semi-Supervised Learning with Deep Generative Models}}, 
eprint = {1406.5298}, 
abstract = {{The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.}}, 
year = {2014}
}
@article{undefined, 
author = {Timo and Karsten and Biagio}, 
title = {{Sharing Matters: Improving Deep Metric Learning
using Shared Features}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_Sharing%20MattersUnknown%20Improving%20Deep%20Metric%20LearningUnknownusing%20Shared%20Features_Unknown_Unknown.pdf}
}