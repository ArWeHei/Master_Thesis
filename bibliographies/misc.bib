@article{worldmodel, 
author = {Ha, David and Schmidhuber, Jürgen}, 
title = {{World Models}}, 
doi = {10.5281/zenodo.1207631}, 
eprint = {1803.10122}, 
abstract = {{We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/}}, 
journal = {arXiv}, 
year = {2018}
}
@misc{krita, 
author = {KDE}, 
title = {{Krita}}, 
url = {https://krita.org/}
}
@misc{artrage, 
author = {Ltd, Ambient Design}, 
title = {{Artrage}}, 
url = {http://www.artrage.com/}
}
@misc{zolee, 
author = {zolee}, 
title = {{zolee | onlyGFX}}, 
url = {https://www.onlygfx.com/author/zolee/}, 
urldate = {2020-6-29}
}
@article{genetic_algo, 
author = {Eiben, A E and Smith, J E}, 
title = {{Natural Computing Series}}, 
issn = {1619-7127}, 
doi = {10.1007/978-3-662-44874-8}, 
year = {2015}
}
@phdthesis{grad, 
title = {{Deep Learning for Information Extraction in the Biomedical Domain}}, 
author = {Suárez-Paniagua, Víctor}, 
year = {2019}
}
@article{lomo, 
author = {Bliss, Tim V.P. and Collingridge, Graham L. and Morris, Richard G.M. and Reymann, Klaus G.}, 
title = {{Long-term potentiation in the hippocampus: discovery, mechanisms and function}}, 
url = {https://www.degruyter.com/view/journals/nf/24/3/article-pA103.xml}, 
pages = {A103 -- A120}, 
number = {3}, 
volume = {24}, 
journal = {Neuroforum}, 
year = {2018}
}
@book{NN_book, 
author = {Geron, A.}, 
title = {{Neural Networks and Deep Learning}}, 
url = {https://books.google.de/books?id=5pm6tQEACAAJ}, 
publisher = {O'Reilly}, 
year = {2018}
}
@misc{VG_museum, 
author = {}, 
title = {{Van Gogh Museum}}, 
url = {https://www.vangoghmuseum.nl/en}, 
urldate = {2020-7-1}
}
@article{inception, 
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew}, 
title = {{Going Deeper with Convolutions}}, 
eprint = {1409.4842}, 
abstract = {{We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.}}, 
journal = {arXiv}, 
year = {2014}
}
@article{coordconv, 
author = {Liu, Rosanne and Lehman, Joel and Molino, Piero and Such, Felipe Petroski and Frank, Eric and Sergeev, Alex and Yosinski, Jason}, 
title = {{An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution}}, 
eprint = {1807.03247}, 
abstract = {{Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x,y) Cartesian space and one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either complete translation invariance or varying degrees of translation dependence, as required by the end task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10--100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST showed 24\% better IOU when using CoordConv, and in the RL domain agents playing Atari games benefit significantly from the use of CoordConv layers.}}, 
journal = {arXiv}, 
year = {2018}
}
@article{superpixel_segmentation, 
author = {Ren and Malik}, 
title = {{Learning a classification model for segmentation}}, 
doi = {10.1109/iccv.2003.1238308}, 
abstract = {{We propose a two-class classification model for grouping. Human segmented natural images are used as positive examples. Negative examples of grouping are constructed by randomly matching human segmentations and images. In a preprocessing stage an image is oversegmented into superpixels. We define a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation. Information-theoretic analysis is applied to evaluate the power of these grouping cues. We train a linear classifier to combine these features. To demonstrate the power of the classification model, a simple algorithm is used to randomly search for good segmentations. Results are shown on a wide range of images.}}, 
pages = {10--17 vol.1}, 
journal = {Proceedings Ninth IEEE International Conference on Computer Vision}, 
year = {2003}
}
@article{superpixels, 
author = {Stutz, David and Hermans, Alexander and Leibe, Bastian}, 
title = {{Superpixels}}, 
issn = {1077-3142}, 
doi = {10.1016/j.cviu.2017.03.007}, 
eprint = {1612.01601}, 
url = {https://doi.org/10.1016/j.cviu.2017.03.007}, 
pages = {1–27}, 
number = {C}, 
volume = {166}, 
journal = {Comput. Vis. Image Underst.}, 
keywords = {Perceptual grouping,Evaluation,Benchmark,Superpixel segmentation,Image segmentation,Superpixels}, 
year = {2018}
}
@article{guardian_article, 
author = {Dickson, Andrew}, 
title = {{The new tool in the art of spotting forgeries: artificial intelligence}}, 
url = {https://www.theguardian.com/us-news/2018/aug/06/the-new-tool-in-the-art-of-spotting-forgeries-artificial-intelligence}, 
address = {The Guardian}, 
year = {2018}
}
@article{WGAN, 
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon}, 
title = {{Wasserstein GAN}}, 
eprint = {1701.07875}, 
abstract = {{We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.}}, 
journal = {arXiv}, 
year = {2017}
}
@article{cGAN, 
author = {Mirza, Mehdi and Osindero, Simon}, 
title = {{Conditional Generative Adversarial Nets}}, 
eprint = {1411.1784}, 
abstract = {{Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.}}, 
journal = {arXiv}, 
year = {2014}
}
@article{AEGAN, 
author = {Lazarou, Conor}, 
title = {{Autoencoding Generative Adversarial Networks}}, 
eprint = {2004.05472}, 
abstract = {{In the years since Goodfellow et al. introduced Generative Adversarial Networks (GANs), there has been an explosion in the breadth and quality of generative model applications. Despite this work, GANs still have a long way to go before they see mainstream adoption, owing largely to their infamous training instability. Here I propose the Autoencoding Generative Adversarial Network (AEGAN), a four-network model which learns a bijective mapping between a specified latent space and a given sample space by applying an adversarial loss and a reconstruction loss to both the generated images and the generated latent vectors. The AEGAN technique offers several improvements to typical GAN training, including training stabilization, mode-collapse prevention, and permitting the direct interpolation between real samples. The effectiveness of the technique is illustrated using an anime face dataset.}}, 
journal = {arXiv}, 
year = {2020}
}
@inproceedings{turingaward, 
author = {}, 
title = {Fathers of the Deep Learning Revolution Receive ACM A.M. Turing Award}
}
@inproceedings{GAN, 
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua}, 
title = {{Generative adversarial nets}}, 
booktitle = {Advances in neural information processing systems}, 
pages = {2672---2680}, 
year = {2014}
}
@inproceedings{superhuman, 
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, 
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}}, 
doi = {10.1109/iccv.2015.123}, 
abstract = {{Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [33]). To our knowledge, our result is the first11reported in Feb. 2015. to surpass the reported human-level performance (5.1\%, [26]) on this dataset. reported in Feb. 2015.}}, 
pages = {1026--1034}, 
journal = {2015 IEEE International Conference on Computer Vision (ICCV)}, 
year = {2015}
}
@article{VGG, 
author = {Simonyan, Karen and Zisserman, Andrew}, 
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}}, 
eprint = {1409.1556}, 
abstract = {{In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.}}, 
journal = {arXiv}, 
year = {2014}
}
@inproceedings{imagenet, 
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li}, 
title = {{ImageNet: A large-scale hierarchical image database}}, 
issn = {1063-6919}, 
doi = {10.1109/cvpr.2009.5206848}, 
abstract = {{The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.}}, 
pages = {248--255}, 
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
year = {2009}
}
@article{uri_alon, 
author = {Alon, Uri}, 
title = {{Network motifs: theory and experimental approaches}}, 
issn = {1471-0056}, 
doi = {10.1038/nrg2102}, 
pmid = {17510665}, 
abstract = {{Transcription regulation networks control the expression of genes. The transcription networks of well-studied microorganisms appear to be made up of a small set of recurring regulation patterns, called network motifs. The same network motifs have recently been found in diverse organisms from bacteria to humans, suggesting that they serve as basic building blocks of transcription networks. Here I review network motifs and their functions, with an emphasis on experimental studies. Network motifs in other biological networks are also mentioned, including signalling and neuronal networks.}}, 
pages = {450--461}, 
number = {6}, 
volume = {8}, 
journal = {Nature Reviews Genetics}, 
year = {2007}
}
@article{IN, 
author = {Ulyanov, Dmitry and Vedaldi, Andrea}, 
title = {{Instance Normalization: The Missing Ingredient for Fast Stylization}}, 
url = {http://arxiv.org/abs/1607.08022}, 
volume = {abs/1607.08022}, 
journal = {CoRR}, 
year = {2016}
}
@article{BN, 
author = {Ioffe, Sergey and Szegedy, Christian}, 
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing
               Internal Covariate Shift}}, 
url = {http://arxiv.org/abs/1502.03167}, 
volume = {abs/1502.03167}, 
journal = {CoRR}, 
year = {2015}
}
@article{resnet, 
author = {He, Kaiming and Yahng, Xiangyu and Ren, Shaoqing and Sun, Jian}, 
title = {{Deep Residual Learning for Image Recognition}}, 
url = {http://arxiv.org/abs/1512.03385}, 
volume = {abs/1512.03385}, 
journal = {CoRR}, 
year = {2015}
}
@article{ojas_rule, 
author = {Oja, Erkki}, 
title = {{Simplified neuron model as a principal component analyzer}}, 
pages = {267---273}, 
number = {3}, 
volume = {15}, 
journal = {Journal of mathematical biology}, 
year = {1982}
}
@article{lomo, 
author = {Bliss, Tim VP and Lmo, Terje}, 
title = {{Long-lasting potentiation of synaptic transmission in the dentate area of the anaesthetized rabbit following stimulation of the perforant path}}, 
pages = {331---356}, 
number = {2}, 
volume = {232}, 
journal = {The Journal of physiology}, 
year = {1973}
}
@inproceedings{brainscales, 
author = {}, 
title = {{Understanding multiple spatial and temporal scales in brain information processing based on in-vivo experimentation, computational analysis and computational synthesis}}, 
url = {https://brainscales.kip.uni-heidelberg.de/images/3/39/Public--BrainScaleSFlyerObjectives\_2011.pdf}
}
@article{hebb, 
author = {Lowel, S and Singer, W}, 
title = {{Selection of intrinsic horizontal connections in the visual cortex by correlated neuronal activity}}, 
issn = {0036-8075}, 
doi = {10.1126/science.1372754}, 
url = {https://science.sciencemag.org/content/255/5041/209}, 
abstract = {{In the visual cortex of the brain, long-ranging tangentially oriented axon collaterals interconnect regularly spaced clusters of cells. These connections develop after birth and attain their specificity by pruning. To test whether there is selective stabilization of connections between those cells that exhibit correlated activity, kittens were raised with artificially induced strabismus (eye deviation) to eliminate the correlation between signals from the two eyes. In area 17, cell clusters were driven almost exclusively from either the right or the left eye and tangential intracortical fibers preferentially connected cell groups activated by the same eye. Thus, circuit selection depends on visual experience, and the selection criterion is the correlation of activity.}}, 
pages = {209---212}, 
number = {5041}, 
volume = {255}, 
journal = {Science}, 
year = {1992}
}
@article{alphago, 
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and Driessche, George van den and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis}, 
title = {{Mastering the game of Go with deep neural networks and tree search}}, 
issn = {0028-0836}, 
doi = {10.1038/nature16961}, 
pmid = {26819042}, 
abstract = {{The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away. A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence. The victory in 1997 of the chess-playing computer Deep Blue in a six-game series against the then world champion Gary Kasparov was seen as a significant milestone in the development of artificial intelligence. An even greater challenge remained — the ancient game of Go. Despite decades of refinement, until recently the strongest computers were still playing Go at the level of human amateurs. Enter AlphaGo. Developed by Google DeepMind, this program uses deep neural networks to mimic expert players, and further improves its performance by learning from games played against itself. AlphaGo has achieved a 99\% win rate against the strongest other Go programs, and defeated the reigning European champion Fan Hui 5–0 in a tournament match. This is the first time that a computer program has defeated a human professional player in even games, on a full, 19 x 19 board, in even games with no handicap.}}, 
pages = {484--489}, 
number = {7587}, 
volume = {529}, 
journal = {Nature}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2016_Mastering%20the%20game%20of%20Go%20with%20deep%20neural%20networks%20and%20tree%20search_Silver_Hassabis.pdf}, 
year = {2016}
}
@misc{ommer, 
author = {Ommer, Björn}, 
title = {{Deep Vision}}, 
type = {Lecture on deep computer vision at Heidelberg university}, 
year = {2020}
}
@article{perceptrons, 
author = {Minsky, Marvin and Papert, Seymour A.}, 
title = {{Perceptrons: An Introduction to Computational Geometry}}, 
isbn = {0262534770}, 
publisher = {The MIT Press}, 
year = {2017}
}
@book{coloratlas, 
author = {Silbernagl, S. and Despopoulos, A.}, 
title = {{Color Atlas of Physiology}}, 
isbn = {9783135450063}, 
url = {https://books.google.de/books?id=pEHDfzwrHJQC}, 
series = {Basic sciences}, 
publisher = {Thieme}, 
year = {2009}
}
@phdthesis{3Dscan_thesis, 
title = {{Development of a Topographic Imaging Device For The Near-Planar Surfaces Of Paintings}}, 
author = {Zaman, Tim}, 
abstract = {{Paintings are versatile near-planar objects with material
characteristics that vary widely. The
fact that paint has a material presence is often overlooked,
mostly because of the fact that
we encounter many of these artworks through two dimensional
reproductions. The capture
of paintings in the third dimension is not only interesting f
or study, restoration and con-
servation, but it also facilitates making three dimensiona
l reproductions through novel 3-D
printing methods. These varying material characteristics
of paintings are first investigated,
after which an overview is given of the feasible imaging meth
ods that can capture a painting’s
color and topography. Because no imaging method is ideally s
uited for this task, a hybrid
solution between fringe projection and stereo imaging is pr
oposed involving two cameras and
a projector. Fringe projection is aided by sparse stereo mat
ching to serve as an image encoder.
These encoded images processed by the stereo cameras then he
lp solve the correspondence
problem in stereo matching, leading to a dense and accurate t
opographical map, while simul-
taneously capturing its color. Through high-end cameras, s
pecial lenses and filters we capture
a surface area of 170 cm
2
with an in-plane effective resolution of 50 μm and a depth prec
ision
of 9
.
2 μm. Semi-automated positioning of the system and data stit
ching consequently allows
for the capture of surfaces up to 1 m
2
. The reproductive properties are conform the digitiza-
tion guidelines for cultural heritage. The preliminary res
ults of the capture of a painting by
Rembrandt reveal that this system is indeed ideally suited a
s a topographic imaging device
for the near-surface of paintings. This data can be used for m
aking a lifelike reproduction in
full dimension and full color.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Thesis_Tim_Zaman.pdf}
}
@misc{3Dscan_art, 
author = {Zaman, Tim}, 
title = {{3D Scanning Paintings}}, 
url = {http://www.timzaman.nl/3d-scanning-paintings}, 
urldate = {2020-6-30}, 
year = {2013}
}
@inproceedings{googleartproject, 
author = {Savov, Vlad}, 
title = {{Google Art Project offers gigapixel images of art classics, indoor Street View of museums}}, 
url = {https://www.engadget.com/2011-02-01-google-art-project-offers-gigapixel-images-of-art-classics-ind.html?guccounter=1}, 
urldate = {2020-6-30}, 
year = {2011}
}
@article{momentum, 
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.}, 
title = {{Learning representations by back-propagating errors}}, 
issn = {0028-0836}, 
doi = {10.1038/323533a0}, 
abstract = {{We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.}}, 
pages = {533--536}, 
number = {6088}, 
volume = {323}, 
journal = {Nature}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/1986_Learning%20representations%20by%20back-propagating%20errors_Rumelhart_Williams.pdf}, 
year = {1986}
}
@article{undefined, 
author = {}, 
title = {{l5\_JB.pdf}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/l5_JB.pdf}
}
@article{10.1007/978-3-642-17432-2_29, 
author = {Izadi, Ashkan and Ciesielski, Vic and Berry, Marsha}, 
title = {{AI 2010: Advances in Artificial Intelligence, 23rd Australasian Joint Conference, Adelaide, Australia, December 7-10, 2010. Proceedings}}, 
doi = {10.1007/978-3-642-17432-2\_29}, 
abstract = {{We have developed a method for generating non photorealistic animations of a target image. The animations start as a random collection of triangular strokes on a canvas and the target gradually emerges as the animation proceeds. We use genetic programming to evolve programs that draw the brushstrokes. A measure of similarity to the target is used as the fitness function. The best individual in a generation becomes a frame of the animation. We have experimented with open and filled triangles. Both kinds of triangles resulted in animations that our artist collaborators found engaging and interesting. In particular, the use of filled triangles generated animations that exhibited a novel immersive quality. The evolutionary approach requires artistic judgment in selecting the target images and values for the various parameters and provides a rich environment for exploring novel non photo–realistic renderings.}}, 
pages = {283--292}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2010_AI%202010-%20Advances%20in%20Artificial%20Intelligence,%2023rd%20Australasian%20Joint%20Conference,%20Adelaide,%20Australia,%20December%207-10,%202010.%20Proceedings_Izadi_Berry.pdf}, 
year = {2010}
}
@book{deeplearning, 
author = {Goodfellow, Ian and Bengio, Yoshua and {Courvill, and Aaron}}, 
title = {{Deep Learning}}, 
volume = {19}, 
series = {Genetic Programming and Evolvable Machines}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2016_Deep%20Learning_Goodfellow_Courvill.pdf}, 
year = {2016}
}
@article{LPIPS, 
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver}, 
title = {{The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}}, 
eprint = {1801.03924}, 
abstract = {{While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.}}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2018_The%20Unreasonable%20Effectiveness%20of%20Deep%20Features%20as%20a%20Perceptual%20Metric_Zhang_Wang.pdf}, 
year = {2018}
}
@misc{CV_art, 
author = {Sarin, Helena}, 
title = {{Computer Vision Art}}, 
url = {https://computervisionart.com/pieces/pretty-in-gan/}, 
urldate = {2020-6-25}, 
year = {2018}
}
@misc{fluidpaint, 
author = {Li, David}, 
title = {{Fluid Paint}}, 
url = {http://david.li/paint}, 
urldate = {2020-6-25}
}
@book{grosse, 
author = {Grosse, Roger}, 
title = {{Intro to Neural Networks}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/CS321_Grosse_Lecture_Notes.pdf}
}
@article{undefined, 
author = {Vo, Duc Minh and Sugimoto, Akihiro}, 
title = {{Two-Stream FCNs to Balance Content and Style for Style Transfer}}, 
eprint = {1911.08079}, 
abstract = {{Style transfer is to render given image contents in given styles, and it has an important role in both computer vision fundamental research and industrial applications. Following the success of deep learning based approaches, this problem has been re-launched very recently, but still remains a difficult task because of trade-off between preserving contents and faithful rendering of styles. In this paper, we propose an end-to-end two-stream Fully Convolutional Networks (FCNs) aiming at balancing the contributions of the content and the style in rendered images. Our proposed network consists of the encoder and decoder parts. The encoder part utilizes a FCN for content and a FCN for style where the two FCNs have feature injections and are independently trained to preserve the semantic content and to learn the faithful style representation in each. The semantic content feature and the style representation feature are then concatenated adaptively and fed into the decoder to generate style-transferred (stylized) images. In order to train our proposed network, we employ a loss network, the pre-trained VGG-16, to compute content loss and style loss, both of which are efficiently used for the feature injection as well as the feature concatenation. Our intensive experiments show that our proposed model generates more balanced stylized images in content and style than state-of-the-art methods. Moreover, our proposed network achieves efficiency in speed.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1911.08079.pdf_Unknown_Unknown.pdf}, 
year = {2019}
}
@article{FID, 
author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp}, 
title = {{GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium}}, 
eprint = {1706.08500}, 
abstract = {{Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\textbackslash'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1706.08500.pdf_Unknown_Unknown.pdf}, 
year = {2017}
}
@article{RGAN, 
author = {Jolicoeur-Martineau, Alexia}, 
title = {{The relativistic discriminator: a key element missing from standard GAN}}, 
eprint = {1807.00734}, 
abstract = {{In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs. We show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function. Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400\%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1807.00734.pdf_Unknown_Unknown.pdf}, 
year = {2018}
}
@article{VAE-GAN, 
author = {Larsen, Anders Boesen Lindbo and Sønderby, Søren Kaae and Larochelle, Hugo and Winther, Ole}, 
title = {{Autoencoding beyond pixels using a learned similarity metric}}, 
eprint = {1512.09300}, 
abstract = {{We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder with a generative adversarial network we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1512.09300.pdf_Unknown_Unknown.pdf}, 
year = {2015}
}
@article{MSG-GAN, 
author = {Karnewar, Animesh and Wang, Oliver and Iyengar, Raghu Sesha}, 
title = {{MSG-GAN: Multi-Scale Gradient GAN for Stable Image Synthesis}}, 
eprint = {1903.06048}, 
abstract = {{While Generative Adversarial Networks (GANs) have seen huge successes in image synthesis tasks, they are notoriously difficult to use, in part due to instability during training. One commonly accepted reason for this instability is that gradients passing from the discriminator to the generator can quickly become uninformative, due to a learning imbalance during training. In this work, we propose the Multi-Scale Gradient Generative Adversarial Network (MSG-GAN), a simple but effective technique for addressing this problem which allows the flow of gradients from the discriminator to the generator at multiple scales. This technique provides a stable approach for generating synchronized multi-scale images. We present a very intuitive implementation of the mathematical MSG-GAN framework which uses the concatenation operation in the discriminator computations. We empirically validate the effect of our MSG-GAN approach through experiments on the CIFAR10 and Oxford102 flowers datasets and compare it with other relevant techniques which perform multi-scale image synthesis. In addition, we also provide details of our experiment on CelebA-HQ dataset for synthesizing 1024 x 1024 high resolution images.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1903.06048.pdf_Unknown_Unknown.pdf}, 
year = {2019}, 
rating = {5}
}
@article{pruning,
  author    = {Pavlo Molchanov and
               Stephen Tyree and
               Tero Karras and
               Timo Aila and
               Jan Kautz},
  title     = {Pruning Convolutional Neural Networks for Resource Efficient Transfer
               Learning},
  journal   = {CoRR},
  volume    = {abs/1611.06440},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.06440},
  archivePrefix = {arXiv},
  eprint    = {1611.06440},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MolchanovTKAK16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{StyleGAN,
  author    = {Tero Karras and
               Samuli Laine and
               Timo Aila},
  title     = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1812.04948},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.04948},
  archivePrefix = {arXiv},
  eprint    = {1812.04948},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-04948.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{ProGAN,
  author    = {Tero Karras and
               Timo Aila and
               Samuli Laine and
               Jaakko Lehtinen},
  title     = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  journal   = {CoRR},
  volume    = {abs/1710.10196},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.10196},
  archivePrefix = {arXiv},
  eprint    = {1710.10196},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1710-10196.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
