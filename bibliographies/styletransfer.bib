@article{Art2Real, 
author = {Tomei, Matteo and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita}, 
title = {{Art2Real: Unfolding the Reality of Artworks via Semantically-Aware Image-to-Image Translation}}, 
eprint = {1811.10666}, 
abstract = {{The applicability of computer vision to real paintings and artworks has been rarely investigated, even though a vast heritage would greatly benefit from techniques which can understand and process data from the artistic domain. This is partially due to the small amount of annotated artistic data, which is not even comparable to that of natural images captured by cameras. In this paper, we propose a semantic-aware architecture which can translate artworks to photo-realistic visualizations, thus reducing the gap between visual features of artistic and realistic data. Our architecture can generate natural images by retrieving and learning details from real photos through a similarity matching strategy which leverages a weakly-supervised semantic understanding of the scene. Experimental results show that the proposed technique leads to increased realism and to a reduction in domain shift, which improves the performance of pre-trained architectures for classification, detection, and segmentation. Code is publicly available at: https://github.com/aimagelab/art2real.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2018_Art2Real-%20Unfolding%20the%20Reality%20of%20Artworks%20via%20Semantically-Aware%20Image-to-Image%20Translation_Tomei_Cucchiara.pdf}, 
year = {2018}
}
@article{lendu, 
author = {Du, Len}, 
title = {{How Much Deep Learning does Neural Style Transfer Really Need? An Ablation Study}}, 
doi = {10.1109/wacv45572.2020.9093537}, 
abstract = {{Neural style transfer has been a "killer app" for deep learning, drawing attention from and advertising the effectiveness to both the academic and the general public. However, we have found by ablative experiments that optimizing an image in the way neural style transfer does, while the objective functions (or more precisely, the functions to transform raw images to corresponding feature maps being compared) are constructed without pretrained weights or biases, worked almost as well. We can even factor out the deepness (multiple layers of alternating linear and nonlinear transformations) alltogether and have neural style transfer working to a certain extent. This raises the question how much of the the current success of deep learning in computer vision should be attributed to training, structure or simply spatially aggregating the image.}}, 
pages = {3139--3148}, 
volume = {00}, 
journal = {2020 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_Du_How_Much_Deep_Learning_does_Neural_Style_Transfer_Really_Need_WACV_2020_paper.pdf_Unknown_Unknown.pdf}, 
year = {2020}
}
@article{undefined, 
author = {Li, Yijun and Liu, Ming-Yu and Li, Xueting and Yang, Ming-Hsuan and Kautz, Jan}, 
title = {{A Closed-form Solution to Photorealistic Image Stylization}}, 
eprint = {1802.06474}, 
abstract = {{Photorealistic image stylization concerns transferring style of a reference photo to a content photo with the constraint that the stylized photo should remain photorealistic. While several photorealistic image stylization methods exist, they tend to generate spatially inconsistent stylizations with noticeable artifacts. In this paper, we propose a method to address these issues. The proposed method consists of a stylization step and a smoothing step. While the stylization step transfers the style of the reference photo to the content photo, the smoothing step ensures spatially consistent stylizations. Each of the steps has a closed-form solution and can be computed efficiently. We conduct extensive experimental validations. The results show that the proposed method generates photorealistic stylization outputs that are more preferred by human subjects as compared to those by the competing methods while running much faster. Source code and additional results are available at https://github.com/NVIDIA/FastPhotoStyle .}}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1802.06474.pdf_Unknown_Unknown.pdf}, 
year = {2018}
}
@article{dima, 
author = {Kotovenko, Dmytro and Sanakoyeu, Artsiom and Ma, Pingchuan and Lang, Sabine and Ommer, Bjorn}, 
title = {{A Content Transformation Block for Image Style Transfer}}, 
doi = {10.1109/cvpr.2019.01027}, 
abstract = {{Style transfer has recently received a lot of attention, since it allows to study fundamental challenges in image understanding and synthesis. Recent work has significantly improved the representation of color and texture and computational speed and image resolution. The explicit transformation of image content has, however, been mostly neglected: while artistic style affects formal characteristics of an image, such as color, shape or texture, it also deforms, adds or removes content details. This paper explicitly focuses on a content-and style-aware stylization of a content image. Therefore, we introduce a content transformation module between the encoder and decoder. Moreover, we utilize similar content appearing in photographs and style samples to learn how style alters content details and we generalize this to other class details. Additionally, this work presents a novel normalization layer critical for high resolution image synthesis. The robustness and speed of our model enables a video stylization in real-time and high definition. We perform extensive qualitative and quantitative evaluations to demonstrate the validity of our approach.}}, 
pages = {10024--10033}, 
volume = {00}, 
journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_Kotovenko_A_Content_Transformation_Block_for_Image_Style_Transfer_CVPR_2019_paper.pdf_Unknown_Unknown.pdf}, 
year = {2019}
}
@article{MMD, 
author = {Li, Yanghao and Wang, Naiyan and Liu, Jiaying and Hou, Xiaodi}, 
title = {{Demystifying Neural Style Transfer}}, 
eprint = {1701.01036}, 
abstract = {{Neural Style Transfer has recently demonstrated very exciting results which catches eyes in both academia and industry. Despite the amazing results, the principle of neural style transfer, especially why the Gram matrices could represent style remains unclear. In this paper, we propose a novel interpretation of neural style transfer by treating it as a domain adaptation problem. Specifically, we theoretically show that matching the Gram matrices of feature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with the second order polynomial kernel. Thus, we argue that the essence of neural style transfer is to match the feature distributions between the style images and the generated images. To further support our standpoint, we experiment with several other distribution alignment methods, and achieve appealing results. We believe this novel interpretation connects these two important research fields, and could enlighten future researches.}}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1701.01036.pdf_Unknown_Unknown.pdf}, 
year = {2017}
}
@article{undefined, 
author = {Lin, Jianxin and Pang, Yingxue and Xia, Yingce and Chen, Zhibo and Luo, Jiebo}, 
title = {{TuiGAN: Learning Versatile Image-to-Image Translation with Two Unpaired Images}}, 
eprint = {2004.04634}, 
abstract = {{An unsupervised image-to-image translation (UI2I) task deals with learning a mapping between two domains without paired images. While existing UI2I methods usually require numerous unpaired images from different domains for training, there are many scenarios where training data is quite limited. In this paper, we argue that even if each domain contains a single image, UI2I can still be achieved. To this end, we propose TuiGAN, a generative model that is trained on only two unpaired images and amounts to one-shot unsupervised learning. With TuiGAN, an image is translated in a coarse-to-fine manner where the generated image is gradually refined from global structures to local details. We conduct extensive experiments to verify that our versatile method can outperform strong baselines on a wide variety of UI2I tasks. Moreover, TuiGAN is capable of achieving comparable performance with the state-of-the-art UI2I models trained with sufficient data.}}, 
journal = {arXiv}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_2004.04634.pdf_Unknown_Unknown.pdf}, 
year = {2020}
}
@article{undefined, 
author = {Xiang, Sitao and Li, Hao}, 
title = {{Disentangling Style and Content in Anime Illustrations}}, 
eprint = {1905.10742}, 
abstract = {{Existing methods for AI-generated artworks still struggle with generating high-quality stylized content, where high-level semantics are preserved, or separating fine-grained styles from various artists. We propose a novel Generative Adversarial Disentanglement Network which can fully decompose complex anime illustrations into style and content. Training such model is challenging, since given a style, various content data may exist but not the other way round. In particular, we disentangle two complementary factors of variations, where one of the factors is labelled. Our approach is divided into two stages, one that encodes an input image into a style independent content, and one based on a dual-conditional generator. We demonstrate the ability to generate high-fidelity anime portraits with a fixed content and a large variety of styles from over a thousand artists, and vice versa, using a single end-to-end network and with applications in style transfer. We show this unique capability as well as superior output to the current state-of-the-art.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/Unknown_1905.10742.pdf_Unknown_Unknown.pdf}, 
year = {2019}
}
@article{WCT, 
author = {Li, Yijun and Fang, Chen and Yang, Jimei and Wang, Zhaowen and Lu, Xin and Yang, Ming-Hsuan}, 
title = {{Universal Style Transfer via Feature Transforms}}, 
eprint = {1705.08086}, 
abstract = {{Universal style transfer aims to transfer arbitrary visual styles to content images. Existing feed-forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. In this paper, we present a simple yet effective method that tackles these limitations without training on any pre-defined styles. The key ingredient of our method is a pair of feature transforms, whitening and coloring, that are embedded to an image reconstruction network. The whitening and coloring transforms reflect a direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high-quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures via simple feature coloring.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2017_Universal%20Style%20Transfer%20via%20Feature%20Transforms_Li_Yang.pdf}, 
year = {2017}
}
@article{gatys, 
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias}, 
title = {{A Neural Algorithm of Artistic Style}}, 
eprint = {1508.06576}, 
abstract = {{In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2015_A%20Neural%20Algorithm%20of%20Artistic%20Style_Gatys_Bethge.pdf}, 
year = {2015}
}
@article{johnson, 
author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li}, 
title = {{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}}, 
eprint = {1603.08155}, 
abstract = {{We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \textbackslashemph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \textbackslashemph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2016_Perceptual%20Losses%20for%20Real-Time%20Style%20Transfer%20and%20Super-Resolution_Johnson_Fei-Fei.pdf}, 
year = {2016}
}
@article{artsiom, 
author = {Sanakoyeu, Artsiom and Kotovenko, Dmytro and Lang, Sabine and Ommer, Björn}, 
title = {{A Style-Aware Content Loss for Real-time HD Style Transfer}}, 
eprint = {1807.10201}, 
abstract = {{Recently, style transfer has received a lot of attention. While much of this research has aimed at speeding up processing, the approaches are still lacking from a principled, art historical standpoint: a style is more than just a single image or an artist, but previous work is limited to only a single instance of a style or shows no benefit from more images. Moreover, previous work has relied on a direct comparison of art in the domain of RGB images or on CNNs pre-trained on ImageNet, which requires millions of labeled object bounding boxes and can introduce an extra bias, since it has been assembled without artistic consideration. To circumvent these issues, we propose a style-aware content loss, which is trained jointly with a deep encoder-decoder network for real-time, high-resolution stylization of images and videos. We propose a quantitative measure for evaluating the quality of a stylized image and also have art historians rank patches from our approach against those from previous work. These and our qualitative results ranging from small image patches to megapixel stylistic images and videos show that our approach better captures the subtle nature in which a style affects content.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2018_A%20Style-Aware%20Content%20Loss%20for%20Real-time%20HD%20Style%20Transfer_Sanakoyeu_Ommer.pdf}, 
year = {2018}, 
rating = {5}
}
@article{10.1007/978-1-4471-4519-6_1, 
author = {Vanderhaeghe, David and Collomosse, John}, 
title = {{Image and Video-Based Artistic Stylisation}}, 
issn = {1381-6446}, 
doi = {10.1007/978-1-4471-4519-6\_1}, 
abstract = {{Many traditional art forms are produced by an artist sequentially placing a set of marks, such as brush strokes, on a canvas. Stroke based Rendering (SBR) is inspired by this process, and underpins many early and contemporary Artistic Stylization algorithms. This chapter outlines the origins of SBR, and describes key algorithms for placement of brush strokes to create painterly renderings from source images. The chapter explores both local greedy, and global optimization based approaches to stroke placement. The issue of creative control in SBR is also briefly discussed.}}, 
pages = {3--21}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2012_Image%20and%20Video-Based%20Artistic%20Stylisation_Vanderhaeghe_Collomosse.pdf}, 
year = {2012}
}
@article{StarGAN, 
author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul}, 
title = {{StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation}}, 
eprint = {1711.09020}, 
abstract = {{Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2017_StarGAN-%20Unified%20Generative%20Adversarial%20Networks%20for%20Multi-Domain%20Image-to-Image%20Translation_Choi_Choo.pdf}, 
year = {2017}
}
@article{AdaIN, 
author = {Huang, Xun and Belongie, Serge}, 
title = {{Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization}}, 
eprint = {1703.06868}, 
abstract = {{Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color \& spatial controls, all using a single feed-forward neural network.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2017_Arbitrary%20Style%20Transfer%20in%20Real-time%20with%20Adaptive%20Instance%20Normalization_Huang_Belongie.pdf}, 
year = {2017}
}
@article{CycleGAN, 
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A}, 
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}}, 
eprint = {1703.10593}, 
abstract = {{Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X \textbackslashrightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y \textbackslashrightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) \textbackslashapprox X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.}}, 
keywords = {CycleGAN}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2017_Unpaired%20Image-to-Image%20Translation%20using%20Cycle-Consistent%20Adversarial%20Networks_Zhu_Efros.pdf}, 
year = {2017}
}
@article{undefined, 
author = {Kang, Taewon and Lee, Kwang Hee}, 
title = {{Unsupervised Image-to-Image Translation with Self-Attention Networks}}, 
eprint = {1901.08242}, 
abstract = {{Unsupervised image translation aims to learn the transformation from a source domain to another target domain given unpaired training data. Several state-of-the-art works have yielded impressive results in the GANs-based unsupervised image-to-image translation. It fails to capture strong geometric or structural changes between domains, or it produces unsatisfactory result for complex scenes, compared to local texture mapping tasks such as style transfer. Recently, SAGAN (Han Zhang, 2018) showed that the self-attention network produces better results than the convolution-based GAN. However, the effectiveness of the self-attention network in unsupervised image-to-image translation tasks have not been verified. In this paper, we propose an unsupervised image-to-image translation with self-attention networks, in which long range dependency helps to not only capture strong geometric change but also generate details using cues from all feature locations. In experiments, we qualitatively and quantitatively show superiority of the proposed method compared to existing state-of-the-art unsupervised image-to-image translation task.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2019_Unsupervised%20Image-to-Image%20Translation%20with%20Self-Attention%20Networks_Kang_Lee.pdf}, 
year = {2019}
}
@article{10.1109/cvpr.2018.00986, 
author = {Chen, Yang and Lai, Yu-Kun and Liu, Yong-Jin}, 
title = {{CartoonGAN: Generative Adversarial Networks for Photo Cartoonization}}, 
doi = {10.1109/cvpr.2018.00986}, 
abstract = {{In this paper, we propose a solution to transforming photos of real-world scenes into cartoon style images, which is valuable and challenging in computer vision and computer graphics. Our solution belongs to learning based methods, which have recently become popular to stylize images in artistic forms such as painting. However, existing methods do not produce satisfactory results for cartoonization, due to the fact that (1) cartoon styles have unique characteristics with high level simplification and abstraction, and (2) cartoon images tend to have clear edges, smooth color shading and relatively simple textures, which exhibit significant challenges for texture-descriptor-based loss functions used in existing methods. In this paper, we propose CartoonGAN, a generative adversarial network (GAN) framework for cartoon stylization. Our method takes unpaired photos and cartoon images for training, which is easy to use. Two novel losses suitable for cartoonization are proposed: (1) a semantic content loss, which is formulated as a sparse regularization in the high-level feature maps of the VGG network to cope with substantial style variation between photos and cartoons, and (2) an edge-promoting adversarial loss for preserving clear edges. We further introduce an initialization phase, to improve the convergence of the network to the target manifold. Our method is also much more efficient to train than existing methods. Experimental results show that our method is able to generate high-quality cartoon images from real-world photos (i.e., following specific artists' styles and with clear edges and smooth shading) and outperforms state-of-the-art methods.}}, 
pages = {9465--9474}, 
journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2018_CartoonGANUnknown%20Generative%20Adversarial%20Networks%20for%20Photo%20Cartoonization_Chen_Liu.pdf}, 
year = {2018}
}
@article{funit, 
author = {Liu, Ming-Yu and Huang, Xun and Mallya, Arun and Karras, Tero and Aila, Timo and Lehtinen, Jaakko and Kautz, Jan}, 
title = {{Few-Shot Unsupervised Image-to-Image Translation}}, 
eprint = {1905.01723}, 
abstract = {{Unsupervised image-to-image translation methods learn to map images in a given class to an analogous image in a different class, drawing on unstructured (non-registered) datasets of images. While remarkably successful, current methods require access to many images in both source and destination classes at training time. We argue this greatly limits their use. Drawing inspiration from the human capability of picking up the essence of a novel object from a small number of examples and generalizing from there, we seek a few-shot, unsupervised image-to-image translation algorithm that works on previously unseen target classes that are specified, at test time, only by a few example images. Our model achieves this few-shot generation capability by coupling an adversarial training scheme with a novel network design. Through extensive experimental validation and comparisons to several baseline methods on benchmark datasets, we verify the effectiveness of the proposed framework. Our implementation and datasets are available at https://github.com/NVlabs/FUNIT .}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2019_Few-Shot%20Unsupervised%20Image-to-Image%20Translation_Liu_Kautz.pdf}, 
year = {2019}
}
@article{drit++, 
author = {Lee, Hsin-Ying and Tseng, Hung-Yu and Mao, Qi and Huang, Jia-Bin and Lu, Yu-Ding and Singh, Maneesh and Yang, Ming-Hsuan}, 
title = {{DRIT++: Diverse Image-to-Image Translation via Disentangled Representations}}, 
eprint = {1905.01270}, 
abstract = {{Image-to-image translation aims to learn the mapping between two visual domains. There are two main challenges for this task: 1) lack of aligned training pairs and 2) multiple possible outputs from a single input image. In this work, we present an approach based on disentangled representation for generating diverse outputs without paired training images. To synthesize diverse outputs, we propose to embed images onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. Our model takes the encoded content features extracted from a given input and attribute vectors sampled from the attribute space to synthesize diverse outputs at test time. To handle unpaired training data, we introduce a cross-cycle consistency loss based on disentangled representations. Qualitative results show that our model can generate diverse and realistic images on a wide range of tasks without paired training data. For quantitative evaluations, we measure realism with user study and Fr\textbackslash'\{e\}chet inception distance, and measure diversity with the perceptual distance metric, Jensen-Shannon divergence, and number of statistically-different bins.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2019_DRIT++-%20Diverse%20Image-to-Image%20Translation%20via%20Disentangled%20Representations_Lee_Yang.pdf}, 
year = {2019}
}
@article{drit, 
author = {Lee, Hsin-Ying and Tseng, Hung-Yu and Huang, Jia-Bin and Singh, Maneesh Kumar and Yang, Ming-Hsuan}, 
title = {{Diverse Image-to-Image Translation via Disentangled Representations}}, 
eprint = {1808.00948}, 
abstract = {{Image-to-image translation aims to learn the mapping between two visual domains. There are two main challenges for many applications: 1) the lack of aligned training pairs and 2) multiple possible outputs from a single input image. In this work, we present an approach based on disentangled representation for producing diverse outputs without paired training images. To achieve diversity, we propose to embed images onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. Our model takes the encoded content features extracted from a given input and the attribute vectors sampled from the attribute space to produce diverse outputs at test time. To handle unpaired training data, we introduce a novel cross-cycle consistency loss based on disentangled representations. Qualitative results show that our model can generate diverse and realistic images on a wide range of tasks without paired training data. For quantitative comparisons, we measure realism with user study and diversity with a perceptual distance metric. We apply the proposed model to domain adaptation and show competitive performance when compared to the state-of-the-art on the MNIST-M and the LineMod datasets.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2018_Diverse%20Image-to-Image%20Translation%20via%20Disentangled%20Representations_Lee_Yang.pdf}, 
year = {2018}
}
@article{munit, 
author = {Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan}, 
title = {{Multimodal Unsupervised Image-to-Image Translation}}, 
eprint = {1804.04732}, 
abstract = {{Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2018_Multimodal%20Unsupervised%20Image-to-Image%20Translation_Huang_Kautz.pdf}, 
year = {2018}
}
@article{ugatit, 
author = {Kim, Junho and Kim, Minjae and Kang, Hyeonwoo and Lee, Kwanghee}, 
title = {{U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation}}, 
eprint = {1907.10830}, 
abstract = {{We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based methods which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2019_U-GAT-IT-%20Unsupervised%20Generative%20Attentional%20Networks%20with%20Adaptive%20Layer-Instance%20Normalization%20for%20Image-to-Image%20Translation_Kim_Lee.pdf}, 
year = {2019}, 
rating = {4}
}
@article{undefined, 
author = {Texler, O. and Fišer, J. and Luká, M. and Lu, J. and Shechtman, E. and Sýkora, D.}, 
title = {{Enhancing Neural Style Transfer using Patch-Based Synthesis}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/043-050.pdf}
}
@article{pix2pix, 
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A}, 
title = {{Image-to-Image Translation with Conditional Adversarial Networks}}, 
doi = {pix2pix}, 
eprint = {1611.07004}, 
abstract = {{We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.}}, 
local-url = {file://localhost/Users/arthur/Documents/Papers%20Library/2016_Image-to-Image%20Translation%20with%20Conditional%20Adversarial%20Networks_Isola_Efros.pdf}, 
year = {2016}
}